Script started on 2021-04-06 10:56:23+0900
(base) imza@watson:~/nlp-architect$ . tr_test.sh
2021-04-06 10:56:28,347 INFO PyTorch version 1.4.0 available.
/home/imza/anaconda3/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.
  warnings.warn(msg)
2021-04-06 10:56:31,131 INFO loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/imza/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
2021-04-06 10:56:31,535 INFO loading configuration file https://d2zs9tzlek599f.cloudfront.net/models/transformers/bert-base-uncased.json from cache at /home/imza/.cache/torch/transformers/83883e496b9149de73afca2459ddc99b497f01f7b5a11a9d30412ae7c71c6466.e0985c00916db6b867b615e70d07ad741a1287f6495229bc4af91f0a3d12c102
2021-04-06 10:56:31,536 INFO Model config QuantizedBertConfig {
  "architectures": null,
  "attention_key": {
    "mode": "ema"
  },
  "attention_output": {
    "mode": "ema",
    "requantize_output": false
  },
  "attention_probs_dropout_prob": 0.1,
  "attention_query": {
    "mode": "ema"
  },
  "attention_value": {
    "mode": "ema",
    "requantize_output": false
  },
  "bos_token_id": 0,
  "do_sample": false,
  "eos_token_ids": 0,
  "ffn_intermediate": {
    "mode": "ema",
    "requantize_output": false
  },
  "ffn_output": {
    "mode": "ema",
    "requantize_output": false
  },
  "finetuning_task": null,
  "head": {
    "mode": "ema",
    "requantize_output": false,
    "start_step": 200
  },
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pooler": {
    "mode": "ema",
    "requantize_output": false
  },
  "position_embeddings": {
    "mode": "ema"
  },
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "token_type_embeddings": {
    "mode": "none"
  },
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 30522,
  "word_embeddings": {
    "mode": "ema"
  }
}

2021-04-06 10:56:32,338 INFO loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/imza/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
2021-04-06 10:56:35,316 INFO Weights of QuantizedBertForSequenceClassification not initialized from pretrained model: ['bert.embeddings.word_embeddings._step', 'bert.embeddings.position_embeddings._step', 'bert.embeddings.token_type_embeddings._step', 'bert.encoder.layer.0.attention.self.query._step', 'bert.encoder.layer.0.attention.self.query.input_thresh', 'bert.encoder.layer.0.attention.self.query.output_thresh', 'bert.encoder.layer.0.attention.self.key._step', 'bert.encoder.layer.0.attention.self.key.input_thresh', 'bert.encoder.layer.0.attention.self.key.output_thresh', 'bert.encoder.layer.0.attention.self.value._step', 'bert.encoder.layer.0.attention.self.value.input_thresh', 'bert.encoder.layer.0.attention.output.dense._step', 'bert.encoder.layer.0.attention.output.dense.input_thresh', 'bert.encoder.layer.0.intermediate.dense._step', 'bert.encoder.layer.0.intermediate.dense.input_thresh', 'bert.encoder.layer.0.output.dense._step', 'bert.encoder.layer.0.output.dense.input_thresh', 'bert.encoder.layer.1.attention.self.query._step', 'bert.encoder.layer.1.attention.self.query.input_thresh', 'bert.encoder.layer.1.attention.self.query.output_thresh', 'bert.encoder.layer.1.attention.self.key._step', 'bert.encoder.layer.1.attention.self.key.input_thresh', 'bert.encoder.layer.1.attention.self.key.output_thresh', 'bert.encoder.layer.1.attention.self.value._step', 'bert.encoder.layer.1.attention.self.value.input_thresh', 'bert.encoder.layer.1.attention.output.dense._step', 'bert.encoder.layer.1.attention.output.dense.input_thresh', 'bert.encoder.layer.1.intermediate.dense._step', 'bert.encoder.layer.1.intermediate.dense.input_thresh', 'bert.encoder.layer.1.output.dense._step', 'bert.encoder.layer.1.output.dense.input_thresh', 'bert.encoder.layer.2.attention.self.query._step', 'bert.encoder.layer.2.attention.self.query.input_thresh', 'bert.encoder.layer.2.attention.self.query.output_thresh', 'bert.encoder.layer.2.attention.self.key._step', 'bert.encoder.layer.2.attention.self.key.input_thresh', 'bert.encoder.layer.2.attention.self.key.output_thresh', 'bert.encoder.layer.2.attention.self.value._step', 'bert.encoder.layer.2.attention.self.value.input_thresh', 'bert.encoder.layer.2.attention.output.dense._step', 'bert.encoder.layer.2.attention.output.dense.input_thresh', 'bert.encoder.layer.2.intermediate.dense._step', 'bert.encoder.layer.2.intermediate.dense.input_thresh', 'bert.encoder.layer.2.output.dense._step', 'bert.encoder.layer.2.output.dense.input_thresh', 'bert.encoder.layer.3.attention.self.query._step', 'bert.encoder.layer.3.attention.self.query.input_thresh', 'bert.encoder.layer.3.attention.self.query.output_thresh', 'bert.encoder.layer.3.attention.self.key._step', 'bert.encoder.layer.3.attention.self.key.input_thresh', 'bert.encoder.layer.3.attention.self.key.output_thresh', 'bert.encoder.layer.3.attention.self.value._step', 'bert.encoder.layer.3.attention.self.value.input_thresh', 'bert.encoder.layer.3.attention.output.dense._step', 'bert.encoder.layer.3.attention.output.dense.input_thresh', 'bert.encoder.layer.3.intermediate.dense._step', 'bert.encoder.layer.3.intermediate.dense.input_thresh', 'bert.encoder.layer.3.output.dense._step', 'bert.encoder.layer.3.output.dense.input_thresh', 'bert.encoder.layer.4.attention.self.query._step', 'bert.encoder.layer.4.attention.self.query.input_thresh', 'bert.encoder.layer.4.attention.self.query.output_thresh', 'bert.encoder.layer.4.attention.self.key._step', 'bert.encoder.layer.4.attention.self.key.input_thresh', 'bert.encoder.layer.4.attention.self.key.output_thresh', 'bert.encoder.layer.4.attention.self.value._step', 'bert.encoder.layer.4.attention.self.value.input_thresh', 'bert.encoder.layer.4.attention.output.dense._step', 'bert.encoder.layer.4.attention.output.dense.input_thresh', 'bert.encoder.layer.4.intermediate.dense._step', 'bert.encoder.layer.4.intermediate.dense.input_thresh', 'bert.encoder.layer.4.output.dense._step', 'bert.encoder.layer.4.output.dense.input_thresh', 'bert.encoder.layer.5.attention.self.query._step', 'bert.encoder.layer.5.attention.self.query.input_thresh', 'bert.encoder.layer.5.attention.self.query.output_thresh', 'bert.encoder.layer.5.attention.self.key._step', 'bert.encoder.layer.5.attention.self.key.input_thresh', 'bert.encoder.layer.5.attention.self.key.output_thresh', 'bert.encoder.layer.5.attention.self.value._step', 'bert.encoder.layer.5.attention.self.value.input_thresh', 'bert.encoder.layer.5.attention.output.dense._step', 'bert.encoder.layer.5.attention.output.dense.input_thresh', 'bert.encoder.layer.5.intermediate.dense._step', 'bert.encoder.layer.5.intermediate.dense.input_thresh', 'bert.encoder.layer.5.output.dense._step', 'bert.encoder.layer.5.output.dense.input_thresh', 'bert.encoder.layer.6.attention.self.query._step', 'bert.encoder.layer.6.attention.self.query.input_thresh', 'bert.encoder.layer.6.attention.self.query.output_thresh', 'bert.encoder.layer.6.attention.self.key._step', 'bert.encoder.layer.6.attention.self.key.input_thresh', 'bert.encoder.layer.6.attention.self.key.output_thresh', 'bert.encoder.layer.6.attention.self.value._step', 'bert.encoder.layer.6.attention.self.value.input_thresh', 'bert.encoder.layer.6.attention.output.dense._step', 'bert.encoder.layer.6.attention.output.dense.input_thresh', 'bert.encoder.layer.6.intermediate.dense._step', 'bert.encoder.layer.6.intermediate.dense.input_thresh', 'bert.encoder.layer.6.output.dense._step', 'bert.encoder.layer.6.output.dense.input_thresh', 'bert.encoder.layer.7.attention.self.query._step', 'bert.encoder.layer.7.attention.self.query.input_thresh', 'bert.encoder.layer.7.attention.self.query.output_thresh', 'bert.encoder.layer.7.attention.self.key._step', 'bert.encoder.layer.7.attention.self.key.input_thresh', 'bert.encoder.layer.7.attention.self.key.output_thresh', 'bert.encoder.layer.7.attention.self.value._step', 'bert.encoder.layer.7.attention.self.value.input_thresh', 'bert.encoder.layer.7.attention.output.dense._step', 'bert.encoder.layer.7.attention.output.dense.input_thresh', 'bert.encoder.layer.7.intermediate.dense._step', 'bert.encoder.layer.7.intermediate.dense.input_thresh', 'bert.encoder.layer.7.output.dense._step', 'bert.encoder.layer.7.output.dense.input_thresh', 'bert.encoder.layer.8.attention.self.query._step', 'bert.encoder.layer.8.attention.self.query.input_thresh', 'bert.encoder.layer.8.attention.self.query.output_thresh', 'bert.encoder.layer.8.attention.self.key._step', 'bert.encoder.layer.8.attention.self.key.input_thresh', 'bert.encoder.layer.8.attention.self.key.output_thresh', 'bert.encoder.layer.8.attention.self.value._step', 'bert.encoder.layer.8.attention.self.value.input_thresh', 'bert.encoder.layer.8.attention.output.dense._step', 'bert.encoder.layer.8.attention.output.dense.input_thresh', 'bert.encoder.layer.8.intermediate.dense._step', 'bert.encoder.layer.8.intermediate.dense.input_thresh', 'bert.encoder.layer.8.output.dense._step', 'bert.encoder.layer.8.output.dense.input_thresh', 'bert.encoder.layer.9.attention.self.query._step', 'bert.encoder.layer.9.attention.self.query.input_thresh', 'bert.encoder.layer.9.attention.self.query.output_thresh', 'bert.encoder.layer.9.attention.self.key._step', 'bert.encoder.layer.9.attention.self.key.input_thresh', 'bert.encoder.layer.9.attention.self.key.output_thresh', 'bert.encoder.layer.9.attention.self.value._step', 'bert.encoder.layer.9.attention.self.value.input_thresh', 'bert.encoder.layer.9.attention.output.dense._step', 'bert.encoder.layer.9.attention.output.dense.input_thresh', 'bert.encoder.layer.9.intermediate.dense._step', 'bert.encoder.layer.9.intermediate.dense.input_thresh', 'bert.encoder.layer.9.output.dense._step', 'bert.encoder.layer.9.output.dense.input_thresh', 'bert.encoder.layer.10.attention.self.query._step', 'bert.encoder.layer.10.attention.self.query.input_thresh', 'bert.encoder.layer.10.attention.self.query.output_thresh', 'bert.encoder.layer.10.attention.self.key._step', 'bert.encoder.layer.10.attention.self.key.input_thresh', 'bert.encoder.layer.10.attention.self.key.output_thresh', 'bert.encoder.layer.10.attention.self.value._step', 'bert.encoder.layer.10.attention.self.value.input_thresh', 'bert.encoder.layer.10.attention.output.dense._step', 'bert.encoder.layer.10.attention.output.dense.input_thresh', 'bert.encoder.layer.10.intermediate.dense._step', 'bert.encoder.layer.10.intermediate.dense.input_thresh', 'bert.encoder.layer.10.output.dense._step', 'bert.encoder.layer.10.output.dense.input_thresh', 'bert.encoder.layer.11.attention.self.query._step', 'bert.encoder.layer.11.attention.self.query.input_thresh', 'bert.encoder.layer.11.attention.self.query.output_thresh', 'bert.encoder.layer.11.attention.self.key._step', 'bert.encoder.layer.11.attention.self.key.input_thresh', 'bert.encoder.layer.11.attention.self.key.output_thresh', 'bert.encoder.layer.11.attention.self.value._step', 'bert.encoder.layer.11.attention.self.value.input_thresh', 'bert.encoder.layer.11.attention.output.dense._step', 'bert.encoder.layer.11.attention.output.dense.input_thresh', 'bert.encoder.layer.11.intermediate.dense._step', 'bert.encoder.layer.11.intermediate.dense.input_thresh', 'bert.encoder.layer.11.output.dense._step', 'bert.encoder.layer.11.output.dense.input_thresh', 'bert.pooler.dense._step', 'bert.pooler.dense.input_thresh', 'classifier.weight', 'classifier.bias', 'classifier._step', 'classifier.input_thresh']
2021-04-06 10:56:35,316 INFO Weights from pretrained model not used in QuantizedBertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-04-06 10:56:40,190 INFO ***** Running training *****
2021-04-06 10:56:40,190 INFO   Num examples = 2490
2021-04-06 10:56:40,190 INFO   Num Epochs = 1
2021-04-06 10:56:40,191 INFO   Instantaneous batch size per GPU/CPU = 8
2021-04-06 10:56:40,191 INFO   Total train batch size (w. parallel, distributed & accumulation) = 8
2021-04-06 10:56:40,191 INFO   Gradient Accumulation steps = 1
2021-04-06 10:56:40,191 INFO   Total optimization steps = 312

Epoch:   0%|                                                                                                                                   | 0/1 [00:00<?, ?it/s]****** Epoch: 0


Train iteration:   0%|                                                                                                                       | 0/312 [00:00<?, ?it/s][A

Train iteration:   0%|▎                                                                                                              | 1/312 [00:00<01:12,  4.29it/s][A

Train iteration:   1%|▋                                                                                                              | 2/312 [00:00<01:08,  4.52it/s][A


_________________Check Weight quantization____________________
name:  bert.encoder.layer.0.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0165,  0.0260, -0.0262,  ...,  0.0154,  0.0768,  0.0549],
        [-0.0327,  0.0345, -0.0424,  ..., -0.0527,  0.1393,  0.0077],
        [ 0.0105,  0.0333,  0.0110,  ..., -0.0278,  0.0257, -0.0469],
        ...,
        [-0.0085,  0.0515,  0.0556,  ...,  0.0282,  0.0544, -0.0541],
        [-0.0198,  0.0943,  0.0616,  ..., -0.1043,  0.0602,  0.0470],
        [ 0.0015, -0.0952,  0.0099,  ..., -0.0190, -0.0508, -0.0084]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0165,  0.0248, -0.0248,  ...,  0.0165,  0.0785,  0.0537],
        [-0.0331,  0.0331, -0.0413,  ..., -0.0537,  0.1405,  0.0083],
        [ 0.0124,  0.0331,  0.0124,  ..., -0.0289,  0.0248, -0.0455],
        ...,
        [-0.0083,  0.0496,  0.0537,  ...,  0.0289,  0.0537, -0.0537],
        [-0.0207,  0.0950,  0.0620,  ..., -0.1033,  0.0620,  0.0455],
        [ 0.0000, -0.0950,  0.0083,  ..., -0.0207, -0.0496, -0.0083]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.5248, -0.4546, -0.4504, -0.4339, -0.4298, -0.4133, -0.3967, -0.3926,
        -0.3802, -0.3719, -0.3637, -0.3595, -0.3554, -0.3513, -0.3389, -0.3347,
        -0.3265, -0.3182, -0.3141, -0.3058, -0.3017, -0.2934, -0.2893, -0.2851,
        -0.2810, -0.2686, -0.2645, -0.2603, -0.2562, -0.2521, -0.2480, -0.2438,
        -0.2397, -0.2356, -0.2314, -0.2273, -0.2232, -0.2190, -0.2149, -0.2108,
        -0.2066, -0.2025, -0.1984, -0.1942, -0.1901, -0.1860, -0.1818, -0.1777,
        -0.1736, -0.1694, -0.1653, -0.1612, -0.1570, -0.1529, -0.1488, -0.1446,
        -0.1405, -0.1364, -0.1322, -0.1281, -0.1240, -0.1198, -0.1157, -0.1116,
        -0.1074, -0.1033, -0.0992, -0.0950, -0.0909, -0.0868, -0.0827, -0.0785,
        -0.0744, -0.0703, -0.0661, -0.0620, -0.0579, -0.0537, -0.0496, -0.0455,
        -0.0413, -0.0372, -0.0331, -0.0289, -0.0248, -0.0207, -0.0165, -0.0124,
        -0.0083, -0.0041, -0.0000,  0.0041,  0.0083,  0.0124,  0.0165,  0.0207,
         0.0248,  0.0289,  0.0331,  0.0372,  0.0413,  0.0455,  0.0496,  0.0537,
         0.0579,  0.0620,  0.0661,  0.0703,  0.0744,  0.0785,  0.0827,  0.0868,
         0.0909,  0.0950,  0.0992,  0.1033,  0.1074,  0.1116,  0.1157,  0.1198,
         0.1240,  0.1281,  0.1322,  0.1364,  0.1405,  0.1446,  0.1488,  0.1529,
         0.1570,  0.1612,  0.1653,  0.1694,  0.1736,  0.1777,  0.1818,  0.1860,
         0.1901,  0.1942,  0.1984,  0.2025,  0.2066,  0.2108,  0.2149,  0.2190,
         0.2232,  0.2273,  0.2314,  0.2356,  0.2397,  0.2438,  0.2480,  0.2521,
         0.2562,  0.2603,  0.2686,  0.2727,  0.2810,  0.2851,  0.2893,  0.2934,
         0.2975,  0.3099,  0.3141,  0.3182,  0.3223,  0.3265,  0.3306,  0.3347,
         0.3389,  0.3430,  0.3471,  0.3513,  0.3595,  0.3761,  0.3843,  0.4050,
         0.4133,  0.4298,  0.4339,  0.4546], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  180
---------Q_out---------
tensor([[[ 0.6916, -0.1257, -0.0629,  ..., -3.3951,  1.0059, -1.6975],
         [ 0.7545, -0.1886, -0.3144,  ...,  0.9431, -0.3144, -1.8233],
         [-0.0629, -0.1257,  0.8173,  ..., -1.2574,  0.0000, -2.4520],
         ...,
         [ 0.7545,  0.3772, -0.5658,  ..., -0.5658,  0.4401, -1.1317],
         [ 0.4401, -0.0000,  0.0629,  ..., -0.0000,  0.5658, -0.8173],
         [-0.3144,  0.3772, -0.1886,  ..., -0.8173,  1.0059, -1.1317]],

        [[ 0.6916, -0.1886,  0.0629,  ..., -3.3322,  1.1317, -1.5718],
         [ 0.3772, -0.0000, -0.4401,  ..., -0.2515,  1.2574, -1.5718],
         [ 0.8173,  0.1257, -0.7545,  ...,  0.3144, -0.2515, -1.8233],
         ...,
         [ 0.3144, -0.1886,  0.0000,  ..., -1.0059,  0.0000,  1.6347],
         [ 0.3772, -0.3772, -0.1886,  ...,  0.0000,  0.2515,  1.6975],
         [ 0.0000, -0.0629, -0.1257,  ..., -1.7604,  0.2515,  2.2005]],

        [[ 0.6916, -0.1886,  0.0629,  ..., -3.4579,  1.2574, -1.6975],
         [ 1.1946, -0.0629,  0.5030,  ..., -1.2574, -0.0629, -2.2005],
         [ 0.3144, -0.4401,  0.1886,  ..., -0.3144, -0.1886, -2.7035],
         ...,
         [ 0.6287,  0.5030, -0.2515,  ..., -1.1317,  0.3772, -1.4460],
         [ 0.4401,  0.3772, -0.1257,  ...,  0.0629,  0.5030, -0.8802],
         [ 0.1886,  0.4401, -0.3144,  ..., -1.4460,  0.5030, -1.1946]],

        ...,

        [[ 0.6916, -0.1886,  0.1257,  ..., -3.2693,  1.0688, -1.5089],
         [ 0.2515,  0.8802,  0.3772,  ..., -0.3772, -0.5658, -1.6975],
         [-0.0629, -1.0688, -0.1257,  ..., -1.0059,  0.4401, -1.5718],
         ...,
         [ 0.2515,  0.3772, -0.6287,  ..., -0.4401,  0.3144, -1.6975],
         [ 0.1257,  0.2515,  0.0629,  ..., -0.3772,  0.4401, -0.8173],
         [ 0.0629,  0.3144, -0.1886,  ..., -1.0688,  0.4401, -0.5030]],

        [[ 0.7545, -0.3144,  0.1257,  ..., -3.4579,  1.1317, -1.5718],
         [-0.5658,  0.6916,  0.5030,  ..., -0.3772,  1.1317, -1.6347],
         [-0.2515,  0.6287, -1.1946,  ..., -1.9490, -0.6916, -2.0119],
         ...,
         [ 0.3144,  0.3144, -0.1257,  ..., -0.7545,  0.2515, -0.9431],
         [ 0.0629,  0.5030, -0.2515,  ..., -0.2515,  0.6287, -0.5030],
         [ 0.0629,  0.2515, -0.4401,  ..., -1.6975,  0.5030, -1.5718]],

        [[ 0.5658, -0.1886, -0.0000,  ..., -3.8352,  1.0059, -0.4401],
         [-0.2515,  0.8173,  0.2515,  ..., -1.8861,  1.3203, -1.3203],
         [-0.4401, -0.5658, -0.1257,  ...,  0.1886, -1.5089, -2.2005],
         ...,
         [ 0.5030,  0.6287, -0.1257,  ..., -0.8173,  0.1886, -1.1946],
         [ 0.0629,  0.3772,  0.0629,  ..., -0.3772,  0.6287, -0.9431],
         [-0.0000,  0.5030, -0.2515,  ..., -1.1946,  0.7545, -1.2574]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.7332, -7.6703, -7.6075, -7.4817, -7.4189, -7.3560, -7.1674, -7.1045,
        -7.0416, -6.9788, -6.9159, -6.8530, -6.7901, -6.6015, -6.4129, -6.3500,
        -6.2872, -6.2243, -6.1614, -6.0985, -6.0357, -5.9728, -5.9099, -5.8471,
        -5.7842, -5.7213, -5.6584, -5.5956, -5.5327, -5.4698, -5.4070, -5.3441,
        -5.2812, -5.2183, -5.1555, -5.0926, -5.0297, -4.9669, -4.9040, -4.8411,
        -4.7782, -4.7154, -4.6525, -4.5896, -4.5268, -4.4639, -4.4010, -4.3381,
        -4.2753, -4.2124, -4.1495, -4.0867, -4.0238, -3.9609, -3.8980, -3.8352,
        -3.7723, -3.7094, -3.6466, -3.5837, -3.5208, -3.4579, -3.3951, -3.3322,
        -3.2693, -3.2065, -3.1436, -3.0807, -3.0178, -2.9550, -2.8921, -2.8292,
        -2.7664, -2.7035, -2.6406, -2.5777, -2.5149, -2.4520, -2.3891, -2.3263,
        -2.2634, -2.2005, -2.1376, -2.0748, -2.0119, -1.9490, -1.8861, -1.8233,
        -1.7604, -1.6975, -1.6347, -1.5718, -1.5089, -1.4460, -1.3832, -1.3203,
        -1.2574, -1.1946, -1.1317, -1.0688, -1.0059, -0.9431, -0.8802, -0.8173,
        -0.7545, -0.6916, -0.6287, -0.5658, -0.5030, -0.4401, -0.3772, -0.3144,
        -0.2515, -0.1886, -0.1257, -0.0629, -0.0000,  0.0629,  0.1257,  0.1886,
         0.2515,  0.3144,  0.3772,  0.4401,  0.5030,  0.5658,  0.6287,  0.6916,
         0.7545,  0.8173,  0.8802,  0.9431,  1.0059,  1.0688,  1.1317,  1.1946,
         1.2574,  1.3203,  1.3832,  1.4460,  1.5089,  1.5718,  1.6347,  1.6975,
         1.7604,  1.8233,  1.8861,  1.9490,  2.0119,  2.0748,  2.1376,  2.2005,
         2.2634,  2.3263,  2.3891,  2.4520,  2.5149,  2.5777,  2.6406,  2.7035,
         2.7664,  2.8292,  2.8921,  2.9550,  3.0178,  3.0807,  3.1436,  3.2065,
         3.2693,  3.3322,  3.3951,  3.4579,  3.5208,  3.5837,  3.6466,  3.7094,
         3.7723,  3.8352,  3.8980,  3.9609,  4.0238,  4.0867,  4.1495,  4.2124,
         4.2753,  4.3381,  4.4010,  4.4639,  4.5268,  4.5896,  4.6525,  4.7154,
         4.7782,  4.8411,  4.9040,  4.9669,  5.0297,  5.0926,  5.1555,  5.2183,
         5.2812,  5.3441,  5.4070,  5.4698,  5.5327,  5.5956,  5.6584,  5.7213,
         5.7842,  5.9728,  6.0357,  6.0985,  6.3500], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_out:  213
name:  bert.encoder.layer.0.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0082, -0.0376,  0.0200,  ...,  0.0142, -0.0138, -0.0093],
        [ 0.0265, -0.0635,  0.0169,  ..., -0.0172, -0.0426,  0.0163],
        [-0.0187, -0.0173, -0.0341,  ..., -0.0270, -0.0437, -0.0114],
        ...,
        [ 0.0199, -0.0189, -0.0108,  ...,  0.0182, -0.0569,  0.0845],
        [-0.0540,  0.0227, -0.0182,  ..., -0.0077, -0.0189, -0.0662],
        [ 0.0067, -0.0501,  0.0221,  ..., -0.0055, -0.0029, -0.0017]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0066, -0.0394,  0.0197,  ...,  0.0131, -0.0131, -0.0066],
        [ 0.0262, -0.0656,  0.0197,  ..., -0.0197, -0.0394,  0.0131],
        [-0.0197, -0.0197, -0.0328,  ..., -0.0262, -0.0459, -0.0131],
        ...,
        [ 0.0197, -0.0197, -0.0131,  ...,  0.0197, -0.0591,  0.0853],
        [-0.0525,  0.0197, -0.0197,  ..., -0.0066, -0.0197, -0.0656],
        [ 0.0066, -0.0525,  0.0197,  ..., -0.0066, -0.0000, -0.0000]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.7087, -0.6759, -0.6299, -0.6234, -0.5906, -0.5840, -0.5184, -0.5053,
        -0.4331, -0.4265, -0.4068, -0.4003, -0.3937, -0.3872, -0.3806, -0.3740,
        -0.3675, -0.3609, -0.3543, -0.3412, -0.3347, -0.3281, -0.3215, -0.3150,
        -0.3084, -0.3018, -0.2953, -0.2887, -0.2822, -0.2756, -0.2690, -0.2625,
        -0.2559, -0.2494, -0.2428, -0.2362, -0.2297, -0.2231, -0.2165, -0.2100,
        -0.2034, -0.1969, -0.1903, -0.1837, -0.1772, -0.1706, -0.1640, -0.1575,
        -0.1509, -0.1444, -0.1378, -0.1312, -0.1247, -0.1181, -0.1116, -0.1050,
        -0.0984, -0.0919, -0.0853, -0.0787, -0.0722, -0.0656, -0.0591, -0.0525,
        -0.0459, -0.0394, -0.0328, -0.0262, -0.0197, -0.0131, -0.0066, -0.0000,
         0.0066,  0.0131,  0.0197,  0.0262,  0.0328,  0.0394,  0.0459,  0.0525,
         0.0591,  0.0656,  0.0722,  0.0787,  0.0853,  0.0919,  0.0984,  0.1050,
         0.1116,  0.1181,  0.1247,  0.1312,  0.1378,  0.1444,  0.1509,  0.1575,
         0.1640,  0.1706,  0.1772,  0.1837,  0.1903,  0.1969,  0.2034,  0.2100,
         0.2165,  0.2231,  0.2297,  0.2362,  0.2428,  0.2494,  0.2559,  0.2625,
         0.2690,  0.2756,  0.2822,  0.2887,  0.2953,  0.3018,  0.3084,  0.3150,
         0.3215,  0.3281,  0.3347,  0.3412,  0.3478,  0.3543,  0.3609,  0.3675,
         0.3740,  0.3806,  0.3872,  0.3937,  0.4003,  0.4068,  0.4265,  0.4331,
         0.4397,  0.4462,  0.4659,  0.4725,  0.4790,  0.4987,  0.5250,  0.5446,
         0.5709,  0.6562,  0.7349,  0.7546,  0.7940,  0.8334], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  150
---------Q_out---------
tensor([[[ 1.3433,  0.1414,  0.3535,  ..., -0.0000, -0.1414,  0.7777],
         [-1.3433, -0.7070,  0.2828,  ...,  0.1414, -0.8484, -3.3937],
         [ 0.6363, -0.9191, -0.3535,  ..., -0.8484, -0.9191, -3.3230],
         ...,
         [ 0.7777,  0.2121, -0.7070,  ...,  1.1312,  0.4242,  0.4949],
         [ 0.9191,  0.0707, -1.1312,  ...,  0.1414,  0.8484, -0.0000],
         [ 1.2019, -0.1414, -1.2019,  ...,  0.4949,  0.7777, -0.4949]],

        [[ 1.3433,  0.3535,  0.3535,  ..., -0.2121, -0.0707,  0.7777],
         [ 0.7777, -0.9191, -0.0707,  ..., -2.5453, -0.1414, -2.1918],
         [ 0.2828, -0.9191,  0.2121,  ..., -1.4140, -0.0707, -2.6160],
         ...,
         [ 0.3535,  0.1414,  0.8484,  ..., -0.2828, -0.2121,  2.1211],
         [ 0.7070, -0.0707, -0.2828,  ...,  2.7574, -1.3433,  1.0605],
         [-0.1414,  0.9191,  0.2828,  ...,  0.4949, -0.7777,  2.6867]],

        [[ 1.2019,  0.3535,  0.3535,  ...,  0.2121, -0.1414,  0.5656],
         [-0.5656, -0.4242,  0.7777,  ..., -0.4949,  0.2828, -3.2523],
         [ 0.2121, -0.4949,  0.9898,  ..., -0.6363, -1.4140, -2.7574],
         ...,
         [ 0.8484,  0.1414, -1.2019,  ...,  0.8484,  0.0000,  0.4242],
         [ 1.0605, -0.0000, -1.1312,  ...,  0.3535,  0.6363,  0.0707],
         [ 0.9898,  0.2121, -0.9898,  ...,  0.6363,  1.3433, -0.3535]],

        ...,

        [[ 1.2019,  0.2121,  0.4242,  ..., -0.0707, -0.1414,  1.0605],
         [ 0.1414, -0.7777, -0.2121,  ..., -0.7777,  0.1414, -1.6261],
         [ 0.7777, -0.7777,  1.2019,  ..., -1.7675, -0.0707, -2.6160],
         ...,
         [ 0.9898,  0.2828, -0.7070,  ...,  0.9898,  0.6363,  0.0707],
         [ 1.1312,  0.1414, -1.0605,  ...,  0.0707,  0.7070,  0.2121],
         [ 1.2019, -0.1414, -1.4140,  ...,  0.8484,  0.8484,  0.2828]],

        [[ 1.4140,  0.2121,  0.3535,  ...,  0.2121, -0.0000,  0.6363],
         [ 0.9898, -0.9191, -0.7070,  ..., -1.2019, -1.2726, -2.0504],
         [ 0.8484, -0.5656, -0.4242,  ...,  0.4949,  1.6261, -1.5554],
         ...,
         [ 1.1312,  0.2828, -1.0605,  ...,  1.5554,  0.9898,  1.0605],
         [ 0.8484, -0.0000, -1.2726,  ...,  0.4242,  0.5656,  0.7777],
         [ 0.9898,  0.1414, -1.2019,  ...,  0.4949,  0.8484, -0.9191]],

        [[ 0.9191,  0.4949,  0.6363,  ..., -0.1414, -0.4242,  1.9090],
         [ 1.0605, -0.4949, -1.3433,  ...,  0.1414,  0.4242, -1.9090],
         [ 0.8484, -0.4949,  0.4949,  ...,  0.5656, -1.0605, -3.0402],
         ...,
         [ 0.7070,  0.2121, -1.1312,  ...,  1.4847, -0.0707,  0.6363],
         [ 1.1312, -0.1414, -1.2019,  ...,  0.0707,  0.7070, -0.0707],
         [ 1.2019,  0.0707, -1.0605,  ...,  0.7777,  0.9191, -0.5656]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-6.5753, -6.5046, -6.4339, -6.3632, -6.2925, -6.2218, -6.1511, -6.0804,
        -6.0097, -5.9390, -5.8683, -5.7976, -5.7269, -5.6562, -5.5855, -5.5148,
        -5.4440, -5.3733, -5.3026, -5.2319, -5.1612, -5.0905, -5.0198, -4.9491,
        -4.8784, -4.8077, -4.7370, -4.6663, -4.5956, -4.5249, -4.4542, -4.3835,
        -4.3128, -4.2421, -4.1714, -4.1007, -4.0300, -3.9593, -3.8886, -3.8179,
        -3.7472, -3.6765, -3.6058, -3.5351, -3.4644, -3.3937, -3.3230, -3.2523,
        -3.1816, -3.1109, -3.0402, -2.9695, -2.8988, -2.8281, -2.7574, -2.6867,
        -2.6160, -2.5453, -2.4746, -2.4039, -2.3332, -2.2625, -2.1918, -2.1211,
        -2.0504, -1.9797, -1.9090, -1.8383, -1.7675, -1.6968, -1.6261, -1.5554,
        -1.4847, -1.4140, -1.3433, -1.2726, -1.2019, -1.1312, -1.0605, -0.9898,
        -0.9191, -0.8484, -0.7777, -0.7070, -0.6363, -0.5656, -0.4949, -0.4242,
        -0.3535, -0.2828, -0.2121, -0.1414, -0.0707, -0.0000,  0.0707,  0.1414,
         0.2121,  0.2828,  0.3535,  0.4242,  0.4949,  0.5656,  0.6363,  0.7070,
         0.7777,  0.8484,  0.9191,  0.9898,  1.0605,  1.1312,  1.2019,  1.2726,
         1.3433,  1.4140,  1.4847,  1.5554,  1.6261,  1.6968,  1.7675,  1.8383,
         1.9090,  1.9797,  2.0504,  2.1211,  2.1918,  2.2625,  2.3332,  2.4039,
         2.4746,  2.5453,  2.6160,  2.6867,  2.7574,  2.8281,  2.8988,  2.9695,
         3.0402,  3.1109,  3.1816,  3.2523,  3.3230,  3.3937,  3.4644,  3.5351,
         3.6058,  3.6765,  3.7472,  3.8179,  3.8886,  3.9593,  4.0300,  4.1007,
         4.1714,  4.2421,  4.3128,  4.3835,  4.4542,  4.5249,  4.5956,  4.6663,
         4.7370,  4.8077,  4.8784,  4.9491,  5.0198,  5.0905,  5.1612,  5.2319,
         5.3026,  5.3733,  5.4440,  5.5148,  5.5855,  5.6562,  5.7269,  5.7976,
         5.8683,  5.9390,  6.0097,  6.0804,  6.1511,  6.2218,  6.2925,  6.3632,
         6.4339,  6.5046,  6.5753,  6.6460,  6.7167,  6.7874,  6.8581,  6.9288,
         6.9995,  7.0702,  7.1409,  7.2116,  7.2823,  7.3530,  7.4237,  7.4944,
         7.5651,  7.6358,  7.7772,  7.8479,  7.9893,  8.0600,  8.1307,  8.2721,
         8.3428,  8.4842,  8.6256,  8.6963,  8.9791], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  213
name:  bert.encoder.layer.0.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 1.1361e-02,  1.1967e-03, -9.5003e-03,  ...,  2.3739e-02,
         -2.7311e-02,  4.5181e-05],
        [-2.6546e-02, -1.2237e-02, -4.2465e-02,  ...,  5.9106e-02,
         -1.1424e-02, -2.4932e-02],
        [ 1.9199e-02, -2.3767e-02,  3.2010e-02,  ...,  5.6889e-03,
         -3.7764e-02, -2.2555e-02],
        ...,
        [-3.1549e-02, -2.0657e-02,  3.1081e-02,  ..., -3.9790e-02,
         -5.0490e-02, -5.9687e-03],
        [-6.7604e-03,  4.2676e-02,  4.7805e-02,  ...,  2.5200e-02,
          1.6247e-02, -8.8603e-03],
        [-1.8947e-03, -2.4515e-02,  5.4903e-03,  ..., -4.9545e-02,
         -5.7872e-02, -3.3843e-02]], device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0106,  0.0015, -0.0091,  ...,  0.0243, -0.0274,  0.0000],
        [-0.0258, -0.0122, -0.0426,  ...,  0.0593, -0.0122, -0.0243],
        [ 0.0198, -0.0243,  0.0319,  ...,  0.0061, -0.0380, -0.0228],
        ...,
        [-0.0319, -0.0213,  0.0304,  ..., -0.0395, -0.0502, -0.0061],
        [-0.0061,  0.0426,  0.0471,  ...,  0.0258,  0.0167, -0.0091],
        [-0.0015, -0.0243,  0.0061,  ..., -0.0502, -0.0578, -0.0334]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.1642, -0.1536, -0.1460, -0.1384, -0.1353, -0.1338, -0.1323, -0.1308,
        -0.1292, -0.1277, -0.1262, -0.1247, -0.1232, -0.1216, -0.1201, -0.1186,
        -0.1171, -0.1156, -0.1140, -0.1125, -0.1110, -0.1095, -0.1079, -0.1064,
        -0.1049, -0.1034, -0.1019, -0.1003, -0.0988, -0.0973, -0.0958, -0.0943,
        -0.0927, -0.0912, -0.0897, -0.0882, -0.0867, -0.0851, -0.0836, -0.0821,
        -0.0806, -0.0791, -0.0775, -0.0760, -0.0745, -0.0730, -0.0715, -0.0699,
        -0.0684, -0.0669, -0.0654, -0.0639, -0.0623, -0.0608, -0.0593, -0.0578,
        -0.0563, -0.0547, -0.0532, -0.0517, -0.0502, -0.0487, -0.0471, -0.0456,
        -0.0441, -0.0426, -0.0411, -0.0395, -0.0380, -0.0365, -0.0350, -0.0334,
        -0.0319, -0.0304, -0.0289, -0.0274, -0.0258, -0.0243, -0.0228, -0.0213,
        -0.0198, -0.0182, -0.0167, -0.0152, -0.0137, -0.0122, -0.0106, -0.0091,
        -0.0076, -0.0061, -0.0046, -0.0030, -0.0015, -0.0000,  0.0015,  0.0030,
         0.0046,  0.0061,  0.0076,  0.0091,  0.0106,  0.0122,  0.0137,  0.0152,
         0.0167,  0.0182,  0.0198,  0.0213,  0.0228,  0.0243,  0.0258,  0.0274,
         0.0289,  0.0304,  0.0319,  0.0334,  0.0350,  0.0365,  0.0380,  0.0395,
         0.0411,  0.0426,  0.0441,  0.0456,  0.0471,  0.0487,  0.0502,  0.0517,
         0.0532,  0.0547,  0.0563,  0.0578,  0.0593,  0.0608,  0.0623,  0.0639,
         0.0654,  0.0669,  0.0684,  0.0699,  0.0715,  0.0730,  0.0745,  0.0760,
         0.0775,  0.0791,  0.0806,  0.0821,  0.0836,  0.0851,  0.0867,  0.0882,
         0.0897,  0.0912,  0.0927,  0.0943,  0.0958,  0.0973,  0.0988,  0.1003,
         0.1019,  0.1034,  0.1049,  0.1064,  0.1079,  0.1095,  0.1110,  0.1125,
         0.1140,  0.1156,  0.1171,  0.1186,  0.1201,  0.1216,  0.1232,  0.1247,
         0.1262,  0.1277,  0.1292,  0.1308,  0.1338,  0.1353,  0.1384,  0.1414,
         0.1429,  0.1475,  0.1505,  0.1931], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  188
---------Q_out---------
None
name:  bert.encoder.layer.0.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0057, -0.0029,  0.0028,  ..., -0.0075, -0.0267,  0.0368],
        [ 0.0318,  0.0073, -0.0163,  ..., -0.0149,  0.0205, -0.0106],
        [-0.0615, -0.0343,  0.0214,  ...,  0.0129,  0.0019, -0.0118],
        ...,
        [ 0.0474, -0.0049,  0.0513,  ..., -0.0092, -0.0011, -0.0153],
        [-0.0100, -0.0253, -0.0535,  ...,  0.0463, -0.0129, -0.0402],
        [-0.0507,  0.0198, -0.0150,  ...,  0.0324, -0.0132, -0.0058]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0046, -0.0046,  0.0046,  ..., -0.0092, -0.0277,  0.0370],
        [ 0.0323,  0.0092, -0.0185,  ..., -0.0139,  0.0185, -0.0092],
        [-0.0601, -0.0323,  0.0231,  ...,  0.0139,  0.0000, -0.0139],
        ...,
        [ 0.0462, -0.0046,  0.0508,  ..., -0.0092, -0.0000, -0.0139],
        [-0.0092, -0.0231, -0.0555,  ...,  0.0462, -0.0139, -0.0416],
        [-0.0508,  0.0185, -0.0139,  ...,  0.0323, -0.0139, -0.0046]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.4945, -0.4113, -0.3327, -0.3050, -0.3004, -0.2958, -0.2819, -0.2773,
        -0.2680, -0.2588, -0.2542, -0.2495, -0.2449, -0.2403, -0.2357, -0.2311,
        -0.2264, -0.2218, -0.2172, -0.2126, -0.2080, -0.2033, -0.1987, -0.1941,
        -0.1895, -0.1848, -0.1802, -0.1756, -0.1710, -0.1664, -0.1617, -0.1571,
        -0.1525, -0.1479, -0.1433, -0.1386, -0.1340, -0.1294, -0.1248, -0.1202,
        -0.1155, -0.1109, -0.1063, -0.1017, -0.0970, -0.0924, -0.0878, -0.0832,
        -0.0786, -0.0739, -0.0693, -0.0647, -0.0601, -0.0555, -0.0508, -0.0462,
        -0.0416, -0.0370, -0.0323, -0.0277, -0.0231, -0.0185, -0.0139, -0.0092,
        -0.0046, -0.0000,  0.0046,  0.0092,  0.0139,  0.0185,  0.0231,  0.0277,
         0.0323,  0.0370,  0.0416,  0.0462,  0.0508,  0.0555,  0.0601,  0.0647,
         0.0693,  0.0739,  0.0786,  0.0832,  0.0878,  0.0924,  0.0970,  0.1017,
         0.1063,  0.1109,  0.1155,  0.1202,  0.1248,  0.1294,  0.1340,  0.1386,
         0.1433,  0.1479,  0.1525,  0.1571,  0.1617,  0.1664,  0.1710,  0.1756,
         0.1802,  0.1848,  0.1895,  0.1941,  0.1987,  0.2033,  0.2080,  0.2126,
         0.2172,  0.2218,  0.2264,  0.2311,  0.2357,  0.2403,  0.2449,  0.2495,
         0.2634,  0.2680,  0.2727,  0.2773,  0.2819,  0.2958,  0.3281,  0.3697,
         0.4113,  0.4252,  0.5130,  0.5869], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  132
---------Q_out---------
None
name:  bert.encoder.layer.0.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0101, -0.0018,  0.0131,  ..., -0.0538,  0.0136, -0.0191],
        [-0.0603,  0.0103,  0.0649,  ..., -0.0486, -0.0276,  0.0070],
        [-0.0146,  0.0427,  0.0398,  ..., -0.0082,  0.0114, -0.0274],
        ...,
        [-0.0499, -0.0659,  0.0270,  ..., -0.0130,  0.0042,  0.0069],
        [ 0.0447, -0.0238,  0.0166,  ...,  0.0035,  0.0068,  0.0204],
        [-0.0093,  0.0362,  0.0257,  ..., -0.0087,  0.0063,  0.0604]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0101, -0.0025,  0.0127,  ..., -0.0533,  0.0127, -0.0203],
        [-0.0609,  0.0101,  0.0660,  ..., -0.0482, -0.0279,  0.0076],
        [-0.0152,  0.0431,  0.0406,  ..., -0.0076,  0.0101, -0.0279],
        ...,
        [-0.0507, -0.0660,  0.0279,  ..., -0.0127,  0.0051,  0.0076],
        [ 0.0457, -0.0228,  0.0178,  ...,  0.0025,  0.0076,  0.0203],
        [-0.0101,  0.0355,  0.0254,  ..., -0.0076,  0.0051,  0.0609]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2968, -0.2790, -0.2689, -0.2486, -0.2385, -0.2359, -0.2334, -0.2308,
        -0.2283, -0.2232, -0.2182, -0.2156, -0.2106, -0.2080, -0.2055, -0.2029,
        -0.2004, -0.1979, -0.1953, -0.1928, -0.1877, -0.1852, -0.1826, -0.1801,
        -0.1776, -0.1750, -0.1725, -0.1700, -0.1674, -0.1649, -0.1624, -0.1598,
        -0.1573, -0.1547, -0.1522, -0.1497, -0.1471, -0.1446, -0.1421, -0.1395,
        -0.1370, -0.1344, -0.1319, -0.1294, -0.1268, -0.1243, -0.1218, -0.1192,
        -0.1167, -0.1142, -0.1116, -0.1091, -0.1065, -0.1040, -0.1015, -0.0989,
        -0.0964, -0.0939, -0.0913, -0.0888, -0.0863, -0.0837, -0.0812, -0.0786,
        -0.0761, -0.0736, -0.0710, -0.0685, -0.0660, -0.0634, -0.0609, -0.0583,
        -0.0558, -0.0533, -0.0507, -0.0482, -0.0457, -0.0431, -0.0406, -0.0381,
        -0.0355, -0.0330, -0.0304, -0.0279, -0.0254, -0.0228, -0.0203, -0.0178,
        -0.0152, -0.0127, -0.0101, -0.0076, -0.0051, -0.0025, -0.0000,  0.0025,
         0.0051,  0.0076,  0.0101,  0.0127,  0.0152,  0.0178,  0.0203,  0.0228,
         0.0254,  0.0279,  0.0304,  0.0330,  0.0355,  0.0381,  0.0406,  0.0431,
         0.0457,  0.0482,  0.0507,  0.0533,  0.0558,  0.0583,  0.0609,  0.0634,
         0.0660,  0.0685,  0.0710,  0.0736,  0.0761,  0.0786,  0.0812,  0.0837,
         0.0863,  0.0888,  0.0913,  0.0939,  0.0964,  0.0989,  0.1015,  0.1040,
         0.1065,  0.1091,  0.1116,  0.1142,  0.1167,  0.1192,  0.1218,  0.1243,
         0.1268,  0.1294,  0.1319,  0.1344,  0.1370,  0.1395,  0.1421,  0.1446,
         0.1471,  0.1497,  0.1522,  0.1547,  0.1573,  0.1598,  0.1624,  0.1649,
         0.1674,  0.1700,  0.1725,  0.1750,  0.1776,  0.1801,  0.1826,  0.1852,
         0.1877,  0.1903,  0.1928,  0.1953,  0.1979,  0.2004,  0.2029,  0.2055,
         0.2080,  0.2106,  0.2131,  0.2156,  0.2182,  0.2207,  0.2232,  0.2258,
         0.2283,  0.2308,  0.2334,  0.2359,  0.2435,  0.2562,  0.2892,  0.3120,
         0.3146,  0.3222], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  194
---------Q_out---------
None
name:  bert.encoder.layer.0.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0372, -0.0198,  0.0282,  ..., -0.0119,  0.0127,  0.0155],
        [ 0.0648, -0.0122,  0.0462,  ..., -0.0040, -0.0236, -0.0011],
        [ 0.0075, -0.0682,  0.0349,  ..., -0.0061,  0.0325,  0.0055],
        ...,
        [-0.0410, -0.0224,  0.0126,  ...,  0.0089,  0.0107,  0.0163],
        [-0.0699,  0.0252,  0.0324,  ..., -0.0022, -0.0585,  0.0520],
        [-0.0542,  0.0008,  0.0004,  ...,  0.0286,  0.0025,  0.0285]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0397, -0.0132,  0.0264,  ..., -0.0132,  0.0132,  0.0132],
        [ 0.0661, -0.0132,  0.0397,  ..., -0.0000, -0.0264, -0.0000],
        [ 0.0132, -0.0661,  0.0397,  ..., -0.0000,  0.0264,  0.0000],
        ...,
        [-0.0397, -0.0264,  0.0132,  ...,  0.0132,  0.0132,  0.0132],
        [-0.0661,  0.0264,  0.0264,  ..., -0.0000, -0.0529,  0.0529],
        [-0.0529,  0.0000,  0.0000,  ...,  0.0264,  0.0000,  0.0264]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-1.6792, -1.3090, -0.9388, -0.9123, -0.8726, -0.8462, -0.8065, -0.7933,
        -0.7801, -0.7669, -0.7140, -0.7008, -0.6479, -0.6347, -0.6214, -0.5818,
        -0.5685, -0.5553, -0.5421, -0.5157, -0.5024, -0.4892, -0.4760, -0.4628,
        -0.4495, -0.4363, -0.4231, -0.4099, -0.3967, -0.3834, -0.3702, -0.3570,
        -0.3438, -0.3305, -0.3173, -0.3041, -0.2909, -0.2777, -0.2644, -0.2512,
        -0.2380, -0.2248, -0.2116, -0.1983, -0.1851, -0.1719, -0.1587, -0.1454,
        -0.1322, -0.1190, -0.1058, -0.0926, -0.0793, -0.0661, -0.0529, -0.0397,
        -0.0264, -0.0132, -0.0000,  0.0132,  0.0264,  0.0397,  0.0529,  0.0661,
         0.0793,  0.0926,  0.1058,  0.1190,  0.1322,  0.1454,  0.1587,  0.1719,
         0.1851,  0.1983,  0.2116,  0.2248,  0.2380,  0.2512,  0.2644,  0.2777,
         0.2909,  0.3041,  0.3173,  0.3305,  0.3438,  0.3570,  0.3702,  0.3834,
         0.4099,  0.4363,  0.4495,  0.5157,  0.5421,  0.5553,  0.5685,  0.6479,
         0.6611,  0.7008,  0.7404,  0.7669], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  100
---------Q_out---------
None
name:  bert.encoder.layer.1.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0314, -0.0256,  0.0049,  ..., -0.0337, -0.0592,  0.0649],
        [-0.0133, -0.0246, -0.0282,  ..., -0.0348,  0.0525,  0.0495],
        [-0.0776, -0.0501, -0.0083,  ...,  0.0185,  0.0116, -0.0471],
        ...,
        [ 0.0597, -0.0207, -0.0365,  ...,  0.0079,  0.0104, -0.0283],
        [ 0.0064, -0.0108, -0.0044,  ..., -0.0040,  0.0233, -0.0587],
        [-0.0127, -0.0116,  0.0550,  ...,  0.1038, -0.0564,  0.0245]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0307, -0.0245,  0.0061,  ..., -0.0337, -0.0582,  0.0644],
        [-0.0123, -0.0245, -0.0276,  ..., -0.0337,  0.0521,  0.0490],
        [-0.0766, -0.0490, -0.0092,  ...,  0.0184,  0.0123, -0.0460],
        ...,
        [ 0.0582, -0.0215, -0.0368,  ...,  0.0092,  0.0092, -0.0276],
        [ 0.0061, -0.0123, -0.0031,  ..., -0.0031,  0.0245, -0.0582],
        [-0.0123, -0.0123,  0.0552,  ...,  0.1042, -0.0552,  0.0245]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3893, -0.3648, -0.3341, -0.2789, -0.2728, -0.2667, -0.2605, -0.2544,
        -0.2513, -0.2483, -0.2391, -0.2360, -0.2330, -0.2299, -0.2268, -0.2238,
        -0.2207, -0.2176, -0.2146, -0.2115, -0.2084, -0.2054, -0.2023, -0.1992,
        -0.1962, -0.1931, -0.1900, -0.1870, -0.1839, -0.1808, -0.1778, -0.1747,
        -0.1716, -0.1686, -0.1655, -0.1625, -0.1594, -0.1563, -0.1533, -0.1502,
        -0.1471, -0.1441, -0.1410, -0.1379, -0.1349, -0.1318, -0.1287, -0.1257,
        -0.1226, -0.1195, -0.1165, -0.1134, -0.1103, -0.1073, -0.1042, -0.1011,
        -0.0981, -0.0950, -0.0920, -0.0889, -0.0858, -0.0828, -0.0797, -0.0766,
        -0.0736, -0.0705, -0.0674, -0.0644, -0.0613, -0.0582, -0.0552, -0.0521,
        -0.0490, -0.0460, -0.0429, -0.0398, -0.0368, -0.0337, -0.0307, -0.0276,
        -0.0245, -0.0215, -0.0184, -0.0153, -0.0123, -0.0092, -0.0061, -0.0031,
        -0.0000,  0.0031,  0.0061,  0.0092,  0.0123,  0.0153,  0.0184,  0.0215,
         0.0245,  0.0276,  0.0307,  0.0337,  0.0368,  0.0398,  0.0429,  0.0460,
         0.0490,  0.0521,  0.0552,  0.0582,  0.0613,  0.0644,  0.0674,  0.0705,
         0.0736,  0.0766,  0.0797,  0.0828,  0.0858,  0.0889,  0.0920,  0.0950,
         0.0981,  0.1011,  0.1042,  0.1073,  0.1103,  0.1134,  0.1165,  0.1195,
         0.1226,  0.1257,  0.1287,  0.1318,  0.1349,  0.1379,  0.1410,  0.1441,
         0.1471,  0.1502,  0.1533,  0.1563,  0.1594,  0.1625,  0.1655,  0.1686,
         0.1716,  0.1747,  0.1778,  0.1808,  0.1839,  0.1870,  0.1900,  0.1931,
         0.1962,  0.1992,  0.2023,  0.2054,  0.2084,  0.2115,  0.2146,  0.2207,
         0.2238,  0.2268,  0.2299,  0.2330,  0.2360,  0.2391,  0.2452,  0.2483,
         0.2544,  0.2667,  0.2697,  0.2759,  0.2789,  0.2881,  0.3034],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  175
---------Q_out---------
tensor([[[-0.5591, -0.6709, -0.4473,  ..., -0.3354,  0.7268,  0.5591],
         [-0.2236, -0.1118,  0.1118,  ..., -1.3418,  2.6277,  0.9504],
         [-0.9504,  0.6709,  0.5032,  ..., -1.3418,  1.0063, -1.0063],
         ...,
         [ 0.6150,  0.1118,  0.2795,  ...,  0.5032, -0.4473, -0.3914],
         [ 0.5591, -0.2236,  0.2236,  ...,  0.7827, -0.2795, -0.5591],
         [ 0.1118, -0.5032,  0.3354,  ..., -0.3354,  0.1118, -0.1677]],

        [[-0.8945, -0.8386, -0.6709,  ..., -0.2236,  0.8386,  0.7268],
         [ 0.0000,  0.1118,  1.2300,  ..., -2.1245,  1.6213, -1.7891],
         [-0.1677, -0.3914,  0.6150,  ..., -0.3354,  1.4536,  0.4473],
         ...,
         [-0.4473, -1.0622,  3.6899,  ..., -1.5095, -0.8386, -0.6709],
         [-0.2795, -0.6150, -0.2236,  ...,  0.1118,  0.8945, -0.6709],
         [-0.7827, -1.2300, -0.7268,  ...,  0.0559,  0.2795, -0.3354]],

        [[-0.6709, -0.6709, -0.4473,  ..., -0.3914,  0.7827,  0.5032],
         [-0.2236, -0.2236, -0.2795,  ..., -0.2795,  1.9009,  0.3914],
         [-0.6709,  2.3481,  1.0063,  ..., -0.5032,  1.7891,  1.1182],
         ...,
         [ 0.1677, -0.3914,  0.3914,  ...,  0.3354, -1.0622, -0.0559],
         [ 0.3914, -0.2795,  0.5032,  ...,  0.5591, -0.3914, -0.8945],
         [ 0.3354,  0.3354,  0.6709,  ...,  0.5032, -0.8945,  0.1118]],

        ...,

        [[-0.4473, -0.5032, -0.6150,  ..., -0.3354,  0.8386,  0.2236],
         [-0.1677,  0.5032, -1.6213,  ..., -0.8386,  0.1677, -1.2300],
         [ 1.3977, -1.2859, -0.0559,  ...,  0.0000,  1.2859,  0.0000],
         ...,
         [ 0.8945,  0.0559,  0.0559,  ...,  0.3354, -0.7268, -0.2236],
         [ 0.6709, -0.2795,  0.5032,  ...,  0.6709,  0.1118, -0.6150],
         [ 1.0063, -0.2795,  0.8386,  ..., -0.0559,  0.1677, -0.1677]],

        [[-0.6709, -0.6709, -0.4473,  ..., -0.1118,  0.5591,  0.6150],
         [ 0.1677,  1.3977,  0.2236,  ..., -0.7268,  1.2300, -2.0127],
         [-0.5032,  0.6150, -1.5654,  ..., -0.5591,  0.2236,  0.2795],
         ...,
         [ 0.6150, -0.3914,  0.1677,  ...,  0.6150, -0.8945, -0.2795],
         [ 0.3914, -0.1118,  0.2236,  ...,  0.3354, -0.3354, -0.5591],
         [ 0.6150, -0.5032,  0.1118,  ...,  0.4473, -0.1118, -0.1118]],

        [[-0.7268, -0.6709, -0.5591,  ..., -0.2795,  0.7268,  0.4473],
         [ 0.6709,  0.2795,  0.2236,  ..., -0.7268,  2.1804, -0.3914],
         [ 0.1677,  0.7827,  2.2922,  ...,  0.1118,  2.6836,  0.2795],
         ...,
         [ 0.6709, -0.6709, -0.0559,  ...,  0.3914, -0.3914,  0.1118],
         [ 0.6150, -0.2236,  0.0000,  ...,  0.4473,  0.2236, -0.7268],
         [ 0.8386,  0.0000,  0.8945,  ...,  0.3354,  0.1677, -0.1118]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-6.7089, -6.5412, -6.4294, -6.3735, -5.9821, -5.9262, -5.8703, -5.7585,
        -5.7026, -5.6467, -5.5908, -5.5349, -5.4790, -5.4231, -5.3672, -5.3112,
        -5.2553, -5.1994, -5.1435, -5.0876, -5.0317, -4.9758, -4.9199, -4.8640,
        -4.8081, -4.7522, -4.6963, -4.6404, -4.5844, -4.5285, -4.4726, -4.4167,
        -4.3608, -4.3049, -4.2490, -4.1931, -4.1372, -4.0813, -4.0254, -3.9695,
        -3.9136, -3.8576, -3.8017, -3.7458, -3.6899, -3.6340, -3.5781, -3.5222,
        -3.4663, -3.4104, -3.3545, -3.2986, -3.2427, -3.1867, -3.1308, -3.0749,
        -3.0190, -2.9631, -2.9072, -2.8513, -2.7954, -2.7395, -2.6836, -2.6277,
        -2.5718, -2.5159, -2.4599, -2.4040, -2.3481, -2.2922, -2.2363, -2.1804,
        -2.1245, -2.0686, -2.0127, -1.9568, -1.9009, -1.8450, -1.7891, -1.7331,
        -1.6772, -1.6213, -1.5654, -1.5095, -1.4536, -1.3977, -1.3418, -1.2859,
        -1.2300, -1.1741, -1.1182, -1.0622, -1.0063, -0.9504, -0.8945, -0.8386,
        -0.7827, -0.7268, -0.6709, -0.6150, -0.5591, -0.5032, -0.4473, -0.3914,
        -0.3354, -0.2795, -0.2236, -0.1677, -0.1118, -0.0559, -0.0000,  0.0559,
         0.1118,  0.1677,  0.2236,  0.2795,  0.3354,  0.3914,  0.4473,  0.5032,
         0.5591,  0.6150,  0.6709,  0.7268,  0.7827,  0.8386,  0.8945,  0.9504,
         1.0063,  1.0622,  1.1182,  1.1741,  1.2300,  1.2859,  1.3418,  1.3977,
         1.4536,  1.5095,  1.5654,  1.6213,  1.6772,  1.7331,  1.7891,  1.8450,
         1.9009,  1.9568,  2.0127,  2.0686,  2.1245,  2.1804,  2.2363,  2.2922,
         2.3481,  2.4040,  2.4599,  2.5159,  2.5718,  2.6277,  2.6836,  2.7395,
         2.7954,  2.8513,  2.9072,  2.9631,  3.0190,  3.0749,  3.1308,  3.1867,
         3.2427,  3.2986,  3.3545,  3.4104,  3.4663,  3.5222,  3.5781,  3.6340,
         3.6899,  3.7458,  3.8017,  3.8576,  3.9136,  3.9695,  4.0254,  4.0813,
         4.1372,  4.1931,  4.2490,  4.3049,  4.3608,  4.4167,  4.4726,  4.5285,
         4.5844,  4.6404,  4.6963,  4.7522,  4.8081,  4.8640,  4.9199,  4.9758,
         5.0317,  5.0876,  5.1435,  5.1994,  5.2553,  5.3112,  5.3672,  5.4231,
         5.4790,  5.5349,  5.5908,  5.6467,  5.7026,  5.7585,  5.8144,  5.8703,
         5.9262,  5.9821,  6.0380,  6.0940,  6.1499,  6.2058,  6.2617,  6.3176,
         6.3735,  6.4294,  6.5412,  6.6530,  6.8767,  7.1003], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  230
name:  bert.encoder.layer.1.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0336,  0.0324, -0.0381,  ...,  0.0010, -0.0154,  0.0268],
        [ 0.0400,  0.0624,  0.0059,  ...,  0.0590, -0.0429,  0.0192],
        [ 0.0533,  0.0053,  0.0055,  ..., -0.0802,  0.0151, -0.0215],
        ...,
        [ 0.0103,  0.0799,  0.0187,  ...,  0.0308, -0.0590, -0.0593],
        [ 0.0018, -0.0023, -0.0299,  ..., -0.0177, -0.0311,  0.0058],
        [-0.0033, -0.0594,  0.0413,  ...,  0.0244,  0.0298,  0.0520]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0341,  0.0313, -0.0369,  ...,  0.0000, -0.0142,  0.0256],
        [ 0.0398,  0.0625,  0.0057,  ...,  0.0597, -0.0426,  0.0199],
        [ 0.0540,  0.0057,  0.0057,  ..., -0.0796,  0.0142, -0.0227],
        ...,
        [ 0.0114,  0.0796,  0.0199,  ...,  0.0313, -0.0597, -0.0597],
        [ 0.0028, -0.0028, -0.0313,  ..., -0.0170, -0.0313,  0.0057],
        [-0.0028, -0.0597,  0.0426,  ...,  0.0256,  0.0313,  0.0511]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3608, -0.3125, -0.3068, -0.2955, -0.2898, -0.2870, -0.2813, -0.2727,
        -0.2699, -0.2671, -0.2642, -0.2585, -0.2557, -0.2529, -0.2500, -0.2472,
        -0.2443, -0.2415, -0.2387, -0.2358, -0.2330, -0.2301, -0.2273, -0.2244,
        -0.2216, -0.2188, -0.2159, -0.2131, -0.2102, -0.2074, -0.2046, -0.2017,
        -0.1989, -0.1960, -0.1932, -0.1904, -0.1875, -0.1847, -0.1818, -0.1790,
        -0.1762, -0.1733, -0.1705, -0.1676, -0.1648, -0.1619, -0.1591, -0.1563,
        -0.1534, -0.1506, -0.1477, -0.1449, -0.1421, -0.1392, -0.1364, -0.1335,
        -0.1307, -0.1279, -0.1250, -0.1222, -0.1193, -0.1165, -0.1136, -0.1108,
        -0.1080, -0.1051, -0.1023, -0.0994, -0.0966, -0.0938, -0.0909, -0.0881,
        -0.0852, -0.0824, -0.0796, -0.0767, -0.0739, -0.0710, -0.0682, -0.0653,
        -0.0625, -0.0597, -0.0568, -0.0540, -0.0511, -0.0483, -0.0455, -0.0426,
        -0.0398, -0.0369, -0.0341, -0.0313, -0.0284, -0.0256, -0.0227, -0.0199,
        -0.0170, -0.0142, -0.0114, -0.0085, -0.0057, -0.0028, -0.0000,  0.0028,
         0.0057,  0.0085,  0.0114,  0.0142,  0.0170,  0.0199,  0.0227,  0.0256,
         0.0284,  0.0313,  0.0341,  0.0369,  0.0398,  0.0426,  0.0455,  0.0483,
         0.0511,  0.0540,  0.0568,  0.0597,  0.0625,  0.0653,  0.0682,  0.0710,
         0.0739,  0.0767,  0.0796,  0.0824,  0.0852,  0.0881,  0.0909,  0.0938,
         0.0966,  0.0994,  0.1023,  0.1051,  0.1080,  0.1108,  0.1136,  0.1165,
         0.1193,  0.1222,  0.1250,  0.1279,  0.1307,  0.1335,  0.1364,  0.1392,
         0.1421,  0.1449,  0.1477,  0.1506,  0.1534,  0.1563,  0.1591,  0.1619,
         0.1648,  0.1676,  0.1705,  0.1733,  0.1762,  0.1790,  0.1818,  0.1847,
         0.1875,  0.1904,  0.1932,  0.1960,  0.1989,  0.2017,  0.2046,  0.2074,
         0.2102,  0.2131,  0.2159,  0.2188,  0.2216,  0.2244,  0.2273,  0.2301,
         0.2330,  0.2358,  0.2387,  0.2443,  0.2472,  0.2500,  0.2529,  0.2557,
         0.2585,  0.2642,  0.2671,  0.2699,  0.2727,  0.2841,  0.2870,  0.2926,
         0.3125,  0.3267,  0.3324,  0.3495,  0.3608], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  205
---------Q_out---------
tensor([[[-0.2020,  0.0673, -0.6061,  ..., -0.7408,  1.0775, -0.3367],
         [-1.0775,  0.8081, -0.8755,  ..., -0.6734,  1.8183,  0.1347],
         [-0.6734, -2.2223,  0.1347,  ..., -0.1347, -0.2694, -1.4815],
         ...,
         [-0.2020, -0.6061, -1.1448,  ..., -0.1347,  0.6734,  0.4714],
         [-0.3367, -0.6061, -1.4815,  ..., -0.6734, -0.2020,  0.4714],
         [ 0.0673, -0.6734, -1.9529,  ..., -0.9428, -0.0673,  0.3367]],

        [[-0.0673,  0.0673, -0.3367,  ..., -0.6061,  1.1448, -0.2694],
         [ 1.0775, -0.6734,  1.4815,  ..., -0.8755,  0.9428, -2.5590],
         [-0.4714, -1.4815,  1.2122,  ...,  0.4714,  0.6061,  0.6061],
         ...,
         [-0.0673, -1.3469,  0.4041,  ..., -1.1448,  0.0673, -0.2694],
         [-0.1347,  0.8081, -0.5387,  ..., -0.8081, -0.6061,  0.2020],
         [-0.1347,  0.2694,  0.5387,  ..., -1.0101,  0.3367, -0.6734]],

        [[-0.2020,  0.0673, -0.6734,  ..., -0.7408,  1.3469, -0.4714],
         [-0.9428,  1.0101,  0.0673,  ..., -0.1347,  0.5387, -0.8081],
         [-0.2020,  0.5387,  0.4714,  ...,  0.5387,  0.7408,  1.0101],
         ...,
         [-0.2020, -0.4714, -1.3469,  ..., -0.8755,  0.7408,  0.0000],
         [-0.0673, -0.3367, -1.0775,  ..., -0.4714, -0.3367,  0.5387],
         [-0.2694, -0.7408, -1.4142,  ..., -0.1347, -0.6734,  0.1347]],

        ...,

        [[-0.1347, -0.0673, -0.4714,  ..., -0.6734,  1.1448, -0.4714],
         [-1.6836,  1.2122,  0.0673,  ..., -0.6061, -0.2020, -1.7509],
         [-0.4714,  1.9529, -0.2020,  ...,  0.1347, -0.0000,  0.4041],
         ...,
         [-0.2694, -0.8755, -1.0775,  ..., -0.5387,  0.2694,  0.0673],
         [-0.2694, -0.7408, -1.4142,  ..., -0.8081, -0.2694,  0.7408],
         [-0.1347, -0.7408, -1.3469,  ..., -0.8081,  0.1347, -0.2694]],

        [[-0.3367,  0.2020, -0.5387,  ..., -0.6734,  1.0775, -0.3367],
         [-0.0673, -0.8755, -0.7408,  ...,  0.1347,  0.8081, -2.7611],
         [-1.2122, -0.4714,  0.0000,  ...,  0.2020, -0.6734,  0.2694],
         ...,
         [-0.2020, -0.5387, -1.3469,  ..., -0.6061,  0.1347,  0.0673],
         [-0.1347, -0.7408, -1.6162,  ..., -1.1448, -0.2694,  0.6734],
         [-0.2694, -0.4714, -1.8856,  ..., -0.6061,  0.1347,  0.2020]],

        [[-0.1347, -0.2694, -0.4041,  ..., -0.8081,  1.2795, -0.3367],
         [ 0.3367, -1.0101,  1.1448,  ..., -0.1347,  1.4815, -0.8081],
         [ 0.2020, -1.0101,  1.5489,  ...,  1.9529,  0.6734, -0.2694],
         ...,
         [ 0.0000, -0.6061, -0.8755,  ..., -0.5387,  0.8081,  0.6061],
         [ 0.0673, -0.8755, -1.0101,  ..., -0.7408,  0.2694,  0.6061],
         [ 0.4714, -1.2795, -0.8755,  ..., -0.6061,  0.1347,  0.1347]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-8.5525, -8.0138, -7.9465, -7.8791, -7.7444, -7.6097, -7.3404, -7.2730,
        -7.2057, -7.1383, -7.0710, -7.0037, -6.9363, -6.8690, -6.8016, -6.7343,
        -6.6669, -6.5996, -6.5323, -6.4649, -6.3976, -6.3302, -6.2629, -6.1955,
        -6.1282, -6.0609, -5.9935, -5.9262, -5.8588, -5.7915, -5.7241, -5.6568,
        -5.5895, -5.5221, -5.4548, -5.3874, -5.3201, -5.2527, -5.1854, -5.1181,
        -5.0507, -4.9834, -4.9160, -4.8487, -4.7813, -4.7140, -4.6467, -4.5793,
        -4.5120, -4.4446, -4.3773, -4.3099, -4.2426, -4.1753, -4.1079, -4.0406,
        -3.9732, -3.9059, -3.8385, -3.7712, -3.7039, -3.6365, -3.5692, -3.5018,
        -3.4345, -3.3671, -3.2998, -3.2325, -3.1651, -3.0978, -3.0304, -2.9631,
        -2.8957, -2.8284, -2.7611, -2.6937, -2.6264, -2.5590, -2.4917, -2.4243,
        -2.3570, -2.2897, -2.2223, -2.1550, -2.0876, -2.0203, -1.9529, -1.8856,
        -1.8183, -1.7509, -1.6836, -1.6162, -1.5489, -1.4815, -1.4142, -1.3469,
        -1.2795, -1.2122, -1.1448, -1.0775, -1.0101, -0.9428, -0.8755, -0.8081,
        -0.7408, -0.6734, -0.6061, -0.5387, -0.4714, -0.4041, -0.3367, -0.2694,
        -0.2020, -0.1347, -0.0673, -0.0000,  0.0673,  0.1347,  0.2020,  0.2694,
         0.3367,  0.4041,  0.4714,  0.5387,  0.6061,  0.6734,  0.7408,  0.8081,
         0.8755,  0.9428,  1.0101,  1.0775,  1.1448,  1.2122,  1.2795,  1.3469,
         1.4142,  1.4815,  1.5489,  1.6162,  1.6836,  1.7509,  1.8183,  1.8856,
         1.9529,  2.0203,  2.0876,  2.1550,  2.2223,  2.2897,  2.3570,  2.4243,
         2.4917,  2.5590,  2.6264,  2.6937,  2.7611,  2.8284,  2.8957,  2.9631,
         3.0304,  3.0978,  3.1651,  3.2325,  3.2998,  3.3671,  3.4345,  3.5018,
         3.5692,  3.6365,  3.7039,  3.7712,  3.8385,  3.9059,  3.9732,  4.0406,
         4.1079,  4.1753,  4.2426,  4.3099,  4.3773,  4.4446,  4.5120,  4.5793,
         4.6467,  4.7140,  4.7813,  4.8487,  4.9160,  4.9834,  5.0507,  5.1181,
         5.1854,  5.2527,  5.3201,  5.3874,  5.4548,  5.5221,  5.5895,  5.6568,
         5.7241,  5.7915,  5.8588,  5.9262,  5.9935,  6.0609,  6.1282,  6.2629,
         6.3302], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  209
name:  bert.encoder.layer.1.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0047,  0.0116,  0.0171,  ..., -0.0094,  0.0590,  0.0623],
        [ 0.0667, -0.0023, -0.0304,  ..., -0.0289, -0.0318,  0.0153],
        [-0.0098, -0.0100,  0.0334,  ...,  0.0169,  0.0316, -0.0068],
        ...,
        [-0.0042, -0.0099,  0.0155,  ...,  0.0386, -0.0206,  0.0110],
        [-0.0291, -0.0335, -0.0436,  ..., -0.0021,  0.0213,  0.0146],
        [-0.0372, -0.0088,  0.0156,  ..., -0.0063,  0.0408,  0.0105]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0050,  0.0117,  0.0168,  ..., -0.0101,  0.0587,  0.0620],
        [ 0.0671, -0.0017, -0.0302,  ..., -0.0285, -0.0319,  0.0151],
        [-0.0101, -0.0101,  0.0335,  ...,  0.0168,  0.0319, -0.0067],
        ...,
        [-0.0050, -0.0101,  0.0151,  ...,  0.0386, -0.0201,  0.0117],
        [-0.0285, -0.0335, -0.0436,  ..., -0.0017,  0.0218,  0.0151],
        [-0.0369, -0.0084,  0.0151,  ..., -0.0067,  0.0402,  0.0101]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2129, -0.1911, -0.1895, -0.1811, -0.1593, -0.1576, -0.1559, -0.1543,
        -0.1459, -0.1425, -0.1408, -0.1375, -0.1358, -0.1341, -0.1325, -0.1308,
        -0.1291, -0.1274, -0.1258, -0.1241, -0.1224, -0.1207, -0.1190, -0.1174,
        -0.1157, -0.1140, -0.1123, -0.1107, -0.1090, -0.1073, -0.1056, -0.1040,
        -0.1023, -0.1006, -0.0989, -0.0973, -0.0956, -0.0939, -0.0922, -0.0905,
        -0.0889, -0.0872, -0.0855, -0.0838, -0.0822, -0.0805, -0.0788, -0.0771,
        -0.0755, -0.0738, -0.0721, -0.0704, -0.0687, -0.0671, -0.0654, -0.0637,
        -0.0620, -0.0604, -0.0587, -0.0570, -0.0553, -0.0537, -0.0520, -0.0503,
        -0.0486, -0.0469, -0.0453, -0.0436, -0.0419, -0.0402, -0.0386, -0.0369,
        -0.0352, -0.0335, -0.0319, -0.0302, -0.0285, -0.0268, -0.0252, -0.0235,
        -0.0218, -0.0201, -0.0184, -0.0168, -0.0151, -0.0134, -0.0117, -0.0101,
        -0.0084, -0.0067, -0.0050, -0.0034, -0.0017, -0.0000,  0.0017,  0.0034,
         0.0050,  0.0067,  0.0084,  0.0101,  0.0117,  0.0134,  0.0151,  0.0168,
         0.0184,  0.0201,  0.0218,  0.0235,  0.0252,  0.0268,  0.0285,  0.0302,
         0.0319,  0.0335,  0.0352,  0.0369,  0.0386,  0.0402,  0.0419,  0.0436,
         0.0453,  0.0469,  0.0486,  0.0503,  0.0520,  0.0537,  0.0553,  0.0570,
         0.0587,  0.0604,  0.0620,  0.0637,  0.0654,  0.0671,  0.0687,  0.0704,
         0.0721,  0.0738,  0.0755,  0.0771,  0.0788,  0.0805,  0.0822,  0.0838,
         0.0855,  0.0872,  0.0889,  0.0905,  0.0922,  0.0939,  0.0956,  0.0973,
         0.0989,  0.1006,  0.1023,  0.1040,  0.1056,  0.1073,  0.1090,  0.1107,
         0.1123,  0.1140,  0.1157,  0.1174,  0.1190,  0.1207,  0.1224,  0.1241,
         0.1258,  0.1274,  0.1291,  0.1308,  0.1325,  0.1341,  0.1358,  0.1375,
         0.1392,  0.1408,  0.1425,  0.1492,  0.1509,  0.1677,  0.1710,  0.1928],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  184
---------Q_out---------
None
name:  bert.encoder.layer.1.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0139,  0.0344, -0.0356,  ..., -0.0199, -0.0271,  0.0085],
        [-0.0111,  0.0200, -0.0045,  ...,  0.0318, -0.0049, -0.0286],
        [ 0.0129, -0.0123, -0.0477,  ...,  0.0115, -0.0240,  0.0181],
        ...,
        [-0.0173, -0.0412,  0.0689,  ..., -0.0136, -0.0149, -0.0339],
        [-0.0090,  0.0005,  0.0266,  ..., -0.0159, -0.0211,  0.0134],
        [-0.0140,  0.0025, -0.0068,  ...,  0.0103,  0.0208, -0.0199]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0142,  0.0341, -0.0370,  ..., -0.0199, -0.0284,  0.0085],
        [-0.0114,  0.0199, -0.0057,  ...,  0.0313, -0.0057, -0.0284],
        [ 0.0142, -0.0114, -0.0483,  ...,  0.0114, -0.0227,  0.0171],
        ...,
        [-0.0171, -0.0398,  0.0682,  ..., -0.0142, -0.0142, -0.0341],
        [-0.0085,  0.0000,  0.0256,  ..., -0.0171, -0.0199,  0.0142],
        [-0.0142,  0.0028, -0.0057,  ...,  0.0114,  0.0199, -0.0199]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3099, -0.2787, -0.2730, -0.2673, -0.2587, -0.2303, -0.2275, -0.2246,
        -0.2218, -0.2189, -0.2161, -0.2133, -0.2104, -0.2076, -0.2019, -0.1990,
        -0.1962, -0.1933, -0.1905, -0.1877, -0.1848, -0.1820, -0.1791, -0.1763,
        -0.1734, -0.1706, -0.1678, -0.1649, -0.1621, -0.1592, -0.1564, -0.1535,
        -0.1507, -0.1479, -0.1450, -0.1422, -0.1393, -0.1365, -0.1336, -0.1308,
        -0.1280, -0.1251, -0.1223, -0.1194, -0.1166, -0.1137, -0.1109, -0.1080,
        -0.1052, -0.1024, -0.0995, -0.0967, -0.0938, -0.0910, -0.0881, -0.0853,
        -0.0825, -0.0796, -0.0768, -0.0739, -0.0711, -0.0682, -0.0654, -0.0626,
        -0.0597, -0.0569, -0.0540, -0.0512, -0.0483, -0.0455, -0.0427, -0.0398,
        -0.0370, -0.0341, -0.0313, -0.0284, -0.0256, -0.0227, -0.0199, -0.0171,
        -0.0142, -0.0114, -0.0085, -0.0057, -0.0028, -0.0000,  0.0028,  0.0057,
         0.0085,  0.0114,  0.0142,  0.0171,  0.0199,  0.0227,  0.0256,  0.0284,
         0.0313,  0.0341,  0.0370,  0.0398,  0.0427,  0.0455,  0.0483,  0.0512,
         0.0540,  0.0569,  0.0597,  0.0626,  0.0654,  0.0682,  0.0711,  0.0739,
         0.0768,  0.0796,  0.0825,  0.0853,  0.0881,  0.0910,  0.0938,  0.0967,
         0.0995,  0.1024,  0.1052,  0.1080,  0.1109,  0.1137,  0.1166,  0.1194,
         0.1223,  0.1251,  0.1280,  0.1308,  0.1336,  0.1365,  0.1393,  0.1422,
         0.1450,  0.1479,  0.1507,  0.1535,  0.1564,  0.1592,  0.1621,  0.1649,
         0.1678,  0.1706,  0.1734,  0.1763,  0.1791,  0.1820,  0.1848,  0.1877,
         0.1905,  0.1933,  0.1962,  0.1990,  0.2019,  0.2047,  0.2076,  0.2104,
         0.2133,  0.2218,  0.2246,  0.2332,  0.2388,  0.2417,  0.2474,  0.2502,
         0.2559,  0.2701,  0.2929,  0.3014,  0.3611], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  173
---------Q_out---------
None
name:  bert.encoder.layer.1.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0183,  0.0197,  0.0188,  ...,  0.0750,  0.0155,  0.0288],
        [-0.0265,  0.0063,  0.0228,  ...,  0.0049, -0.0040,  0.0008],
        [-0.0970,  0.0350, -0.0152,  ..., -0.0276, -0.0033,  0.0520],
        ...,
        [-0.0191,  0.0457,  0.0584,  ...,  0.0011,  0.0350, -0.0020],
        [ 0.0182,  0.0584,  0.0220,  ..., -0.0065,  0.0417,  0.0234],
        [-0.0645,  0.0462,  0.0117,  ..., -0.0166, -0.0001, -0.0626]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0199,  0.0199,  0.0199,  ...,  0.0757,  0.0159,  0.0279],
        [-0.0279,  0.0080,  0.0239,  ...,  0.0040, -0.0040,  0.0000],
        [-0.0956,  0.0359, -0.0159,  ..., -0.0279, -0.0040,  0.0518],
        ...,
        [-0.0199,  0.0438,  0.0598,  ...,  0.0000,  0.0359, -0.0040],
        [ 0.0199,  0.0598,  0.0239,  ..., -0.0080,  0.0398,  0.0239],
        [-0.0638,  0.0478,  0.0120,  ..., -0.0159, -0.0000, -0.0638]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3108, -0.2989, -0.2949, -0.2789, -0.2749, -0.2670, -0.2630, -0.2590,
        -0.2510, -0.2471, -0.2391, -0.2351, -0.2311, -0.2271, -0.2231, -0.2192,
        -0.2152, -0.2112, -0.2072, -0.2032, -0.1992, -0.1953, -0.1913, -0.1873,
        -0.1833, -0.1793, -0.1753, -0.1713, -0.1674, -0.1634, -0.1594, -0.1554,
        -0.1514, -0.1474, -0.1435, -0.1395, -0.1355, -0.1315, -0.1275, -0.1235,
        -0.1195, -0.1156, -0.1116, -0.1076, -0.1036, -0.0996, -0.0956, -0.0916,
        -0.0877, -0.0837, -0.0797, -0.0757, -0.0717, -0.0677, -0.0638, -0.0598,
        -0.0558, -0.0518, -0.0478, -0.0438, -0.0398, -0.0359, -0.0319, -0.0279,
        -0.0239, -0.0199, -0.0159, -0.0120, -0.0080, -0.0040, -0.0000,  0.0040,
         0.0080,  0.0120,  0.0159,  0.0199,  0.0239,  0.0279,  0.0319,  0.0359,
         0.0398,  0.0438,  0.0478,  0.0518,  0.0558,  0.0598,  0.0638,  0.0677,
         0.0717,  0.0757,  0.0797,  0.0837,  0.0877,  0.0916,  0.0956,  0.0996,
         0.1036,  0.1076,  0.1116,  0.1156,  0.1195,  0.1235,  0.1275,  0.1315,
         0.1355,  0.1395,  0.1435,  0.1474,  0.1514,  0.1554,  0.1594,  0.1634,
         0.1674,  0.1713,  0.1753,  0.1793,  0.1833,  0.1873,  0.1913,  0.1953,
         0.1992,  0.2032,  0.2072,  0.2112,  0.2152,  0.2192,  0.2231,  0.2271,
         0.2351,  0.2391,  0.2431,  0.2471,  0.2510,  0.2550,  0.2630,  0.2670,
         0.2710,  0.2789,  0.3108,  0.3228,  0.3268,  0.3347,  0.3387,  0.3467,
         0.3507,  0.4064,  0.4104,  0.5061], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  148
---------Q_out---------
None
name:  bert.encoder.layer.1.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0238,  0.0090,  0.0387,  ..., -0.0289,  0.0252,  0.0563],
        [ 0.0332,  0.0035,  0.0402,  ...,  0.0281,  0.0128, -0.0012],
        [ 0.0829,  0.0036, -0.0002,  ..., -0.0174, -0.0156,  0.0262],
        ...,
        [ 0.0083, -0.0078, -0.0275,  ...,  0.0032,  0.0151,  0.0413],
        [ 0.0209, -0.0119, -0.0752,  ...,  0.0358, -0.0540, -0.0316],
        [ 0.0110,  0.0170, -0.0114,  ..., -0.0217, -0.0191, -0.0415]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0169,  0.0169,  0.0339,  ..., -0.0339,  0.0169,  0.0508],
        [ 0.0339,  0.0000,  0.0339,  ...,  0.0339,  0.0169, -0.0000],
        [ 0.0847,  0.0000, -0.0000,  ..., -0.0169, -0.0169,  0.0339],
        ...,
        [ 0.0000, -0.0000, -0.0339,  ...,  0.0000,  0.0169,  0.0339],
        [ 0.0169, -0.0169, -0.0678,  ...,  0.0339, -0.0508, -0.0339],
        [ 0.0169,  0.0169, -0.0169,  ..., -0.0169, -0.0169, -0.0339]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-2.1522, -2.1353, -1.8811, -1.8302, -1.6947, -1.6438, -1.5083, -1.4405,
        -1.4066, -1.3388, -1.3049, -1.2710, -1.1524, -1.1354, -1.1185, -1.0676,
        -1.0337, -0.9490, -0.9151, -0.8643, -0.8473, -0.8304, -0.7965, -0.7795,
        -0.7626, -0.7457, -0.7118, -0.6948, -0.6779, -0.6440, -0.5931, -0.5592,
        -0.5423, -0.5253, -0.4915, -0.4745, -0.4406, -0.4237, -0.4067, -0.3898,
        -0.3728, -0.3559, -0.3389, -0.3220, -0.3050, -0.2881, -0.2711, -0.2542,
        -0.2373, -0.2203, -0.2034, -0.1864, -0.1695, -0.1525, -0.1356, -0.1186,
        -0.1017, -0.0847, -0.0678, -0.0508, -0.0339, -0.0169, -0.0000,  0.0169,
         0.0339,  0.0508,  0.0678,  0.0847,  0.1017,  0.1186,  0.1356,  0.1525,
         0.1695,  0.1864,  0.2034,  0.2203,  0.2373,  0.2542,  0.2711,  0.2881,
         0.3050,  0.3220,  0.3389,  0.3728,  0.3898,  0.4067,  0.4237,  0.4406,
         0.5253,  0.5423,  0.5931,  0.6609,  0.6779,  0.7118,  0.7626,  0.7965,
         0.8304,  0.8643,  0.8812,  1.0337,  1.1693,  1.1863], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  102
---------Q_out---------
None
name:  bert.encoder.layer.2.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0843,  0.0146, -0.0630,  ...,  0.0154, -0.0917, -0.0197],
        [ 0.0957,  0.0032, -0.0572,  ...,  0.0732, -0.0140, -0.0302],
        [ 0.0754,  0.0280, -0.0567,  ...,  0.0433, -0.0351,  0.0510],
        ...,
        [ 0.0244,  0.0878, -0.0443,  ...,  0.0082, -0.0465, -0.0238],
        [-0.0024, -0.0261,  0.0061,  ..., -0.0297, -0.0271, -0.0280],
        [-0.0126, -0.0188,  0.0386,  ..., -0.0164, -0.0436,  0.0492]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0832,  0.0166, -0.0624,  ...,  0.0166, -0.0915, -0.0208],
        [ 0.0957,  0.0042, -0.0582,  ...,  0.0749, -0.0125, -0.0291],
        [ 0.0749,  0.0291, -0.0582,  ...,  0.0416, -0.0333,  0.0499],
        ...,
        [ 0.0250,  0.0873, -0.0458,  ...,  0.0083, -0.0458, -0.0250],
        [-0.0042, -0.0250,  0.0042,  ..., -0.0291, -0.0291, -0.0291],
        [-0.0125, -0.0208,  0.0374,  ..., -0.0166, -0.0416,  0.0499]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.5282, -0.4575, -0.4409, -0.4367, -0.4118, -0.3993, -0.3868, -0.3826,
        -0.3785, -0.3743, -0.3702, -0.3660, -0.3619, -0.3577, -0.3535, -0.3494,
        -0.3452, -0.3411, -0.3369, -0.3327, -0.3286, -0.3244, -0.3203, -0.3161,
        -0.3119, -0.3078, -0.3036, -0.2995, -0.2953, -0.2911, -0.2870, -0.2828,
        -0.2787, -0.2745, -0.2703, -0.2662, -0.2620, -0.2579, -0.2537, -0.2496,
        -0.2454, -0.2412, -0.2371, -0.2329, -0.2288, -0.2246, -0.2204, -0.2163,
        -0.2121, -0.2080, -0.2038, -0.1996, -0.1955, -0.1913, -0.1872, -0.1830,
        -0.1788, -0.1747, -0.1705, -0.1664, -0.1622, -0.1581, -0.1539, -0.1497,
        -0.1456, -0.1414, -0.1373, -0.1331, -0.1289, -0.1248, -0.1206, -0.1165,
        -0.1123, -0.1081, -0.1040, -0.0998, -0.0957, -0.0915, -0.0873, -0.0832,
        -0.0790, -0.0749, -0.0707, -0.0665, -0.0624, -0.0582, -0.0541, -0.0499,
        -0.0458, -0.0416, -0.0374, -0.0333, -0.0291, -0.0250, -0.0208, -0.0166,
        -0.0125, -0.0083, -0.0042, -0.0000,  0.0042,  0.0083,  0.0125,  0.0166,
         0.0208,  0.0250,  0.0291,  0.0333,  0.0374,  0.0416,  0.0458,  0.0499,
         0.0541,  0.0582,  0.0624,  0.0665,  0.0707,  0.0749,  0.0790,  0.0832,
         0.0873,  0.0915,  0.0957,  0.0998,  0.1040,  0.1081,  0.1123,  0.1165,
         0.1206,  0.1248,  0.1289,  0.1331,  0.1373,  0.1414,  0.1456,  0.1497,
         0.1539,  0.1581,  0.1622,  0.1664,  0.1705,  0.1747,  0.1788,  0.1830,
         0.1872,  0.1913,  0.1955,  0.1996,  0.2038,  0.2080,  0.2121,  0.2163,
         0.2204,  0.2246,  0.2288,  0.2329,  0.2371,  0.2412,  0.2454,  0.2496,
         0.2537,  0.2579,  0.2620,  0.2662,  0.2703,  0.2745,  0.2787,  0.2828,
         0.2870,  0.2911,  0.2953,  0.2995,  0.3036,  0.3078,  0.3119,  0.3161,
         0.3203,  0.3244,  0.3327,  0.3411,  0.3535,  0.3577,  0.3660,  0.3702,
         0.3743,  0.3785,  0.3826,  0.3868,  0.4076,  0.4118,  0.4159,  0.4201,
         0.4409,  0.4450,  0.4492,  0.4534,  0.4575,  0.5116,  0.5199],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  199
---------Q_out---------
tensor([[[ 0.6998,  1.2246,  0.5248,  ..., -1.0496,  1.4870,  0.0000],
         [-1.0496,  1.9243,  2.7115,  ...,  0.1749, -1.5744,  0.0875],
         [-4.6359, -0.2624, -1.7494,  ..., -0.1749, -1.6619,  0.7872],
         ...,
         [-0.1749,  0.5248,  2.2742,  ..., -1.6619,  0.3499,  0.7872],
         [-2.0993,  0.5248,  0.2624,  ..., -1.4870,  0.9622,  0.5248],
         [-0.1749,  2.8865,  0.4373,  ..., -1.6619,  1.2246,  0.2624]],

        [[ 0.0000,  0.8747,  0.0875,  ..., -1.1371,  1.3120,  0.0000],
         [-0.0875,  3.3238,  0.9622,  ..., -0.1749,  0.0875, -0.0000],
         [-4.3735,  1.8369, -1.7494,  ..., -0.6998,  0.4373,  1.5744],
         ...,
         [ 0.9622,  2.9740,  0.5248,  ...,  0.1749,  0.7872,  1.3120],
         [-1.8369,  2.4491, -0.5248,  ..., -0.0875,  1.5744, -0.9622],
         [ 0.7872,  2.7990,  0.5248,  ..., -1.1371,  1.8369, -0.6123]],

        [[ 0.2624,  1.4870,  0.0875,  ..., -1.3995,  1.5744, -0.0875],
         [-0.4373,  0.3499,  0.8747,  ...,  0.9622, -0.9622, -0.1749],
         [-2.2742, -1.3120, -1.3995,  ..., -1.3120, -0.1749, -0.2624],
         ...,
         [ 0.3499,  2.1867,  3.1489,  ..., -2.0993,  1.3120,  0.6998],
         [-1.3995,  0.7872,  0.0875,  ..., -1.9243,  1.3120,  0.6998],
         [ 0.0000,  1.7494,  1.0496,  ..., -1.7494,  0.8747,  0.4373]],

        ...,

        [[ 0.6123,  1.3120,  0.0875,  ..., -1.1371,  1.5744,  0.0875],
         [ 0.0875,  1.1371,  1.0496,  ...,  0.2624,  2.0993,  0.0875],
         [-2.8865, -0.7872, -1.6619,  ..., -0.8747, -0.6123, -0.5248],
         ...,
         [ 0.9622,  0.5248,  3.0614,  ..., -1.8369,  1.0496,  1.3120],
         [-1.6619,  0.2624, -0.5248,  ..., -1.4870,  1.2246,  0.6998],
         [ 0.4373,  2.5366,  0.9622,  ..., -1.3995,  1.2246,  0.0000]],

        [[ 0.5248,  1.1371,  0.1749,  ..., -1.5744,  1.4870, -0.0875],
         [-1.8369,  2.0118,  1.4870,  ..., -0.6998, -0.0875, -0.3499],
         [-3.4988,  1.3120, -2.7115,  ..., -1.7494,  0.9622, -0.6998],
         ...,
         [ 0.1749,  1.3120,  2.5366,  ..., -1.2246,  1.3995,  0.9622],
         [-1.3120,  0.7872, -0.0000,  ..., -1.2246,  1.2246,  0.7872],
         [-0.3499,  2.1867,  0.7872,  ..., -1.2246,  0.6998,  0.2624]],

        [[ 0.4373,  0.9622,  0.1749,  ..., -1.2246,  1.4870,  0.2624],
         [-1.2246,  1.0496,  2.1867,  ..., -0.4373, -0.2624, -0.3499],
         [-6.1228, -1.3995, -2.3617,  ..., -0.7872, -0.6123, -1.3995],
         ...,
         [ 0.4373,  0.9622,  3.1489,  ..., -2.0993,  0.8747,  1.0496],
         [-2.0118, -0.0000,  0.2624,  ..., -1.5744,  0.6998,  1.0496],
         [-0.6998,  2.4491,  1.4870,  ..., -1.1371,  1.0496,  0.5248]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-11.1086, -10.3214,  -9.7966,  -9.6216,  -9.1843,  -8.8344,  -8.7469,
         -8.6595,  -8.5720,  -8.4845,  -8.3970,  -8.3096,  -8.2221,  -8.1346,
         -8.0472,  -7.9597,  -7.8722,  -7.7848,  -7.6973,  -7.6098,  -7.5224,
         -7.4349,  -7.3474,  -7.2599,  -7.1725,  -7.0850,  -6.9975,  -6.9101,
         -6.8226,  -6.7351,  -6.6477,  -6.5602,  -6.4727,  -6.3853,  -6.2978,
         -6.2103,  -6.1228,  -6.0354,  -5.9479,  -5.8604,  -5.7730,  -5.6855,
         -5.5980,  -5.5106,  -5.4231,  -5.3356,  -5.2482,  -5.1607,  -5.0732,
         -4.9857,  -4.8983,  -4.8108,  -4.7233,  -4.6359,  -4.5484,  -4.4609,
         -4.3735,  -4.2860,  -4.1985,  -4.1111,  -4.0236,  -3.9361,  -3.8486,
         -3.7612,  -3.6737,  -3.5862,  -3.4988,  -3.4113,  -3.3238,  -3.2364,
         -3.1489,  -3.0614,  -2.9740,  -2.8865,  -2.7990,  -2.7115,  -2.6241,
         -2.5366,  -2.4491,  -2.3617,  -2.2742,  -2.1867,  -2.0993,  -2.0118,
         -1.9243,  -1.8369,  -1.7494,  -1.6619,  -1.5744,  -1.4870,  -1.3995,
         -1.3120,  -1.2246,  -1.1371,  -1.0496,  -0.9622,  -0.8747,  -0.7872,
         -0.6998,  -0.6123,  -0.5248,  -0.4373,  -0.3499,  -0.2624,  -0.1749,
         -0.0875,  -0.0000,   0.0875,   0.1749,   0.2624,   0.3499,   0.4373,
          0.5248,   0.6123,   0.6998,   0.7872,   0.8747,   0.9622,   1.0496,
          1.1371,   1.2246,   1.3120,   1.3995,   1.4870,   1.5744,   1.6619,
          1.7494,   1.8369,   1.9243,   2.0118,   2.0993,   2.1867,   2.2742,
          2.3617,   2.4491,   2.5366,   2.6241,   2.7115,   2.7990,   2.8865,
          2.9740,   3.0614,   3.1489,   3.2364,   3.3238,   3.4113,   3.4988,
          3.5862,   3.6737,   3.7612,   3.8486,   3.9361,   4.0236,   4.1111,
          4.1985,   4.2860,   4.3735,   4.4609,   4.5484,   4.6359,   4.7233,
          4.8108,   4.8983,   4.9857,   5.0732,   5.1607,   5.2482,   5.3356,
          5.4231,   5.5106,   5.5980,   5.6855,   5.7730,   5.8604,   5.9479,
          6.0354,   6.1228,   6.2103,   6.2978,   6.3853,   6.4727,   6.5602,
          6.6477,   6.7351,   6.8226,   6.9101,   6.9975,   7.0850,   7.1725,
          7.2599,   7.3474,   7.4349,   7.5224,   7.6098,   7.6973,   7.7848,
          7.8722,   7.9597,   8.0472,   8.1346,   8.2221,   8.3096,   8.3970,
          8.4845,   8.5720,   8.6595,   8.7469,   8.8344,   9.0968,   9.1843,
          9.2717,   9.3592,   9.4467], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  213
name:  bert.encoder.layer.2.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0480,  0.0821,  0.0674,  ..., -0.0634,  0.0961, -0.0344],
        [ 0.0252,  0.0296, -0.0403,  ...,  0.1094,  0.0623,  0.0129],
        [-0.0131,  0.0469, -0.0055,  ..., -0.0435,  0.0612,  0.0181],
        ...,
        [-0.0492,  0.0560, -0.0345,  ..., -0.0198,  0.0031, -0.0114],
        [ 0.0217, -0.0423,  0.0365,  ...,  0.0129,  0.0513, -0.0119],
        [ 0.0031,  0.0865, -0.0528,  ..., -0.0714,  0.0320,  0.0424]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0471,  0.0834,  0.0689,  ..., -0.0616,  0.0979, -0.0326],
        [ 0.0254,  0.0290, -0.0399,  ...,  0.1087,  0.0616,  0.0145],
        [-0.0145,  0.0471, -0.0072,  ..., -0.0435,  0.0616,  0.0181],
        ...,
        [-0.0507,  0.0544, -0.0362,  ..., -0.0181,  0.0036, -0.0109],
        [ 0.0217, -0.0435,  0.0362,  ...,  0.0145,  0.0507, -0.0109],
        [ 0.0036,  0.0870, -0.0544,  ..., -0.0725,  0.0326,  0.0435]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.4566, -0.4458, -0.4059, -0.3878, -0.3842, -0.3805, -0.3769, -0.3733,
        -0.3697, -0.3660, -0.3624, -0.3515, -0.3479, -0.3443, -0.3407, -0.3370,
        -0.3334, -0.3298, -0.3262, -0.3226, -0.3189, -0.3153, -0.3117, -0.3081,
        -0.3044, -0.3008, -0.2972, -0.2936, -0.2899, -0.2863, -0.2827, -0.2791,
        -0.2754, -0.2718, -0.2682, -0.2646, -0.2609, -0.2573, -0.2537, -0.2501,
        -0.2464, -0.2428, -0.2392, -0.2356, -0.2319, -0.2283, -0.2247, -0.2211,
        -0.2175, -0.2138, -0.2102, -0.2066, -0.2030, -0.1993, -0.1957, -0.1921,
        -0.1885, -0.1848, -0.1812, -0.1776, -0.1740, -0.1703, -0.1667, -0.1631,
        -0.1595, -0.1558, -0.1522, -0.1486, -0.1450, -0.1413, -0.1377, -0.1341,
        -0.1305, -0.1268, -0.1232, -0.1196, -0.1160, -0.1123, -0.1087, -0.1051,
        -0.1015, -0.0979, -0.0942, -0.0906, -0.0870, -0.0834, -0.0797, -0.0761,
        -0.0725, -0.0689, -0.0652, -0.0616, -0.0580, -0.0544, -0.0507, -0.0471,
        -0.0435, -0.0399, -0.0362, -0.0326, -0.0290, -0.0254, -0.0217, -0.0181,
        -0.0145, -0.0109, -0.0072, -0.0036, -0.0000,  0.0036,  0.0072,  0.0109,
         0.0145,  0.0181,  0.0217,  0.0254,  0.0290,  0.0326,  0.0362,  0.0399,
         0.0435,  0.0471,  0.0507,  0.0544,  0.0580,  0.0616,  0.0652,  0.0689,
         0.0725,  0.0761,  0.0797,  0.0834,  0.0870,  0.0906,  0.0942,  0.0979,
         0.1015,  0.1051,  0.1087,  0.1123,  0.1160,  0.1196,  0.1232,  0.1268,
         0.1305,  0.1341,  0.1377,  0.1413,  0.1450,  0.1486,  0.1522,  0.1558,
         0.1595,  0.1631,  0.1667,  0.1703,  0.1740,  0.1776,  0.1812,  0.1848,
         0.1885,  0.1921,  0.1957,  0.1993,  0.2030,  0.2066,  0.2102,  0.2138,
         0.2175,  0.2211,  0.2247,  0.2283,  0.2319,  0.2356,  0.2392,  0.2428,
         0.2464,  0.2501,  0.2537,  0.2573,  0.2609,  0.2646,  0.2682,  0.2718,
         0.2754,  0.2791,  0.2827,  0.2863,  0.2899,  0.2936,  0.2972,  0.3008,
         0.3044,  0.3081,  0.3117,  0.3153,  0.3189,  0.3226,  0.3262,  0.3298,
         0.3334,  0.3370,  0.3407,  0.3443,  0.3479,  0.3515,  0.3552,  0.3624,
         0.3660,  0.3697,  0.3733,  0.3769,  0.3805,  0.3842,  0.3914,  0.3950,
         0.4023,  0.4059,  0.4095,  0.4204,  0.4385,  0.4494,  0.4603],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  223
---------Q_out---------
tensor([[[ 0.3688,  1.9915,  0.9589,  ..., -1.4752,  1.4752, -0.3688],
         [ 0.4426,  1.4752, -1.0326,  ...,  1.5489, -2.5078,  1.7702],
         [-0.2213,  0.2213, -0.7376,  ..., -0.0738, -2.5815,  1.1801],
         ...,
         [-1.4014,  2.2865, -0.5163,  ..., -1.5489,  0.0000,  1.3277],
         [-2.6553,  2.2128, -1.6964,  ..., -1.7702,  0.1475,  0.6638],
         [-4.7943,  1.6964, -4.2780,  ..., -1.3277, -0.4426,  1.4752]],

        [[-0.1475,  1.9177,  1.2539,  ..., -1.4014,  1.5489, -0.2950],
         [-0.0738,  2.1390, -1.9177,  ...,  0.5163, -1.6227, -0.7376],
         [-1.2539,  1.8440, -1.9177,  ...,  0.7376, -3.0979,  0.1475],
         ...,
         [ 1.3277,  2.5078, -1.0326,  ..., -1.7702,  1.0326, -1.7702],
         [-0.2950,  3.9092, -1.1064,  ..., -1.3277,  0.6638, -0.6638],
         [-0.5163,  2.3603, -0.5901,  ..., -0.3688,  0.6638, -0.5163]],

        [[ 0.3688,  1.9915,  1.4752,  ..., -1.4014,  1.5489, -0.3688],
         [ 0.0000,  1.2539,  0.5163,  ...,  1.0326, -0.1475,  1.0326],
         [-1.3277, -0.0738,  0.8851,  ...,  0.3688, -0.8851,  0.5901],
         ...,
         [-1.7702,  2.5815, -0.2213,  ..., -1.8440,  0.2213,  1.4014],
         [-2.0652,  2.4340, -0.9589,  ..., -1.5489,  0.2950,  0.5163],
         [-5.3106,  0.3688, -3.4666,  ..., -1.4014, -0.0000,  0.4426]],

        ...,

        [[ 0.4426,  2.0652,  1.0326,  ..., -1.3277,  1.5489, -0.2950],
         [-0.2950, -0.1475,  0.7376,  ..., -0.1475, -1.6964,  0.8851],
         [-1.1064,  0.4426, -0.5901,  ..., -0.3688, -0.2213,  0.5901],
         ...,
         [-1.4752,  2.1390, -0.8851,  ..., -1.5489, -0.3688,  1.0326],
         [-2.8028,  2.8028, -1.3277,  ..., -1.0326,  0.1475,  0.5901],
         [-4.5730,  2.1390, -3.2454,  ..., -1.2539, -0.0738,  0.3688]],

        [[ 0.5901,  1.6227,  1.2539,  ..., -1.5489,  1.6964, -0.5163],
         [ 1.0326,  0.2213,  1.3277,  ...,  0.4426, -0.8851, -0.0000],
         [-1.7702,  2.2128,  1.4752,  ..., -0.6638, -0.9589, -0.2950],
         ...,
         [-1.5489,  1.3277, -1.2539,  ..., -1.1064,  0.3688,  0.5901],
         [-1.8440,  2.2865, -1.1801,  ..., -0.5901,  0.2213, -0.0738],
         [-4.0567,  2.1390, -3.5404,  ..., -0.9589,  0.8113,  0.2213]],

        [[ 0.4426,  1.9915,  1.0326,  ..., -1.4014,  1.5489, -0.3688],
         [-0.1475,  1.0326,  0.0738,  ...,  0.2950, -0.5901,  0.8113],
         [-2.2865,  2.0652, -1.9915,  ..., -1.1801, -2.9503, -0.2213],
         ...,
         [-1.5489,  2.1390, -0.2213,  ..., -1.5489, -0.0738,  1.4014],
         [-2.2865,  2.2865, -0.9589,  ..., -1.3277, -0.1475,  1.5489],
         [-4.5730,  2.1390, -3.4666,  ..., -1.6964,  0.4426,  1.0326]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-8.0397, -7.8184, -7.7446, -7.6709, -7.5971, -7.5234, -7.4496, -7.3758,
        -7.3021, -7.2283, -7.1546, -7.0808, -7.0071, -6.9333, -6.8595, -6.7858,
        -6.7120, -6.6383, -6.5645, -6.4907, -6.4170, -6.3432, -6.2695, -6.1957,
        -6.1220, -6.0482, -5.9744, -5.9007, -5.8269, -5.7532, -5.6794, -5.6056,
        -5.5319, -5.4581, -5.3844, -5.3106, -5.2369, -5.1631, -5.0893, -5.0156,
        -4.9418, -4.8681, -4.7943, -4.7205, -4.6468, -4.5730, -4.4993, -4.4255,
        -4.3517, -4.2780, -4.2042, -4.1305, -4.0567, -3.9830, -3.9092, -3.8354,
        -3.7617, -3.6879, -3.6142, -3.5404, -3.4666, -3.3929, -3.3191, -3.2454,
        -3.1716, -3.0979, -3.0241, -2.9503, -2.8766, -2.8028, -2.7291, -2.6553,
        -2.5815, -2.5078, -2.4340, -2.3603, -2.2865, -2.2128, -2.1390, -2.0652,
        -1.9915, -1.9177, -1.8440, -1.7702, -1.6964, -1.6227, -1.5489, -1.4752,
        -1.4014, -1.3277, -1.2539, -1.1801, -1.1064, -1.0326, -0.9589, -0.8851,
        -0.8113, -0.7376, -0.6638, -0.5901, -0.5163, -0.4426, -0.3688, -0.2950,
        -0.2213, -0.1475, -0.0738, -0.0000,  0.0738,  0.1475,  0.2213,  0.2950,
         0.3688,  0.4426,  0.5163,  0.5901,  0.6638,  0.7376,  0.8113,  0.8851,
         0.9589,  1.0326,  1.1064,  1.1801,  1.2539,  1.3277,  1.4014,  1.4752,
         1.5489,  1.6227,  1.6964,  1.7702,  1.8440,  1.9177,  1.9915,  2.0652,
         2.1390,  2.2128,  2.2865,  2.3603,  2.4340,  2.5078,  2.5815,  2.6553,
         2.7291,  2.8028,  2.8766,  2.9503,  3.0241,  3.0979,  3.1716,  3.2454,
         3.3191,  3.3929,  3.4666,  3.5404,  3.6142,  3.6879,  3.7617,  3.8354,
         3.9092,  3.9830,  4.0567,  4.1305,  4.2042,  4.2780,  4.3517,  4.4255,
         4.4993,  4.5730,  4.6468,  4.7205,  4.7943,  4.8681,  4.9418,  5.0156,
         5.0893,  5.1631,  5.2369,  5.3106,  5.3844,  5.4581,  5.5319,  5.6056,
         5.6794,  5.7532,  5.8269,  5.9007,  5.9744,  6.0482,  6.1220,  6.1957,
         6.2695,  6.3432,  6.4170,  6.4907,  6.5645,  6.6383,  6.7120,  6.7858,
         6.8595,  6.9333,  7.0071,  7.0808,  7.1546,  7.2283,  7.3021,  7.3758,
         7.4496,  7.5234,  7.5971,  7.6709,  7.7446,  7.8184,  8.1134,  8.1872,
         8.2609,  8.4085,  8.5560,  8.9985,  9.0723], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  221
name:  bert.encoder.layer.2.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0111, -0.0328,  0.0220,  ..., -0.0049,  0.0040,  0.0294],
        [-0.0150, -0.0104,  0.0009,  ..., -0.0003, -0.0271, -0.0223],
        [-0.0295, -0.0034, -0.0120,  ..., -0.0100,  0.0322, -0.0101],
        ...,
        [-0.0156, -0.0329,  0.0411,  ..., -0.0045,  0.0114,  0.0031],
        [-0.0375,  0.0574, -0.0270,  ..., -0.0398,  0.0069,  0.0017],
        [-0.0252,  0.0015, -0.0537,  ...,  0.0198,  0.0238, -0.0427]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0114, -0.0326,  0.0228,  ..., -0.0049,  0.0033,  0.0294],
        [-0.0147, -0.0098,  0.0016,  ..., -0.0000, -0.0277, -0.0228],
        [-0.0294, -0.0033, -0.0114,  ..., -0.0098,  0.0326, -0.0098],
        ...,
        [-0.0163, -0.0326,  0.0408,  ..., -0.0049,  0.0114,  0.0033],
        [-0.0375,  0.0571, -0.0277,  ..., -0.0391,  0.0065,  0.0016],
        [-0.0245,  0.0016, -0.0538,  ...,  0.0196,  0.0245, -0.0424]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.1892, -0.1778, -0.1631, -0.1614, -0.1598, -0.1582, -0.1549, -0.1533,
        -0.1517, -0.1500, -0.1484, -0.1435, -0.1419, -0.1402, -0.1337, -0.1321,
        -0.1305, -0.1288, -0.1272, -0.1256, -0.1239, -0.1223, -0.1207, -0.1190,
        -0.1174, -0.1158, -0.1142, -0.1125, -0.1109, -0.1093, -0.1076, -0.1060,
        -0.1044, -0.1027, -0.1011, -0.0995, -0.0978, -0.0962, -0.0946, -0.0930,
        -0.0913, -0.0897, -0.0881, -0.0864, -0.0848, -0.0832, -0.0815, -0.0799,
        -0.0783, -0.0766, -0.0750, -0.0734, -0.0718, -0.0701, -0.0685, -0.0669,
        -0.0652, -0.0636, -0.0620, -0.0603, -0.0587, -0.0571, -0.0554, -0.0538,
        -0.0522, -0.0506, -0.0489, -0.0473, -0.0457, -0.0440, -0.0424, -0.0408,
        -0.0391, -0.0375, -0.0359, -0.0342, -0.0326, -0.0310, -0.0294, -0.0277,
        -0.0261, -0.0245, -0.0228, -0.0212, -0.0196, -0.0179, -0.0163, -0.0147,
        -0.0130, -0.0114, -0.0098, -0.0082, -0.0065, -0.0049, -0.0033, -0.0016,
        -0.0000,  0.0016,  0.0033,  0.0049,  0.0065,  0.0082,  0.0098,  0.0114,
         0.0130,  0.0147,  0.0163,  0.0179,  0.0196,  0.0212,  0.0228,  0.0245,
         0.0261,  0.0277,  0.0294,  0.0310,  0.0326,  0.0342,  0.0359,  0.0375,
         0.0391,  0.0408,  0.0424,  0.0440,  0.0457,  0.0473,  0.0489,  0.0506,
         0.0522,  0.0538,  0.0554,  0.0571,  0.0587,  0.0603,  0.0620,  0.0636,
         0.0652,  0.0669,  0.0685,  0.0701,  0.0718,  0.0734,  0.0750,  0.0766,
         0.0783,  0.0799,  0.0815,  0.0832,  0.0848,  0.0864,  0.0881,  0.0897,
         0.0913,  0.0930,  0.0946,  0.0962,  0.0978,  0.0995,  0.1011,  0.1027,
         0.1044,  0.1060,  0.1076,  0.1093,  0.1109,  0.1125,  0.1142,  0.1158,
         0.1174,  0.1190,  0.1207,  0.1223,  0.1239,  0.1256,  0.1272,  0.1288,
         0.1305,  0.1321,  0.1337,  0.1370,  0.1386,  0.1419,  0.1451,  0.1484,
         0.1500,  0.1517,  0.1549,  0.1582,  0.1696,  0.1957,  0.2071],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  191
---------Q_out---------
None
name:  bert.encoder.layer.2.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0051,  0.0132, -0.0453,  ..., -0.0727, -0.0247,  0.0102],
        [ 0.0121,  0.0474, -0.0041,  ...,  0.0092, -0.0084, -0.0216],
        [ 0.0392, -0.0269,  0.0159,  ..., -0.0094,  0.0488, -0.0038],
        ...,
        [ 0.0407, -0.0268, -0.0244,  ...,  0.0063, -0.0395, -0.0214],
        [-0.0202, -0.0379,  0.0399,  ...,  0.0006, -0.0302,  0.0230],
        [ 0.0111,  0.0200,  0.0155,  ...,  0.0177, -0.0077,  0.0196]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0042,  0.0127, -0.0444,  ..., -0.0720, -0.0254,  0.0106],
        [ 0.0127,  0.0466, -0.0042,  ...,  0.0085, -0.0085, -0.0212],
        [ 0.0402, -0.0275,  0.0169,  ..., -0.0085,  0.0487, -0.0042],
        ...,
        [ 0.0402, -0.0275, -0.0254,  ...,  0.0063, -0.0402, -0.0212],
        [-0.0212, -0.0381,  0.0402,  ...,  0.0000, -0.0296,  0.0233],
        [ 0.0106,  0.0190,  0.0148,  ...,  0.0169, -0.0085,  0.0190]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2688, -0.2137, -0.2053, -0.1926, -0.1905, -0.1883, -0.1862, -0.1820,
        -0.1778, -0.1757, -0.1693, -0.1672, -0.1651, -0.1630, -0.1608, -0.1587,
        -0.1566, -0.1524, -0.1503, -0.1481, -0.1460, -0.1439, -0.1418, -0.1397,
        -0.1376, -0.1354, -0.1333, -0.1312, -0.1291, -0.1270, -0.1249, -0.1227,
        -0.1206, -0.1185, -0.1164, -0.1143, -0.1122, -0.1100, -0.1079, -0.1058,
        -0.1037, -0.1016, -0.0995, -0.0973, -0.0952, -0.0931, -0.0910, -0.0889,
        -0.0868, -0.0847, -0.0825, -0.0804, -0.0783, -0.0762, -0.0741, -0.0720,
        -0.0698, -0.0677, -0.0656, -0.0635, -0.0614, -0.0593, -0.0571, -0.0550,
        -0.0529, -0.0508, -0.0487, -0.0466, -0.0444, -0.0423, -0.0402, -0.0381,
        -0.0360, -0.0339, -0.0317, -0.0296, -0.0275, -0.0254, -0.0233, -0.0212,
        -0.0190, -0.0169, -0.0148, -0.0127, -0.0106, -0.0085, -0.0063, -0.0042,
        -0.0021, -0.0000,  0.0021,  0.0042,  0.0063,  0.0085,  0.0106,  0.0127,
         0.0148,  0.0169,  0.0190,  0.0212,  0.0233,  0.0254,  0.0275,  0.0296,
         0.0317,  0.0339,  0.0360,  0.0381,  0.0402,  0.0423,  0.0444,  0.0466,
         0.0487,  0.0508,  0.0529,  0.0550,  0.0571,  0.0593,  0.0614,  0.0635,
         0.0656,  0.0677,  0.0698,  0.0720,  0.0741,  0.0762,  0.0783,  0.0804,
         0.0825,  0.0847,  0.0868,  0.0889,  0.0910,  0.0931,  0.0952,  0.0973,
         0.0995,  0.1016,  0.1037,  0.1058,  0.1079,  0.1100,  0.1122,  0.1143,
         0.1164,  0.1185,  0.1206,  0.1227,  0.1249,  0.1270,  0.1291,  0.1312,
         0.1333,  0.1354,  0.1376,  0.1397,  0.1418,  0.1439,  0.1460,  0.1481,
         0.1503,  0.1524,  0.1545,  0.1566,  0.1587,  0.1608,  0.1651,  0.1672,
         0.1693,  0.1714,  0.1735,  0.1757,  0.1778,  0.1820,  0.1883,  0.1968,
         0.2180,  0.2286,  0.2455], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  179
---------Q_out---------
None
name:  bert.encoder.layer.2.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0163, -0.0087, -0.0179,  ...,  0.0448, -0.0416,  0.0542],
        [ 0.0067, -0.0456,  0.0161,  ...,  0.0557, -0.0121, -0.0136],
        [ 0.0149,  0.0316,  0.0115,  ...,  0.0773,  0.0071,  0.0150],
        ...,
        [-0.0208,  0.0314, -0.0195,  ...,  0.0629, -0.0314, -0.0029],
        [-0.0053, -0.0527,  0.0200,  ...,  0.0072,  0.0421, -0.0054],
        [ 0.0107, -0.0571, -0.0092,  ...,  0.0989,  0.0188, -0.0339]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0148, -0.0099, -0.0198,  ...,  0.0445, -0.0396,  0.0544],
        [ 0.0049, -0.0445,  0.0148,  ...,  0.0544, -0.0099, -0.0148],
        [ 0.0148,  0.0297,  0.0099,  ...,  0.0792,  0.0049,  0.0148],
        ...,
        [-0.0198,  0.0297, -0.0198,  ...,  0.0643, -0.0297, -0.0049],
        [-0.0049, -0.0544,  0.0198,  ...,  0.0049,  0.0396, -0.0049],
        [ 0.0099, -0.0594, -0.0099,  ...,  0.0990,  0.0198, -0.0346]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.6286, -0.5395, -0.5098, -0.4999, -0.4702, -0.4356, -0.4257, -0.4207,
        -0.4158, -0.4108, -0.4059, -0.3960, -0.3910, -0.3861, -0.3811, -0.3762,
        -0.3712, -0.3663, -0.3613, -0.3564, -0.3514, -0.3465, -0.3415, -0.3366,
        -0.3316, -0.3267, -0.3217, -0.3168, -0.3118, -0.3069, -0.3019, -0.2970,
        -0.2920, -0.2871, -0.2821, -0.2772, -0.2722, -0.2673, -0.2623, -0.2574,
        -0.2524, -0.2475, -0.2425, -0.2376, -0.2326, -0.2277, -0.2227, -0.2178,
        -0.2128, -0.2079, -0.2029, -0.1980, -0.1930, -0.1881, -0.1831, -0.1782,
        -0.1732, -0.1683, -0.1633, -0.1584, -0.1534, -0.1485, -0.1435, -0.1386,
        -0.1336, -0.1287, -0.1237, -0.1188, -0.1138, -0.1089, -0.1039, -0.0990,
        -0.0940, -0.0891, -0.0841, -0.0792, -0.0742, -0.0693, -0.0643, -0.0594,
        -0.0544, -0.0495, -0.0445, -0.0396, -0.0346, -0.0297, -0.0247, -0.0198,
        -0.0148, -0.0099, -0.0049, -0.0000,  0.0049,  0.0099,  0.0148,  0.0198,
         0.0247,  0.0297,  0.0346,  0.0396,  0.0445,  0.0495,  0.0544,  0.0594,
         0.0643,  0.0693,  0.0742,  0.0792,  0.0841,  0.0891,  0.0940,  0.0990,
         0.1039,  0.1089,  0.1138,  0.1188,  0.1237,  0.1287,  0.1336,  0.1386,
         0.1435,  0.1485,  0.1534,  0.1584,  0.1633,  0.1683,  0.1732,  0.1782,
         0.1831,  0.1881,  0.1930,  0.1980,  0.2029,  0.2079,  0.2128,  0.2178,
         0.2227,  0.2277,  0.2326,  0.2376,  0.2425,  0.2475,  0.2524,  0.2574,
         0.2623,  0.2673,  0.2722,  0.2772,  0.2821,  0.2871,  0.2920,  0.2970,
         0.3069,  0.3118,  0.3168,  0.3217,  0.3267,  0.3316,  0.3366,  0.3415,
         0.3465,  0.3514,  0.3564,  0.3613,  0.3663,  0.3712,  0.3762,  0.3811,
         0.3861,  0.3910,  0.3960,  0.4059,  0.4158,  0.4207,  0.4257,  0.4306,
         0.4356,  0.4554,  0.4603,  0.4801,  0.4900,  0.5049,  0.5593,  0.5742],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  184
---------Q_out---------
None
name:  bert.encoder.layer.2.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0723,  0.0321,  0.0542,  ..., -0.0336, -0.0145,  0.0318],
        [-0.0129, -0.0045,  0.0129,  ..., -0.0814,  0.0052, -0.0246],
        [ 0.0062, -0.0118, -0.0032,  ..., -0.0084,  0.0682,  0.0398],
        ...,
        [ 0.0403, -0.0088, -0.0025,  ..., -0.0082,  0.0236,  0.0318],
        [ 0.0351, -0.0018, -0.0425,  ..., -0.0350, -0.0077,  0.0672],
        [ 0.0245, -0.0480, -0.0222,  ...,  0.0392,  0.0289,  0.0381]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0787,  0.0262,  0.0524,  ..., -0.0262, -0.0262,  0.0262],
        [-0.0000, -0.0000,  0.0000,  ..., -0.0787,  0.0000, -0.0262],
        [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0787,  0.0524],
        ...,
        [ 0.0524, -0.0000, -0.0000,  ..., -0.0000,  0.0262,  0.0262],
        [ 0.0262, -0.0000, -0.0524,  ..., -0.0262, -0.0000,  0.0787],
        [ 0.0262, -0.0524, -0.0262,  ...,  0.0262,  0.0262,  0.0262]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-3.3304, -2.9108, -1.2850, -0.9441, -0.8392, -0.8129, -0.7867, -0.7343,
        -0.6818, -0.6556, -0.6294, -0.5769, -0.5507, -0.5245, -0.4983, -0.4720,
        -0.4458, -0.4196, -0.3934, -0.3671, -0.3409, -0.3147, -0.2885, -0.2622,
        -0.2360, -0.2098, -0.1836, -0.1573, -0.1311, -0.1049, -0.0787, -0.0524,
        -0.0262, -0.0000,  0.0262,  0.0524,  0.0787,  0.1049,  0.1311,  0.1573,
         0.1836,  0.2098,  0.2360,  0.2622,  0.2885,  0.3147,  0.3409,  0.3671,
         0.3934,  0.4196,  0.4458,  0.4720,  0.4983,  0.5507,  0.5769,  0.6031,
         0.7080,  0.7867,  0.8129,  0.9178,  0.9441,  1.0490], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  62
---------Q_out---------
None
name:  bert.encoder.layer.3.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0586, -0.0279,  0.0021,  ..., -0.0204,  0.0418,  0.0559],
        [-0.0011,  0.1282, -0.0563,  ..., -0.0514, -0.0198,  0.0516],
        [-0.0082,  0.0060, -0.0468,  ...,  0.0570, -0.0072, -0.0617],
        ...,
        [ 0.0117,  0.0645, -0.0160,  ..., -0.0027, -0.0030, -0.0142],
        [ 0.0029,  0.0530,  0.0220,  ..., -0.0047,  0.0399,  0.0344],
        [ 0.0512,  0.0246,  0.0077,  ...,  0.0108,  0.0262,  0.0080]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0582, -0.0291,  0.0026,  ..., -0.0212,  0.0423,  0.0556],
        [-0.0000,  0.1270, -0.0556,  ..., -0.0503, -0.0185,  0.0529],
        [-0.0079,  0.0053, -0.0476,  ...,  0.0582, -0.0079, -0.0608],
        ...,
        [ 0.0106,  0.0635, -0.0159,  ..., -0.0026, -0.0026, -0.0132],
        [ 0.0026,  0.0529,  0.0212,  ..., -0.0053,  0.0397,  0.0344],
        [ 0.0503,  0.0238,  0.0079,  ...,  0.0106,  0.0265,  0.0079]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3360, -0.3016, -0.2830, -0.2778, -0.2672, -0.2592, -0.2539, -0.2487,
        -0.2460, -0.2434, -0.2407, -0.2354, -0.2328, -0.2301, -0.2249, -0.2222,
        -0.2196, -0.2143, -0.2116, -0.2090, -0.2063, -0.2037, -0.2010, -0.1984,
        -0.1958, -0.1931, -0.1905, -0.1878, -0.1852, -0.1825, -0.1799, -0.1772,
        -0.1746, -0.1719, -0.1693, -0.1667, -0.1640, -0.1614, -0.1587, -0.1561,
        -0.1534, -0.1508, -0.1481, -0.1455, -0.1428, -0.1402, -0.1376, -0.1349,
        -0.1323, -0.1296, -0.1270, -0.1243, -0.1217, -0.1190, -0.1164, -0.1137,
        -0.1111, -0.1085, -0.1058, -0.1032, -0.1005, -0.0979, -0.0952, -0.0926,
        -0.0899, -0.0873, -0.0846, -0.0820, -0.0794, -0.0767, -0.0741, -0.0714,
        -0.0688, -0.0661, -0.0635, -0.0608, -0.0582, -0.0556, -0.0529, -0.0503,
        -0.0476, -0.0450, -0.0423, -0.0397, -0.0370, -0.0344, -0.0317, -0.0291,
        -0.0265, -0.0238, -0.0212, -0.0185, -0.0159, -0.0132, -0.0106, -0.0079,
        -0.0053, -0.0026, -0.0000,  0.0026,  0.0053,  0.0079,  0.0106,  0.0132,
         0.0159,  0.0185,  0.0212,  0.0238,  0.0265,  0.0291,  0.0317,  0.0344,
         0.0370,  0.0397,  0.0423,  0.0450,  0.0476,  0.0503,  0.0529,  0.0556,
         0.0582,  0.0608,  0.0635,  0.0661,  0.0688,  0.0714,  0.0741,  0.0767,
         0.0794,  0.0820,  0.0846,  0.0873,  0.0899,  0.0926,  0.0952,  0.0979,
         0.1005,  0.1032,  0.1058,  0.1085,  0.1111,  0.1137,  0.1164,  0.1190,
         0.1217,  0.1243,  0.1270,  0.1296,  0.1323,  0.1349,  0.1376,  0.1402,
         0.1428,  0.1455,  0.1481,  0.1508,  0.1534,  0.1561,  0.1587,  0.1614,
         0.1640,  0.1667,  0.1693,  0.1719,  0.1746,  0.1772,  0.1799,  0.1825,
         0.1852,  0.1878,  0.1905,  0.1931,  0.1958,  0.1984,  0.2010,  0.2037,
         0.2063,  0.2090,  0.2116,  0.2143,  0.2169,  0.2196,  0.2222,  0.2249,
         0.2301,  0.2328,  0.2354,  0.2381,  0.2407,  0.2434,  0.2460,  0.2539,
         0.2566,  0.2619,  0.2645,  0.2672,  0.2698,  0.2751,  0.2778,  0.2857,
         0.2910,  0.3333], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  202
---------Q_out---------
tensor([[[ 0.7907,  0.2636, -0.7248,  ..., -0.2636,  0.5930,  0.3295],
         [ 0.7248,  0.5930, -0.0659,  ..., -1.4496,  1.1860,  1.3837],
         [ 1.7132,  0.5930,  0.3295,  ..., -1.7132,  0.3953,  0.1977],
         ...,
         [ 0.2636,  1.0543,  0.1318,  ..., -0.4612,  0.3953, -0.3295],
         [ 0.6589,  0.5930, -0.2636,  ..., -0.4612, -0.7907,  0.0659],
         [-0.1318,  1.5814, -0.0000,  ..., -0.7907, -1.0543, -0.1318]],

        [[ 0.5271,  0.0659, -0.7248,  ..., -0.1977,  0.7907,  0.4612],
         [ 1.4496,  0.7248,  0.6589,  ..., -0.3953,  0.5930,  2.3721],
         [ 1.6473, -0.1977, -2.9651,  ..., -0.7248, -0.4612,  1.9767],
         ...,
         [ 1.9108, -1.0543,  2.4380,  ...,  0.3953,  0.2636,  0.1318],
         [ 1.3837, -0.1318, -0.2636,  ..., -0.7248, -1.4496, -0.7907],
         [-0.3295,  0.0659, -0.9225,  ..., -0.3953,  0.9884,  1.1860]],

        [[ 0.4612, -0.1977, -0.6589,  ..., -0.4612,  0.8566,  0.1977],
         [ 0.3953,  0.3295,  0.0000,  ..., -0.0000,  1.3837,  1.3178],
         [ 0.3953,  0.5930, -1.3178,  ..., -0.7248, -0.1318, -0.7248],
         ...,
         [ 0.4612,  0.9884, -0.1977,  ..., -0.0000, -0.4612, -0.3295],
         [ 0.2636,  1.4496, -0.6589,  ..., -0.3295, -0.6589, -0.5930],
         [-0.4612,  1.5155, -0.8566,  ..., -0.5271, -1.4496, -0.3295]],

        ...,

        [[ 0.5271, -0.1318, -0.9884,  ..., -0.5271,  0.7907,  0.1977],
         [ 1.3837,  0.9225, -2.6356,  ...,  0.2636,  1.5814,  0.5930],
         [ 0.4612,  0.7907,  0.1977,  ...,  0.4612, -0.3953,  0.9884],
         ...,
         [ 0.6589,  0.9225,  0.3295,  ..., -0.4612, -0.9225, -0.1318],
         [ 0.3295,  0.7907, -0.5271,  ..., -0.1977, -1.1202,  0.3953],
         [-0.0000,  0.3953, -0.5930,  ..., -0.0659, -1.8450, -0.2636]],

        [[ 0.2636,  0.1318, -0.3953,  ..., -0.1977,  1.1202,  0.2636],
         [-0.0659,  0.2636, -0.0659,  ..., -0.5271,  0.2636,  0.0659],
         [ 1.5155,  0.6589, -0.4612,  ...,  0.1977, -1.3837,  0.1977],
         ...,
         [ 0.8566,  0.3953, -0.5930,  ..., -1.1860,  0.2636,  0.1318],
         [ 0.9884,  0.7248, -0.8566,  ..., -0.6589, -0.4612,  0.4612],
         [ 0.7248,  1.2519, -1.1860,  ..., -0.7907, -1.4496, -0.3953]],

        [[ 0.5930, -0.1318, -0.3295,  ..., -0.3295,  0.7907,  0.3953],
         [-0.7907,  1.0543, -0.6589,  ...,  0.0659,  0.1318,  1.9767],
         [-0.5930, -1.7791,  0.9884,  ..., -1.5155, -1.1860, -0.3953],
         ...,
         [ 0.3953,  1.1860, -0.6589,  ..., -0.1318, -0.1977, -0.1318],
         [-0.0659,  1.5814, -0.8566,  ..., -0.3953, -0.5930, -0.1977],
         [-0.1977,  1.6473, -0.1318,  ..., -0.7248, -0.9225,  0.1977]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.7093, -7.3798, -7.1821, -7.1163, -6.9845, -6.9186, -6.8527, -6.7868,
        -6.7209, -6.6550, -6.5891, -6.4573, -6.3914, -6.3256, -6.2597, -6.1938,
        -6.1279, -6.0620, -5.9961, -5.9302, -5.8643, -5.7984, -5.7325, -5.6666,
        -5.6008, -5.5349, -5.4690, -5.4031, -5.3372, -5.2713, -5.2054, -5.1395,
        -5.0736, -5.0077, -4.9418, -4.8760, -4.8101, -4.7442, -4.6783, -4.6124,
        -4.5465, -4.4806, -4.4147, -4.3488, -4.2829, -4.2170, -4.1511, -4.0853,
        -4.0194, -3.9535, -3.8876, -3.8217, -3.7558, -3.6899, -3.6240, -3.5581,
        -3.4922, -3.4263, -3.3605, -3.2946, -3.2287, -3.1628, -3.0969, -3.0310,
        -2.9651, -2.8992, -2.8333, -2.7674, -2.7015, -2.6356, -2.5698, -2.5039,
        -2.4380, -2.3721, -2.3062, -2.2403, -2.1744, -2.1085, -2.0426, -1.9767,
        -1.9108, -1.8450, -1.7791, -1.7132, -1.6473, -1.5814, -1.5155, -1.4496,
        -1.3837, -1.3178, -1.2519, -1.1860, -1.1202, -1.0543, -0.9884, -0.9225,
        -0.8566, -0.7907, -0.7248, -0.6589, -0.5930, -0.5271, -0.4612, -0.3953,
        -0.3295, -0.2636, -0.1977, -0.1318, -0.0659, -0.0000,  0.0659,  0.1318,
         0.1977,  0.2636,  0.3295,  0.3953,  0.4612,  0.5271,  0.5930,  0.6589,
         0.7248,  0.7907,  0.8566,  0.9225,  0.9884,  1.0543,  1.1202,  1.1860,
         1.2519,  1.3178,  1.3837,  1.4496,  1.5155,  1.5814,  1.6473,  1.7132,
         1.7791,  1.8450,  1.9108,  1.9767,  2.0426,  2.1085,  2.1744,  2.2403,
         2.3062,  2.3721,  2.4380,  2.5039,  2.5698,  2.6356,  2.7015,  2.7674,
         2.8333,  2.8992,  2.9651,  3.0310,  3.0969,  3.1628,  3.2287,  3.2946,
         3.3605,  3.4263,  3.4922,  3.5581,  3.6240,  3.6899,  3.7558,  3.8217,
         3.8876,  3.9535,  4.0194,  4.0853,  4.1511,  4.2170,  4.2829,  4.3488,
         4.4147,  4.4806,  4.5465,  4.6124,  4.6783,  4.7442,  4.8101,  4.8760,
         4.9418,  5.0077,  5.0736,  5.1395,  5.2054,  5.2713,  5.3372,  5.4031,
         5.4690,  5.5349,  5.6008,  5.6666,  5.7325,  5.7984,  5.8643,  5.9302,
         5.9961,  6.0620,  6.1279,  6.1938,  6.2597,  6.3256,  6.3914,  6.4573,
         6.5232,  6.5891,  6.6550,  6.7209,  6.7868,  6.8527,  6.9186,  6.9845,
         7.0504,  7.3139], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  218
name:  bert.encoder.layer.3.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0255,  0.0115, -0.0421,  ..., -0.0523, -0.1666,  0.0598],
        [ 0.0250,  0.0335, -0.0414,  ...,  0.0785, -0.0086,  0.0131],
        [-0.0336, -0.0245,  0.0051,  ..., -0.0921,  0.0214,  0.0055],
        ...,
        [ 0.0169,  0.0629, -0.0192,  ..., -0.0473, -0.0073,  0.0028],
        [-0.0341,  0.0354,  0.0811,  ..., -0.0198,  0.0090, -0.0066],
        [ 0.0180, -0.0093, -0.0135,  ...,  0.0250, -0.0290,  0.0207]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0245,  0.0109, -0.0408,  ..., -0.0517, -0.1659,  0.0598],
        [ 0.0245,  0.0326, -0.0408,  ...,  0.0789, -0.0082,  0.0136],
        [-0.0326, -0.0245,  0.0054,  ..., -0.0925,  0.0218,  0.0054],
        ...,
        [ 0.0163,  0.0626, -0.0190,  ..., -0.0462, -0.0082,  0.0027],
        [-0.0354,  0.0354,  0.0816,  ..., -0.0190,  0.0082, -0.0054],
        [ 0.0190, -0.0082, -0.0136,  ...,  0.0245, -0.0299,  0.0218]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3454, -0.3345, -0.3046, -0.3019, -0.2992, -0.2801, -0.2774, -0.2747,
        -0.2720, -0.2693, -0.2638, -0.2611, -0.2584, -0.2557, -0.2529, -0.2502,
        -0.2475, -0.2421, -0.2393, -0.2366, -0.2339, -0.2312, -0.2285, -0.2257,
        -0.2230, -0.2203, -0.2176, -0.2149, -0.2121, -0.2094, -0.2067, -0.2040,
        -0.2013, -0.1985, -0.1958, -0.1931, -0.1904, -0.1877, -0.1849, -0.1822,
        -0.1795, -0.1768, -0.1741, -0.1714, -0.1686, -0.1659, -0.1632, -0.1605,
        -0.1578, -0.1550, -0.1523, -0.1496, -0.1469, -0.1442, -0.1414, -0.1387,
        -0.1360, -0.1333, -0.1306, -0.1278, -0.1251, -0.1224, -0.1197, -0.1170,
        -0.1142, -0.1115, -0.1088, -0.1061, -0.1034, -0.1006, -0.0979, -0.0952,
        -0.0925, -0.0898, -0.0870, -0.0843, -0.0816, -0.0789, -0.0762, -0.0734,
        -0.0707, -0.0680, -0.0653, -0.0626, -0.0598, -0.0571, -0.0544, -0.0517,
        -0.0490, -0.0462, -0.0435, -0.0408, -0.0381, -0.0354, -0.0326, -0.0299,
        -0.0272, -0.0245, -0.0218, -0.0190, -0.0163, -0.0136, -0.0109, -0.0082,
        -0.0054, -0.0027, -0.0000,  0.0027,  0.0054,  0.0082,  0.0109,  0.0136,
         0.0163,  0.0190,  0.0218,  0.0245,  0.0272,  0.0299,  0.0326,  0.0354,
         0.0381,  0.0408,  0.0435,  0.0462,  0.0490,  0.0517,  0.0544,  0.0571,
         0.0598,  0.0626,  0.0653,  0.0680,  0.0707,  0.0734,  0.0762,  0.0789,
         0.0816,  0.0843,  0.0870,  0.0898,  0.0925,  0.0952,  0.0979,  0.1006,
         0.1034,  0.1061,  0.1088,  0.1115,  0.1142,  0.1170,  0.1197,  0.1224,
         0.1251,  0.1278,  0.1306,  0.1333,  0.1360,  0.1387,  0.1414,  0.1442,
         0.1469,  0.1496,  0.1523,  0.1550,  0.1578,  0.1605,  0.1632,  0.1659,
         0.1686,  0.1714,  0.1741,  0.1768,  0.1795,  0.1822,  0.1849,  0.1877,
         0.1904,  0.1931,  0.1958,  0.1985,  0.2013,  0.2040,  0.2067,  0.2094,
         0.2121,  0.2149,  0.2176,  0.2203,  0.2230,  0.2257,  0.2285,  0.2312,
         0.2339,  0.2366,  0.2393,  0.2421,  0.2448,  0.2475,  0.2557,  0.2611,
         0.2638,  0.2665,  0.2774,  0.2883,  0.2992,  0.3073,  0.3454],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  207
---------Q_out---------
tensor([[[ 0.7785, -1.1190,  0.2433,  ..., -1.3623,  0.0487,  0.7298],
         [ 1.6056, -0.9244,  0.8271,  ..., -1.5083, -0.7298, -0.9731],
         [ 2.2867, -0.8758,  0.9731,  ..., -1.8488,  0.1460, -0.0487],
         ...,
         [ 0.4865, -1.0704,  0.1460,  ..., -0.0973,  1.4596, -0.0487],
         [ 0.5352, -0.9244, -0.3892,  ..., -0.0487,  0.5352, -0.1460],
         [ 1.3136, -1.3623, -1.0704,  ...,  0.1946, -0.0973,  0.0000]],

        [[ 0.8271, -0.9244,  0.0487,  ..., -1.3136,  0.1946,  0.6325],
         [ 1.4596, -0.6325, -0.0973,  ..., -1.5083, -0.0487, -0.5838],
         [ 2.3840, -2.0435, -2.2381,  ..., -0.3406,  0.5352, -0.5838],
         ...,
         [ 1.2650, -1.8975,  1.6056,  ..., -2.1408, -0.6325, -0.1946],
         [ 1.5569, -1.4110,  0.0973,  ..., -0.0973, -0.3406, -0.3406],
         [ 0.3406, -1.1190, -0.3406,  ..., -0.0973, -0.0973, -0.0487]],

        [[ 0.8758, -1.2650,  0.3406,  ..., -1.3136,  0.3406,  0.4865],
         [ 0.4379, -0.8271,  0.3406,  ..., -0.7785,  0.5838,  0.9244],
         [-0.3406, -0.8271, -0.0973,  ...,  0.0973,  0.2919,  0.2433],
         ...,
         [ 0.8758, -1.6056, -0.4865,  ...,  0.0973,  0.4379,  0.0487],
         [ 1.2163, -1.2163, -0.7298,  ...,  0.2433, -0.0973, -0.4865],
         [ 1.2163, -1.8002, -0.3406,  ...,  0.7785,  0.4379, -0.2919]],

        ...,

        [[ 0.9731, -1.2650,  0.1460,  ..., -1.4110,  0.6325,  0.6325],
         [ 1.7029, -0.3406, -0.9244,  ..., -0.6812, -0.5838,  0.0487],
         [ 0.8271, -0.9244,  1.8975,  ..., -0.4865, -0.2919,  0.4379],
         ...,
         [ 0.6325, -0.9731, -0.6812,  ...,  0.4865,  0.4379, -0.6812],
         [ 0.4865, -1.2650, -0.4379,  ...,  0.4865,  0.4379,  0.0973],
         [ 0.9731, -1.7029, -0.7298,  ...,  0.5838, -0.1946,  0.1460]],

        [[ 0.9244, -0.9244,  0.0487,  ..., -1.4110,  0.4865,  0.7298],
         [-0.0973,  0.0973, -0.1946,  ..., -0.4379, -1.0217,  0.1460],
         [ 1.6542, -0.6325,  1.5569,  ..., -0.6812, -0.8758, -0.1460],
         ...,
         [ 0.4379, -1.3623, -0.6812,  ..., -0.6812,  0.4379,  0.2919],
         [ 0.8758, -1.2650, -0.4865,  ..., -0.4379,  0.0000,  0.1460],
         [ 1.0704, -1.4596, -1.0217,  ...,  0.1460,  0.2433,  0.1946]],

        [[ 0.7298, -1.1190,  0.1460,  ..., -1.3136,  0.5352,  0.5838],
         [-0.2433,  0.1946, -0.5838,  ...,  0.6325, -0.4865, -0.7298],
         [ 0.6812, -3.1625,  1.0217,  ..., -0.3892, -0.8758, -0.3406],
         ...,
         [ 0.3892, -1.6056, -0.5352,  ..., -0.2919,  0.7785, -0.2919],
         [ 1.5569, -1.2163, -0.9731,  ...,  0.1460, -0.1946,  0.0973],
         [ 1.3623, -1.6056, -0.7298,  ...,  0.2433,  0.2919, -0.0487]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-5.6925, -5.6438, -5.5952, -5.5465, -5.4979, -5.4492, -5.4005, -5.3519,
        -5.3032, -5.2546, -5.2059, -5.1573, -5.1086, -5.0600, -5.0113, -4.9627,
        -4.9140, -4.8654, -4.8167, -4.7681, -4.7194, -4.6707, -4.6221, -4.5734,
        -4.5248, -4.4761, -4.4275, -4.3788, -4.3302, -4.2815, -4.2329, -4.1842,
        -4.1356, -4.0869, -4.0382, -3.9896, -3.9409, -3.8923, -3.8436, -3.7950,
        -3.7463, -3.6977, -3.6490, -3.6004, -3.5517, -3.5031, -3.4544, -3.4058,
        -3.3571, -3.3084, -3.2598, -3.2111, -3.1625, -3.1138, -3.0652, -3.0165,
        -2.9679, -2.9192, -2.8706, -2.8219, -2.7733, -2.7246, -2.6759, -2.6273,
        -2.5786, -2.5300, -2.4813, -2.4327, -2.3840, -2.3354, -2.2867, -2.2381,
        -2.1894, -2.1408, -2.0921, -2.0435, -1.9948, -1.9461, -1.8975, -1.8488,
        -1.8002, -1.7515, -1.7029, -1.6542, -1.6056, -1.5569, -1.5083, -1.4596,
        -1.4110, -1.3623, -1.3136, -1.2650, -1.2163, -1.1677, -1.1190, -1.0704,
        -1.0217, -0.9731, -0.9244, -0.8758, -0.8271, -0.7785, -0.7298, -0.6812,
        -0.6325, -0.5838, -0.5352, -0.4865, -0.4379, -0.3892, -0.3406, -0.2919,
        -0.2433, -0.1946, -0.1460, -0.0973, -0.0487, -0.0000,  0.0487,  0.0973,
         0.1460,  0.1946,  0.2433,  0.2919,  0.3406,  0.3892,  0.4379,  0.4865,
         0.5352,  0.5838,  0.6325,  0.6812,  0.7298,  0.7785,  0.8271,  0.8758,
         0.9244,  0.9731,  1.0217,  1.0704,  1.1190,  1.1677,  1.2163,  1.2650,
         1.3136,  1.3623,  1.4110,  1.4596,  1.5083,  1.5569,  1.6056,  1.6542,
         1.7029,  1.7515,  1.8002,  1.8488,  1.8975,  1.9461,  1.9948,  2.0435,
         2.0921,  2.1408,  2.1894,  2.2381,  2.2867,  2.3354,  2.3840,  2.4327,
         2.4813,  2.5300,  2.5786,  2.6273,  2.6759,  2.7246,  2.7733,  2.8219,
         2.8706,  2.9192,  2.9679,  3.0165,  3.0652,  3.1138,  3.1625,  3.2111,
         3.2598,  3.3084,  3.3571,  3.4058,  3.4544,  3.5031,  3.5517,  3.6004,
         3.6490,  3.6977,  3.7463,  3.7950,  3.8436,  3.8923,  3.9409,  3.9896,
         4.0382,  4.0869,  4.1356,  4.1842,  4.2329,  4.2815,  4.3302,  4.3788,
         4.4275,  4.4761,  4.5248,  4.5734,  4.6221,  4.6707,  4.7194,  4.7681,
         4.8167,  4.8654,  4.9140,  4.9627,  5.0113,  5.0600,  5.1086,  5.1573,
         5.2059,  5.2546,  5.3032,  5.3519,  5.4005,  5.4979,  5.6438,  5.6925,
         5.7411,  5.7898,  5.8384,  5.8871,  5.9357,  5.9844,  6.1304,  6.1790],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  240
name:  bert.encoder.layer.3.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0055,  0.0188,  0.0388,  ..., -0.0070,  0.0303, -0.0081],
        [ 0.0097, -0.0152,  0.0069,  ...,  0.0067, -0.0004,  0.0046],
        [ 0.0132,  0.0213,  0.0003,  ..., -0.0208, -0.0172, -0.0106],
        ...,
        [ 0.0255,  0.0050, -0.0383,  ...,  0.0343,  0.0328,  0.0037],
        [-0.0216, -0.0111, -0.0009,  ..., -0.0455, -0.0165, -0.0036],
        [ 0.0085,  0.0356, -0.0460,  ..., -0.0396,  0.0267, -0.0174]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0060,  0.0181,  0.0392,  ..., -0.0075,  0.0301, -0.0075],
        [ 0.0090, -0.0151,  0.0075,  ...,  0.0060, -0.0000,  0.0045],
        [ 0.0136,  0.0211,  0.0000,  ..., -0.0211, -0.0166, -0.0105],
        ...,
        [ 0.0256,  0.0045, -0.0377,  ...,  0.0346,  0.0331,  0.0030],
        [-0.0211, -0.0105, -0.0015,  ..., -0.0452, -0.0166, -0.0030],
        [ 0.0090,  0.0361, -0.0467,  ..., -0.0392,  0.0271, -0.0181]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.1822, -0.1762, -0.1732, -0.1702, -0.1672, -0.1627, -0.1596, -0.1536,
        -0.1521, -0.1506, -0.1491, -0.1476, -0.1461, -0.1446, -0.1431, -0.1416,
        -0.1386, -0.1370, -0.1355, -0.1340, -0.1325, -0.1310, -0.1295, -0.1280,
        -0.1265, -0.1250, -0.1235, -0.1220, -0.1205, -0.1190, -0.1175, -0.1160,
        -0.1145, -0.1130, -0.1114, -0.1099, -0.1084, -0.1069, -0.1054, -0.1039,
        -0.1024, -0.1009, -0.0994, -0.0979, -0.0964, -0.0949, -0.0934, -0.0919,
        -0.0904, -0.0889, -0.0873, -0.0858, -0.0843, -0.0828, -0.0813, -0.0798,
        -0.0783, -0.0768, -0.0753, -0.0738, -0.0723, -0.0708, -0.0693, -0.0678,
        -0.0663, -0.0648, -0.0633, -0.0617, -0.0602, -0.0587, -0.0572, -0.0557,
        -0.0542, -0.0527, -0.0512, -0.0497, -0.0482, -0.0467, -0.0452, -0.0437,
        -0.0422, -0.0407, -0.0392, -0.0377, -0.0361, -0.0346, -0.0331, -0.0316,
        -0.0301, -0.0286, -0.0271, -0.0256, -0.0241, -0.0226, -0.0211, -0.0196,
        -0.0181, -0.0166, -0.0151, -0.0136, -0.0120, -0.0105, -0.0090, -0.0075,
        -0.0060, -0.0045, -0.0030, -0.0015, -0.0000,  0.0015,  0.0030,  0.0045,
         0.0060,  0.0075,  0.0090,  0.0105,  0.0120,  0.0136,  0.0151,  0.0166,
         0.0181,  0.0196,  0.0211,  0.0226,  0.0241,  0.0256,  0.0271,  0.0286,
         0.0301,  0.0316,  0.0331,  0.0346,  0.0361,  0.0377,  0.0392,  0.0407,
         0.0422,  0.0437,  0.0452,  0.0467,  0.0482,  0.0497,  0.0512,  0.0527,
         0.0542,  0.0557,  0.0572,  0.0587,  0.0602,  0.0617,  0.0633,  0.0648,
         0.0663,  0.0678,  0.0693,  0.0708,  0.0723,  0.0738,  0.0753,  0.0768,
         0.0783,  0.0798,  0.0813,  0.0828,  0.0843,  0.0858,  0.0873,  0.0889,
         0.0904,  0.0919,  0.0934,  0.0949,  0.0964,  0.0979,  0.0994,  0.1009,
         0.1024,  0.1039,  0.1054,  0.1069,  0.1084,  0.1099,  0.1114,  0.1130,
         0.1145,  0.1160,  0.1175,  0.1190,  0.1205,  0.1220,  0.1235,  0.1250,
         0.1265,  0.1280,  0.1295,  0.1310,  0.1325,  0.1340,  0.1355,  0.1370,
         0.1386,  0.1401,  0.1416,  0.1431,  0.1446,  0.1461,  0.1476,  0.1491,
         0.1506,  0.1521,  0.1536,  0.1551,  0.1581,  0.1596,  0.1687,  0.1717,
         0.1792,  0.1822,  0.1883,  0.1913], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  220
---------Q_out---------
None
name:  bert.encoder.layer.3.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0186, -0.0188, -0.0424,  ..., -0.0220, -0.0011, -0.0258],
        [ 0.0314, -0.0728,  0.0179,  ..., -0.0423,  0.0123, -0.0270],
        [ 0.0271, -0.0186,  0.0422,  ...,  0.0163, -0.0110, -0.0095],
        ...,
        [-0.0014,  0.0166, -0.0508,  ...,  0.0222,  0.0273,  0.0799],
        [ 0.0130,  0.0107,  0.0204,  ..., -0.0614, -0.0332, -0.0293],
        [-0.0225, -0.0087,  0.0212,  ...,  0.0026, -0.0247, -0.0311]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0183, -0.0183, -0.0428,  ..., -0.0214, -0.0000, -0.0244],
        [ 0.0305, -0.0733,  0.0183,  ..., -0.0428,  0.0122, -0.0275],
        [ 0.0275, -0.0183,  0.0428,  ...,  0.0153, -0.0122, -0.0092],
        ...,
        [-0.0000,  0.0153, -0.0519,  ...,  0.0214,  0.0275,  0.0794],
        [ 0.0122,  0.0092,  0.0214,  ..., -0.0611, -0.0336, -0.0305],
        [-0.0214, -0.0092,  0.0214,  ...,  0.0031, -0.0244, -0.0305]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3879, -0.2810, -0.2535, -0.2321, -0.2291, -0.2230, -0.2108, -0.2016,
        -0.1955, -0.1924, -0.1894, -0.1863, -0.1833, -0.1802, -0.1772, -0.1741,
        -0.1711, -0.1680, -0.1649, -0.1619, -0.1588, -0.1558, -0.1527, -0.1497,
        -0.1466, -0.1436, -0.1405, -0.1375, -0.1344, -0.1313, -0.1283, -0.1252,
        -0.1222, -0.1191, -0.1161, -0.1130, -0.1100, -0.1069, -0.1039, -0.1008,
        -0.0977, -0.0947, -0.0916, -0.0886, -0.0855, -0.0825, -0.0794, -0.0764,
        -0.0733, -0.0703, -0.0672, -0.0641, -0.0611, -0.0580, -0.0550, -0.0519,
        -0.0489, -0.0458, -0.0428, -0.0397, -0.0367, -0.0336, -0.0305, -0.0275,
        -0.0244, -0.0214, -0.0183, -0.0153, -0.0122, -0.0092, -0.0061, -0.0031,
        -0.0000,  0.0031,  0.0061,  0.0092,  0.0122,  0.0153,  0.0183,  0.0214,
         0.0244,  0.0275,  0.0305,  0.0336,  0.0367,  0.0397,  0.0428,  0.0458,
         0.0489,  0.0519,  0.0550,  0.0580,  0.0611,  0.0641,  0.0672,  0.0703,
         0.0733,  0.0764,  0.0794,  0.0825,  0.0855,  0.0886,  0.0916,  0.0947,
         0.0977,  0.1008,  0.1039,  0.1069,  0.1100,  0.1130,  0.1161,  0.1191,
         0.1222,  0.1252,  0.1283,  0.1313,  0.1344,  0.1375,  0.1405,  0.1436,
         0.1466,  0.1497,  0.1527,  0.1558,  0.1588,  0.1619,  0.1649,  0.1680,
         0.1711,  0.1741,  0.1772,  0.1802,  0.1833,  0.1863,  0.1894,  0.1955,
         0.2016,  0.2077,  0.2657], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  139
---------Q_out---------
None
name:  bert.encoder.layer.3.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0273, -0.0320,  0.0052,  ..., -0.0567,  0.0527, -0.0541],
        [ 0.0332, -0.0869,  0.0084,  ...,  0.0100, -0.0586,  0.0377],
        [-0.0134,  0.0086,  0.0570,  ...,  0.0246, -0.0267, -0.0130],
        ...,
        [-0.0378,  0.0276, -0.0195,  ...,  0.0224,  0.0133,  0.0169],
        [-0.0109,  0.0371,  0.0522,  ...,  0.0091,  0.0123,  0.0349],
        [-0.0089,  0.0407,  0.0879,  ...,  0.0031,  0.0262,  0.0027]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0259, -0.0311,  0.0052,  ..., -0.0570,  0.0518, -0.0518],
        [ 0.0311, -0.0880,  0.0104,  ...,  0.0104, -0.0570,  0.0362],
        [-0.0155,  0.0104,  0.0570,  ...,  0.0259, -0.0259, -0.0155],
        ...,
        [-0.0362,  0.0259, -0.0207,  ...,  0.0207,  0.0155,  0.0155],
        [-0.0104,  0.0362,  0.0518,  ...,  0.0104,  0.0104,  0.0362],
        [-0.0104,  0.0414,  0.0880,  ...,  0.0052,  0.0259,  0.0052]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.6576, -0.5074, -0.4867, -0.4712, -0.4142, -0.4090, -0.4039, -0.3832,
        -0.3780, -0.3728, -0.3676, -0.3624, -0.3573, -0.3469, -0.3417, -0.3366,
        -0.3314, -0.3262, -0.3210, -0.3158, -0.3107, -0.3003, -0.2900, -0.2848,
        -0.2796, -0.2744, -0.2692, -0.2641, -0.2589, -0.2537, -0.2485, -0.2434,
        -0.2382, -0.2330, -0.2278, -0.2226, -0.2175, -0.2123, -0.2071, -0.2019,
        -0.1968, -0.1916, -0.1864, -0.1812, -0.1760, -0.1709, -0.1657, -0.1605,
        -0.1553, -0.1502, -0.1450, -0.1398, -0.1346, -0.1294, -0.1243, -0.1191,
        -0.1139, -0.1087, -0.1036, -0.0984, -0.0932, -0.0880, -0.0828, -0.0777,
        -0.0725, -0.0673, -0.0621, -0.0570, -0.0518, -0.0466, -0.0414, -0.0362,
        -0.0311, -0.0259, -0.0207, -0.0155, -0.0104, -0.0052, -0.0000,  0.0052,
         0.0104,  0.0155,  0.0207,  0.0259,  0.0311,  0.0362,  0.0414,  0.0466,
         0.0518,  0.0570,  0.0621,  0.0673,  0.0725,  0.0777,  0.0828,  0.0880,
         0.0932,  0.0984,  0.1036,  0.1087,  0.1139,  0.1191,  0.1243,  0.1294,
         0.1346,  0.1398,  0.1450,  0.1502,  0.1553,  0.1605,  0.1657,  0.1709,
         0.1760,  0.1812,  0.1864,  0.1916,  0.1968,  0.2019,  0.2071,  0.2123,
         0.2175,  0.2226,  0.2278,  0.2330,  0.2382,  0.2434,  0.2485,  0.2537,
         0.2589,  0.2641,  0.2692,  0.2744,  0.2796,  0.2848,  0.2900,  0.2951,
         0.3003,  0.3055,  0.3107,  0.3210,  0.3262,  0.3314,  0.3366,  0.3469,
         0.3521,  0.3573,  0.3728,  0.3780,  0.3832,  0.4039,  0.4090,  0.4194,
         0.4349,  0.4556,  0.5333,  0.5695,  0.5799,  0.6317], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  158
---------Q_out---------
None
name:  bert.encoder.layer.3.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0174, -0.0094,  0.0378,  ...,  0.0423,  0.0226, -0.0134],
        [ 0.0163, -0.0708,  0.0159,  ..., -0.0395,  0.0003, -0.0208],
        [-0.0265, -0.0685,  0.0258,  ...,  0.0037, -0.0067, -0.0047],
        ...,
        [ 0.0888,  0.0234,  0.0554,  ..., -0.0026, -0.0588, -0.0506],
        [ 0.0171, -0.0138,  0.0025,  ..., -0.0149,  0.0111, -0.0100],
        [ 0.0393, -0.0083,  0.0156,  ..., -0.0480,  0.0551, -0.0122]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0000, -0.0000,  0.0523,  ...,  0.0523,  0.0000, -0.0000],
        [ 0.0000, -0.0523,  0.0000,  ..., -0.0523,  0.0000, -0.0000],
        [-0.0523, -0.0523,  0.0000,  ...,  0.0000, -0.0000, -0.0000],
        ...,
        [ 0.1046,  0.0000,  0.0523,  ..., -0.0000, -0.0523, -0.0523],
        [ 0.0000, -0.0000,  0.0000,  ..., -0.0000,  0.0000, -0.0000],
        [ 0.0523, -0.0000,  0.0000,  ..., -0.0523,  0.0523, -0.0000]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-6.6403, -6.2220, -4.1306, -3.0848, -1.2026, -0.8889, -0.6274, -0.5751,
        -0.4706, -0.4183, -0.3660, -0.3137, -0.2614, -0.2091, -0.1569, -0.1046,
        -0.0523, -0.0000,  0.0523,  0.1046,  0.1569,  0.2091,  0.2614,  0.3137,
         0.3660,  0.4183,  0.4706,  0.5229,  0.6274,  0.7843], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  30
---------Q_out---------
None
name:  bert.encoder.layer.4.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0830,  0.0502,  0.0217,  ..., -0.0394,  0.0419,  0.0400],
        [ 0.0208, -0.0456, -0.0104,  ...,  0.0401,  0.0351,  0.0080],
        [ 0.0652,  0.0392,  0.0234,  ..., -0.0624,  0.0300,  0.0037],
        ...,
        [-0.0158, -0.0141, -0.0248,  ...,  0.0306,  0.0290,  0.0581],
        [-0.0366, -0.0180, -0.0305,  ..., -0.0624,  0.0353, -0.0064],
        [-0.0341,  0.0210,  0.0738,  ..., -0.0089, -0.0281, -0.0197]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0828,  0.0490,  0.0215,  ..., -0.0398,  0.0429,  0.0398],
        [ 0.0215, -0.0460, -0.0092,  ...,  0.0398,  0.0337,  0.0092],
        [ 0.0644,  0.0398,  0.0245,  ..., -0.0613,  0.0306,  0.0031],
        ...,
        [-0.0153, -0.0153, -0.0245,  ...,  0.0306,  0.0276,  0.0582],
        [-0.0368, -0.0184, -0.0306,  ..., -0.0613,  0.0368, -0.0061],
        [-0.0337,  0.0215,  0.0736,  ..., -0.0092, -0.0276, -0.0184]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3892, -0.3463, -0.3402, -0.3341, -0.3187, -0.3096, -0.3065, -0.3004,
        -0.2850, -0.2666, -0.2605, -0.2574, -0.2452, -0.2421, -0.2360, -0.2299,
        -0.2207, -0.2145, -0.2115, -0.2053, -0.2023, -0.1992, -0.1962, -0.1931,
        -0.1900, -0.1870, -0.1839, -0.1808, -0.1778, -0.1747, -0.1716, -0.1686,
        -0.1655, -0.1624, -0.1594, -0.1563, -0.1532, -0.1502, -0.1471, -0.1440,
        -0.1410, -0.1379, -0.1349, -0.1318, -0.1287, -0.1257, -0.1226, -0.1195,
        -0.1165, -0.1134, -0.1103, -0.1073, -0.1042, -0.1011, -0.0981, -0.0950,
        -0.0919, -0.0889, -0.0858, -0.0828, -0.0797, -0.0766, -0.0736, -0.0705,
        -0.0674, -0.0644, -0.0613, -0.0582, -0.0552, -0.0521, -0.0490, -0.0460,
        -0.0429, -0.0398, -0.0368, -0.0337, -0.0306, -0.0276, -0.0245, -0.0215,
        -0.0184, -0.0153, -0.0123, -0.0092, -0.0061, -0.0031, -0.0000,  0.0031,
         0.0061,  0.0092,  0.0123,  0.0153,  0.0184,  0.0215,  0.0245,  0.0276,
         0.0306,  0.0337,  0.0368,  0.0398,  0.0429,  0.0460,  0.0490,  0.0521,
         0.0552,  0.0582,  0.0613,  0.0644,  0.0674,  0.0705,  0.0736,  0.0766,
         0.0797,  0.0828,  0.0858,  0.0889,  0.0919,  0.0950,  0.0981,  0.1011,
         0.1042,  0.1073,  0.1103,  0.1134,  0.1165,  0.1195,  0.1226,  0.1257,
         0.1287,  0.1318,  0.1349,  0.1379,  0.1410,  0.1440,  0.1471,  0.1502,
         0.1532,  0.1563,  0.1594,  0.1624,  0.1655,  0.1686,  0.1716,  0.1747,
         0.1778,  0.1808,  0.1839,  0.1870,  0.1900,  0.1931,  0.1962,  0.1992,
         0.2023,  0.2053,  0.2084,  0.2115,  0.2145,  0.2207,  0.2237,  0.2268,
         0.2329,  0.2360,  0.2391,  0.2452,  0.2513,  0.2636,  0.2666,  0.2881,
         0.2942,  0.3494], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  170
---------Q_out---------
tensor([[[-0.4870, -1.0350, -0.5479,  ...,  0.3044,  0.3653,  1.0350],
         [-0.4870,  0.9132, -1.4611,  ...,  0.7306, -0.5479,  1.7047],
         [-1.0959, -1.2176, -1.2785,  ...,  2.3135, -1.1567,  2.7397],
         ...,
         [ 1.0350,  0.1218, -0.6088,  ...,  1.4611, -0.3653,  1.5220],
         [ 0.7915, -0.3653,  0.0609,  ...,  1.8873, -0.3044,  1.4611],
         [ 0.9132,  0.0609, -0.3044,  ...,  2.6179, -0.4262,  3.5311]],

        [[-0.7915, -1.0959, -0.8523,  ...,  0.2435,  0.3044,  1.2785],
         [-1.6438, -1.0350, -1.8873,  ..., -0.3653,  1.5829,  2.1917],
         [-1.8264, -1.0350, -1.2176,  ...,  0.7915,  0.6697,  3.0441],
         ...,
         [ 1.2176, -0.6088, -1.5220,  ...,  0.5479,  0.5479, -0.3044],
         [ 1.0350, -0.4870, -0.3044,  ...,  1.4003,  0.4262,  1.2785],
         [ 0.9132, -0.9132, -1.0350,  ...,  0.3653, -0.1826,  1.8264]],

        [[-0.3044, -1.2176, -1.2785,  ...,  0.5479,  0.2435,  0.9132],
         [-1.5220, -0.1826, -0.3044,  ..., -0.6697,  1.4611,  1.8873],
         [-1.4611, -1.8873,  0.4262,  ...,  0.2435, -0.4262,  2.9832],
         ...,
         [ 0.9132, -0.4262, -0.3044,  ...,  1.6438, -0.2435,  1.4003],
         [ 0.9741, -0.4870, -0.1218,  ...,  1.7047,  0.0609,  2.0700],
         [ 0.3044, -0.8523,  0.6697,  ...,  2.5570, -0.9132,  2.6788]],

        ...,

        [[-0.2435, -1.3394, -0.8523,  ...,  0.2435,  0.4870,  1.0959],
         [-0.8523, -0.3044, -0.6697,  ...,  0.6088,  2.1308,  3.0441],
         [-1.2176,  0.1826,  0.3653,  ...,  0.4262,  0.6088,  3.8964],
         ...,
         [ 0.5479,  0.1826, -0.3044,  ...,  1.6438, -0.5479,  1.4611],
         [ 0.7306, -0.6088, -0.1218,  ...,  1.7656,  0.8523,  2.2526],
         [ 0.9132, -0.3653, -0.0609,  ...,  2.9223, -0.5479,  2.6179]],

        [[-0.6088, -1.2785, -1.0350,  ...,  0.1218, -0.1218,  0.9741],
         [-0.0000, -1.1567, -1.0350,  ..., -0.2435,  1.4003,  2.1917],
         [ 0.1218, -1.5829, -0.9741,  ...,  2.1917,  2.9832,  4.5661],
         ...,
         [-0.1218, -0.7306, -0.3653,  ...,  1.1567, -0.1826,  1.3394],
         [-0.6088, -0.4262, -0.4870,  ...,  0.7306,  0.0609,  1.8873],
         [ 0.4870, -0.5479, -0.3653,  ...,  2.3135,  0.1218,  2.8005]],

        [[-0.1826, -1.5220, -0.9132,  ...,  0.3044,  0.2435,  1.1567],
         [-1.4003,  0.9132, -1.0959,  ...,  1.6438,  1.6438,  2.3135],
         [-2.0700,  1.5220, -0.9741,  ...,  0.6697,  0.9741,  2.9223],
         ...,
         [ 0.9132,  0.0000, -0.1826,  ...,  2.0091, -1.1567,  1.8264],
         [ 0.9741,  0.0609, -0.3653,  ...,  1.8264,  0.1826,  2.8005],
         [ 0.9132, -0.6088, -0.2435,  ...,  2.4961, -1.0350,  3.4702]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.7319, -7.6710, -7.5493, -7.4884, -7.4275, -7.2449, -7.1840, -7.1231,
        -7.0622, -7.0013, -6.9405, -6.8796, -6.8187, -6.7578, -6.6969, -6.6361,
        -6.5752, -6.5143, -6.4534, -6.3925, -6.3316, -6.2708, -6.2099, -6.1490,
        -6.0881, -6.0272, -5.9664, -5.9055, -5.8446, -5.7837, -5.7228, -5.6620,
        -5.6011, -5.5402, -5.4793, -5.4184, -5.3575, -5.2967, -5.2358, -5.1749,
        -5.1140, -5.0531, -4.9923, -4.9314, -4.8705, -4.8096, -4.7487, -4.6879,
        -4.6270, -4.5661, -4.5052, -4.4443, -4.3834, -4.3226, -4.2617, -4.2008,
        -4.1399, -4.0790, -4.0182, -3.9573, -3.8964, -3.8355, -3.7746, -3.7138,
        -3.6529, -3.5920, -3.5311, -3.4702, -3.4093, -3.3485, -3.2876, -3.2267,
        -3.1658, -3.1049, -3.0441, -2.9832, -2.9223, -2.8614, -2.8005, -2.7397,
        -2.6788, -2.6179, -2.5570, -2.4961, -2.4352, -2.3744, -2.3135, -2.2526,
        -2.1917, -2.1308, -2.0700, -2.0091, -1.9482, -1.8873, -1.8264, -1.7656,
        -1.7047, -1.6438, -1.5829, -1.5220, -1.4611, -1.4003, -1.3394, -1.2785,
        -1.2176, -1.1567, -1.0959, -1.0350, -0.9741, -0.9132, -0.8523, -0.7915,
        -0.7306, -0.6697, -0.6088, -0.5479, -0.4870, -0.4262, -0.3653, -0.3044,
        -0.2435, -0.1826, -0.1218, -0.0609, -0.0000,  0.0609,  0.1218,  0.1826,
         0.2435,  0.3044,  0.3653,  0.4262,  0.4870,  0.5479,  0.6088,  0.6697,
         0.7306,  0.7915,  0.8523,  0.9132,  0.9741,  1.0350,  1.0959,  1.1567,
         1.2176,  1.2785,  1.3394,  1.4003,  1.4611,  1.5220,  1.5829,  1.6438,
         1.7047,  1.7656,  1.8264,  1.8873,  1.9482,  2.0091,  2.0700,  2.1308,
         2.1917,  2.2526,  2.3135,  2.3744,  2.4352,  2.4961,  2.5570,  2.6179,
         2.6788,  2.7397,  2.8005,  2.8614,  2.9223,  2.9832,  3.0441,  3.1049,
         3.1658,  3.2267,  3.2876,  3.3485,  3.4093,  3.4702,  3.5311,  3.5920,
         3.6529,  3.7138,  3.7746,  3.8355,  3.8964,  3.9573,  4.0182,  4.0790,
         4.1399,  4.2008,  4.2617,  4.3226,  4.3834,  4.4443,  4.5052,  4.5661,
         4.6270,  4.6879,  4.7487,  4.8096,  4.8705,  4.9314,  4.9923,  5.0531,
         5.1140,  5.1749,  5.2358,  5.2967,  5.3575,  5.4184,  5.4793,  5.5402,
         5.6011,  5.6620,  5.7228,  5.7837,  5.8446,  5.9055,  5.9664,  6.0272,
         6.0881,  6.1490,  6.2099,  6.2708,  6.3316,  6.3925,  6.4534,  6.5143,
         6.5752,  6.6969,  6.8187,  6.8796,  6.9405,  7.0013,  7.0622,  7.1231,
         7.2449,  7.3057,  7.3666,  7.4275,  7.5493,  7.7319], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  246
name:  bert.encoder.layer.4.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0693, -0.0506,  0.0166,  ..., -0.0393, -0.0415,  0.0028],
        [ 0.0813, -0.0139,  0.0244,  ..., -0.0614, -0.0014, -0.0194],
        [-0.0455,  0.0163, -0.0602,  ..., -0.0187,  0.0729, -0.0757],
        ...,
        [-0.0116,  0.0192,  0.0136,  ...,  0.0993, -0.0254,  0.0540],
        [ 0.0206,  0.0434,  0.0611,  ...,  0.0017, -0.0706,  0.0504],
        [-0.0270,  0.0429,  0.0070,  ...,  0.0548,  0.0393,  0.0662]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0708, -0.0493,  0.0154,  ..., -0.0400, -0.0400,  0.0031],
        [ 0.0800, -0.0154,  0.0246,  ..., -0.0616, -0.0000, -0.0185],
        [-0.0462,  0.0154, -0.0616,  ..., -0.0185,  0.0739, -0.0770],
        ...,
        [-0.0123,  0.0185,  0.0123,  ...,  0.0985, -0.0246,  0.0554],
        [ 0.0215,  0.0431,  0.0616,  ...,  0.0031, -0.0708,  0.0493],
        [-0.0277,  0.0431,  0.0062,  ...,  0.0554,  0.0400,  0.0646]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3663, -0.3294, -0.3232, -0.3048, -0.2955, -0.2894, -0.2647, -0.2617,
        -0.2586, -0.2493, -0.2370, -0.2340, -0.2309, -0.2278, -0.2216, -0.2186,
        -0.2155, -0.2124, -0.2093, -0.2063, -0.2032, -0.2001, -0.1970, -0.1939,
        -0.1909, -0.1878, -0.1847, -0.1816, -0.1785, -0.1755, -0.1724, -0.1693,
        -0.1662, -0.1632, -0.1601, -0.1570, -0.1539, -0.1508, -0.1478, -0.1447,
        -0.1416, -0.1385, -0.1354, -0.1324, -0.1293, -0.1262, -0.1231, -0.1201,
        -0.1170, -0.1139, -0.1108, -0.1077, -0.1047, -0.1016, -0.0985, -0.0954,
        -0.0924, -0.0893, -0.0862, -0.0831, -0.0800, -0.0770, -0.0739, -0.0708,
        -0.0677, -0.0646, -0.0616, -0.0585, -0.0554, -0.0523, -0.0493, -0.0462,
        -0.0431, -0.0400, -0.0369, -0.0339, -0.0308, -0.0277, -0.0246, -0.0215,
        -0.0185, -0.0154, -0.0123, -0.0092, -0.0062, -0.0031, -0.0000,  0.0031,
         0.0062,  0.0092,  0.0123,  0.0154,  0.0185,  0.0215,  0.0246,  0.0277,
         0.0308,  0.0339,  0.0369,  0.0400,  0.0431,  0.0462,  0.0493,  0.0523,
         0.0554,  0.0585,  0.0616,  0.0646,  0.0677,  0.0708,  0.0739,  0.0770,
         0.0800,  0.0831,  0.0862,  0.0893,  0.0924,  0.0954,  0.0985,  0.1016,
         0.1047,  0.1077,  0.1108,  0.1139,  0.1170,  0.1201,  0.1231,  0.1262,
         0.1293,  0.1324,  0.1354,  0.1385,  0.1416,  0.1447,  0.1478,  0.1508,
         0.1539,  0.1570,  0.1601,  0.1632,  0.1662,  0.1693,  0.1724,  0.1755,
         0.1785,  0.1816,  0.1847,  0.1878,  0.1909,  0.1939,  0.1970,  0.2001,
         0.2032,  0.2063,  0.2093,  0.2124,  0.2216,  0.2247,  0.2340,  0.2370,
         0.2678,  0.2771,  0.3171,  0.3202,  0.3448,  0.3602,  0.3910],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  167
---------Q_out---------
tensor([[[ 0.6663,  0.0000, -0.1666,  ...,  0.0555, -0.2776, -0.1666],
         [-0.7773, -0.2776, -0.5552,  ...,  1.7212, -0.2221,  0.2776],
         [-0.4442,  0.4997,  0.6107,  ...,  0.2221, -1.6657,  0.3331],
         ...,
         [ 0.1666,  0.2221,  0.9439,  ...,  1.8877, -0.9994,  0.0555],
         [-0.0555, -0.0555,  0.6107,  ...,  1.2215, -0.2776, -0.0000],
         [-0.4442,  0.3887,  0.6107,  ...,  1.4991, -0.3887,  0.8884]],

        [[ 0.5552,  0.0000,  0.1110,  ...,  0.3331, -0.1110,  0.0555],
         [-0.7773, -0.0555, -0.0555,  ...,  2.3874,  0.1110,  0.5552],
         [-1.6101,  0.6107,  0.6107,  ...,  0.7218,  0.4997,  1.2770],
         ...,
         [ 0.3331, -1.1104,  0.8328,  ...,  1.9988, -0.7773, -0.6663],
         [-0.4442,  0.3887, -0.3331,  ...,  0.4997,  0.8884, -0.8884],
         [ 0.1666,  0.2776,  0.0000,  ...,  0.4442, -0.2776,  1.4436]],

        [[ 0.6107, -0.1110, -0.0555,  ...,  0.1110, -0.6107, -0.0555],
         [-0.0000,  0.9439,  0.4997,  ...,  0.7218, -0.6107,  0.8884],
         [-0.3887,  0.6107,  2.2209,  ..., -0.7218,  0.5552,  0.9994],
         ...,
         [ 0.1110,  0.3331,  0.7218,  ...,  2.4430, -1.4991, -0.3887],
         [ 0.2221,  0.7773,  1.0549,  ...,  1.7767, -1.1660, -0.2221],
         [-0.1110,  0.2776,  1.0549,  ...,  1.5546, -0.0000,  1.1660]],

        ...,

        [[ 0.7218,  0.5552, -0.1110,  ..., -0.1110, -0.3331,  0.0000],
         [-1.8877,  3.4424,  0.2776,  ...,  2.2209,  0.3887,  0.6107],
         [-2.3319,  2.3874,  0.3887,  ...,  0.3887,  1.2215,  1.7767],
         ...,
         [-0.1110,  0.3331,  0.3887,  ...,  2.7206, -0.3331,  0.1110],
         [ 0.0555,  0.6107,  0.1110,  ...,  2.0543, -0.1666,  0.2776],
         [ 0.4997,  0.8328,  0.4997,  ...,  2.0543,  0.2776,  0.6107]],

        [[ 0.5552, -0.3887, -0.5552,  ...,  0.3331, -0.1666, -0.1110],
         [-1.7212,  0.3331,  0.1110,  ...,  1.0549,  0.0555,  1.1104],
         [ 0.6107,  0.6663, -0.2776,  ...,  0.7218,  0.1666,  0.7773],
         ...,
         [-0.2776,  0.3887,  0.4442,  ...,  2.3319, -0.8884,  0.6663],
         [-0.5552,  0.8328, -0.1666,  ...,  1.6657,  0.0555,  0.7218],
         [-0.4442,  0.9439,  0.1666,  ...,  2.5540, -0.2221,  0.6107]],

        [[ 0.1110, -0.4997, -0.3887,  ...,  0.2776, -0.2776,  0.0000],
         [-0.5552,  2.1098,  0.4442,  ...,  0.8328,  0.2776,  0.8328],
         [-0.1666,  1.2215,  1.1104,  ..., -0.3887,  0.4997,  0.1666],
         ...,
         [-0.1666,  0.8884,  0.8884,  ...,  2.7206, -0.5552,  0.2776],
         [-0.2776,  0.3887,  0.6107,  ...,  1.9433, -0.6663,  0.6663],
         [-0.3887,  1.2215,  1.2215,  ...,  2.7206, -0.6107,  0.7773]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-6.8292, -6.6626, -6.4406, -6.3850, -6.3295, -6.2740, -6.1629, -6.1074,
        -6.0519, -5.9964, -5.8298, -5.7188, -5.6632, -5.6077, -5.5522, -5.4967,
        -5.4412, -5.3856, -5.3301, -5.2746, -5.2191, -5.1635, -5.1080, -5.0525,
        -4.9970, -4.9415, -4.8859, -4.8304, -4.7749, -4.7194, -4.6638, -4.6083,
        -4.5528, -4.4973, -4.4418, -4.3862, -4.3307, -4.2752, -4.2197, -4.1642,
        -4.1086, -4.0531, -3.9976, -3.9421, -3.8865, -3.8310, -3.7755, -3.7200,
        -3.6645, -3.6089, -3.5534, -3.4979, -3.4424, -3.3868, -3.3313, -3.2758,
        -3.2203, -3.1648, -3.1092, -3.0537, -2.9982, -2.9427, -2.8871, -2.8316,
        -2.7761, -2.7206, -2.6651, -2.6095, -2.5540, -2.4985, -2.4430, -2.3874,
        -2.3319, -2.2764, -2.2209, -2.1654, -2.1098, -2.0543, -1.9988, -1.9433,
        -1.8877, -1.8322, -1.7767, -1.7212, -1.6657, -1.6101, -1.5546, -1.4991,
        -1.4436, -1.3881, -1.3325, -1.2770, -1.2215, -1.1660, -1.1104, -1.0549,
        -0.9994, -0.9439, -0.8884, -0.8328, -0.7773, -0.7218, -0.6663, -0.6107,
        -0.5552, -0.4997, -0.4442, -0.3887, -0.3331, -0.2776, -0.2221, -0.1666,
        -0.1110, -0.0555, -0.0000,  0.0555,  0.1110,  0.1666,  0.2221,  0.2776,
         0.3331,  0.3887,  0.4442,  0.4997,  0.5552,  0.6107,  0.6663,  0.7218,
         0.7773,  0.8328,  0.8884,  0.9439,  0.9994,  1.0549,  1.1104,  1.1660,
         1.2215,  1.2770,  1.3325,  1.3881,  1.4436,  1.4991,  1.5546,  1.6101,
         1.6657,  1.7212,  1.7767,  1.8322,  1.8877,  1.9433,  1.9988,  2.0543,
         2.1098,  2.1654,  2.2209,  2.2764,  2.3319,  2.3874,  2.4430,  2.4985,
         2.5540,  2.6095,  2.6651,  2.7206,  2.7761,  2.8316,  2.8871,  2.9427,
         2.9982,  3.0537,  3.1092,  3.1648,  3.2203,  3.2758,  3.3313,  3.3868,
         3.4424,  3.4979,  3.5534,  3.6089,  3.6645,  3.7200,  3.7755,  3.8310,
         3.8865,  3.9421,  3.9976,  4.0531,  4.1086,  4.1642,  4.2197,  4.2752,
         4.3307,  4.3862,  4.4418,  4.4973,  4.5528,  4.6083,  4.6638,  4.7194,
         4.7749,  4.8304,  4.8859,  4.9415,  4.9970,  5.0525,  5.1080,  5.1635,
         5.2191,  5.2746,  5.3301,  5.3856,  5.4412,  5.4967,  5.5522,  5.6077,
         5.6632,  5.7188,  5.7743,  5.8298,  5.8853,  5.9409,  5.9964,  6.0519,
         6.4406,  6.7182,  6.8292], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  227
name:  bert.encoder.layer.4.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0688, -0.0005, -0.0441,  ..., -0.0869,  0.0584,  0.0037],
        [ 0.0664,  0.0439, -0.0548,  ..., -0.0032, -0.0519, -0.0214],
        [ 0.0122, -0.0037,  0.0059,  ...,  0.0559,  0.0590, -0.0132],
        ...,
        [ 0.0221, -0.0752, -0.0456,  ..., -0.0389,  0.0116, -0.0145],
        [ 0.0129,  0.0171, -0.0370,  ...,  0.0105, -0.0074, -0.0254],
        [-0.0074, -0.0167, -0.0606,  ..., -0.0187,  0.1162,  0.0149]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0692, -0.0000, -0.0445,  ..., -0.0873,  0.0577,  0.0033],
        [ 0.0659,  0.0445, -0.0544,  ..., -0.0033, -0.0511, -0.0214],
        [ 0.0115, -0.0033,  0.0066,  ...,  0.0560,  0.0593, -0.0132],
        ...,
        [ 0.0214, -0.0758, -0.0461,  ..., -0.0395,  0.0115, -0.0148],
        [ 0.0132,  0.0165, -0.0362,  ...,  0.0099, -0.0082, -0.0247],
        [-0.0066, -0.0165, -0.0610,  ..., -0.0181,  0.1170,  0.0148]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2092, -0.1779, -0.1763, -0.1746, -0.1713, -0.1697, -0.1680, -0.1664,
        -0.1647, -0.1631, -0.1614, -0.1598, -0.1581, -0.1565, -0.1549, -0.1532,
        -0.1516, -0.1499, -0.1483, -0.1466, -0.1450, -0.1433, -0.1417, -0.1400,
        -0.1384, -0.1367, -0.1351, -0.1334, -0.1318, -0.1301, -0.1285, -0.1268,
        -0.1252, -0.1236, -0.1219, -0.1203, -0.1186, -0.1170, -0.1153, -0.1137,
        -0.1120, -0.1104, -0.1087, -0.1071, -0.1054, -0.1038, -0.1021, -0.1005,
        -0.0988, -0.0972, -0.0955, -0.0939, -0.0923, -0.0906, -0.0890, -0.0873,
        -0.0857, -0.0840, -0.0824, -0.0807, -0.0791, -0.0774, -0.0758, -0.0741,
        -0.0725, -0.0708, -0.0692, -0.0675, -0.0659, -0.0642, -0.0626, -0.0610,
        -0.0593, -0.0577, -0.0560, -0.0544, -0.0527, -0.0511, -0.0494, -0.0478,
        -0.0461, -0.0445, -0.0428, -0.0412, -0.0395, -0.0379, -0.0362, -0.0346,
        -0.0329, -0.0313, -0.0297, -0.0280, -0.0264, -0.0247, -0.0231, -0.0214,
        -0.0198, -0.0181, -0.0165, -0.0148, -0.0132, -0.0115, -0.0099, -0.0082,
        -0.0066, -0.0049, -0.0033, -0.0016, -0.0000,  0.0016,  0.0033,  0.0049,
         0.0066,  0.0082,  0.0099,  0.0115,  0.0132,  0.0148,  0.0165,  0.0181,
         0.0198,  0.0214,  0.0231,  0.0247,  0.0264,  0.0280,  0.0297,  0.0313,
         0.0329,  0.0346,  0.0362,  0.0379,  0.0395,  0.0412,  0.0428,  0.0445,
         0.0461,  0.0478,  0.0494,  0.0511,  0.0527,  0.0544,  0.0560,  0.0577,
         0.0593,  0.0610,  0.0626,  0.0642,  0.0659,  0.0675,  0.0692,  0.0708,
         0.0725,  0.0741,  0.0758,  0.0774,  0.0791,  0.0807,  0.0824,  0.0840,
         0.0857,  0.0873,  0.0890,  0.0906,  0.0923,  0.0939,  0.0955,  0.0972,
         0.0988,  0.1005,  0.1021,  0.1038,  0.1054,  0.1071,  0.1087,  0.1104,
         0.1120,  0.1137,  0.1153,  0.1170,  0.1186,  0.1203,  0.1219,  0.1236,
         0.1252,  0.1268,  0.1285,  0.1301,  0.1318,  0.1334,  0.1351,  0.1367,
         0.1384,  0.1400,  0.1417,  0.1433,  0.1450,  0.1466,  0.1483,  0.1499,
         0.1516,  0.1532,  0.1549,  0.1565,  0.1581,  0.1598,  0.1614,  0.1631,
         0.1664,  0.1680,  0.1697,  0.1713,  0.1746,  0.1779,  0.1796,  0.1812,
         0.1829,  0.1878,  0.1944,  0.2026,  0.2092], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  221
---------Q_out---------
None
name:  bert.encoder.layer.4.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0228,  0.0410,  0.0125,  ...,  0.0153, -0.0158,  0.0225],
        [-0.0418,  0.0037,  0.0365,  ..., -0.0167,  0.0040, -0.0537],
        [-0.0147, -0.0064, -0.0043,  ..., -0.0014, -0.0144, -0.0367],
        ...,
        [-0.0725, -0.0208,  0.0467,  ...,  0.0559,  0.0098, -0.0374],
        [-0.0035, -0.0392,  0.0381,  ...,  0.0437, -0.0363,  0.0106],
        [-0.0234, -0.0046, -0.0148,  ...,  0.0016, -0.0019, -0.0563]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0233,  0.0414,  0.0129,  ...,  0.0155, -0.0155,  0.0233],
        [-0.0414,  0.0026,  0.0363,  ..., -0.0155,  0.0052, -0.0544],
        [-0.0155, -0.0052, -0.0052,  ..., -0.0026, -0.0155, -0.0363],
        ...,
        [-0.0725, -0.0207,  0.0466,  ...,  0.0570,  0.0104, -0.0363],
        [-0.0026, -0.0388,  0.0388,  ...,  0.0440, -0.0363,  0.0104],
        [-0.0233, -0.0052, -0.0155,  ...,  0.0026, -0.0026, -0.0570]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3289, -0.3030, -0.2512, -0.2124, -0.2072, -0.2046, -0.1942, -0.1916,
        -0.1839, -0.1787, -0.1761, -0.1709, -0.1683, -0.1657, -0.1631, -0.1606,
        -0.1580, -0.1554, -0.1528, -0.1502, -0.1476, -0.1450, -0.1424, -0.1398,
        -0.1373, -0.1347, -0.1321, -0.1295, -0.1269, -0.1243, -0.1217, -0.1191,
        -0.1165, -0.1139, -0.1114, -0.1088, -0.1062, -0.1036, -0.1010, -0.0984,
        -0.0958, -0.0932, -0.0906, -0.0880, -0.0855, -0.0829, -0.0803, -0.0777,
        -0.0751, -0.0725, -0.0699, -0.0673, -0.0647, -0.0622, -0.0596, -0.0570,
        -0.0544, -0.0518, -0.0492, -0.0466, -0.0440, -0.0414, -0.0388, -0.0363,
        -0.0337, -0.0311, -0.0285, -0.0259, -0.0233, -0.0207, -0.0181, -0.0155,
        -0.0129, -0.0104, -0.0078, -0.0052, -0.0026, -0.0000,  0.0026,  0.0052,
         0.0078,  0.0104,  0.0129,  0.0155,  0.0181,  0.0207,  0.0233,  0.0259,
         0.0285,  0.0311,  0.0337,  0.0363,  0.0388,  0.0414,  0.0440,  0.0466,
         0.0492,  0.0518,  0.0544,  0.0570,  0.0596,  0.0622,  0.0647,  0.0673,
         0.0699,  0.0725,  0.0751,  0.0777,  0.0803,  0.0829,  0.0855,  0.0880,
         0.0906,  0.0932,  0.0958,  0.0984,  0.1010,  0.1036,  0.1062,  0.1088,
         0.1114,  0.1139,  0.1165,  0.1191,  0.1217,  0.1243,  0.1269,  0.1295,
         0.1321,  0.1347,  0.1373,  0.1398,  0.1424,  0.1450,  0.1476,  0.1502,
         0.1528,  0.1554,  0.1580,  0.1606,  0.1631,  0.1657,  0.1683,  0.1709,
         0.1735,  0.1761,  0.1787,  0.1813,  0.1839,  0.1865,  0.1890,  0.1916,
         0.2201,  0.2253,  0.2279,  0.2357], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  156
---------Q_out---------
None
name:  bert.encoder.layer.4.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0064, -0.0216,  0.0332,  ..., -0.0442, -0.0336, -0.0122],
        [ 0.0621,  0.0101,  0.0709,  ...,  0.0285, -0.0548, -0.0223],
        [-0.0319, -0.0077,  0.0148,  ..., -0.0471,  0.0225, -0.0042],
        ...,
        [-0.0008,  0.0227,  0.0378,  ..., -0.0376, -0.0318,  0.0036],
        [ 0.0014, -0.0293, -0.0535,  ...,  0.0377,  0.0367, -0.0148],
        [-0.0174,  0.0709,  0.0240,  ..., -0.0105,  0.0548,  0.0343]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0068, -0.0204,  0.0340,  ..., -0.0408, -0.0340, -0.0136],
        [ 0.0613,  0.0068,  0.0681,  ...,  0.0272, -0.0545, -0.0204],
        [-0.0340, -0.0068,  0.0136,  ..., -0.0476,  0.0204, -0.0068],
        ...,
        [-0.0000,  0.0204,  0.0408,  ..., -0.0408, -0.0340,  0.0068],
        [ 0.0000, -0.0272, -0.0545,  ...,  0.0408,  0.0340, -0.0136],
        [-0.0204,  0.0681,  0.0272,  ..., -0.0136,  0.0545,  0.0340]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.5377, -0.4764, -0.4424, -0.4288, -0.4016, -0.3880, -0.3812, -0.3607,
        -0.3539, -0.3471, -0.3403, -0.3335, -0.3267, -0.3199, -0.3131, -0.3063,
        -0.2995, -0.2927, -0.2859, -0.2791, -0.2723, -0.2654, -0.2586, -0.2518,
        -0.2450, -0.2382, -0.2314, -0.2246, -0.2178, -0.2110, -0.2042, -0.1974,
        -0.1906, -0.1838, -0.1770, -0.1702, -0.1634, -0.1565, -0.1497, -0.1429,
        -0.1361, -0.1293, -0.1225, -0.1157, -0.1089, -0.1021, -0.0953, -0.0885,
        -0.0817, -0.0749, -0.0681, -0.0613, -0.0545, -0.0476, -0.0408, -0.0340,
        -0.0272, -0.0204, -0.0136, -0.0068, -0.0000,  0.0068,  0.0136,  0.0204,
         0.0272,  0.0340,  0.0408,  0.0476,  0.0545,  0.0613,  0.0681,  0.0749,
         0.0817,  0.0885,  0.0953,  0.1021,  0.1089,  0.1157,  0.1225,  0.1293,
         0.1361,  0.1429,  0.1497,  0.1565,  0.1634,  0.1702,  0.1770,  0.1838,
         0.1906,  0.1974,  0.2042,  0.2110,  0.2178,  0.2246,  0.2314,  0.2382,
         0.2450,  0.2518,  0.2586,  0.2654,  0.2723,  0.2791,  0.2859,  0.2927,
         0.3063,  0.3131,  0.3199,  0.3267,  0.3403,  0.3675,  0.3743,  0.3948,
         0.4016,  0.4356,  0.4560,  0.5241,  0.5377,  0.5990,  0.7419,  0.8644],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  120
---------Q_out---------
None
name:  bert.encoder.layer.4.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0541,  0.0419, -0.0154,  ..., -0.0147,  0.0068, -0.0043],
        [ 0.0222,  0.0091,  0.0430,  ...,  0.0187,  0.0771,  0.0204],
        [-0.0267,  0.0820,  0.0108,  ..., -0.0304, -0.0103,  0.0183],
        ...,
        [-0.0191,  0.0098, -0.0731,  ..., -0.0333,  0.0218,  0.0141],
        [ 0.0138,  0.0554,  0.0410,  ..., -0.0783,  0.0022, -0.0388],
        [-0.0573, -0.0401, -0.0049,  ..., -0.0020, -0.0262, -0.0302]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0537,  0.0537, -0.0000,  ..., -0.0000,  0.0000, -0.0000],
        [ 0.0000,  0.0000,  0.0537,  ...,  0.0000,  0.0537,  0.0000],
        [-0.0000,  0.1074,  0.0000,  ..., -0.0537, -0.0000,  0.0000],
        ...,
        [-0.0000,  0.0000, -0.0537,  ..., -0.0537,  0.0000,  0.0000],
        [ 0.0000,  0.0537,  0.0537,  ..., -0.0537,  0.0000, -0.0537],
        [-0.0537, -0.0537, -0.0000,  ..., -0.0000, -0.0000, -0.0537]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-6.8177, -3.1136, -0.9663, -0.6442, -0.5905, -0.5368, -0.4831, -0.4295,
        -0.3758, -0.3221, -0.2684, -0.2147, -0.1610, -0.1074, -0.0537, -0.0000,
         0.0537,  0.1074,  0.1610,  0.2147,  0.2684,  0.3221,  0.3758,  0.4295,
         0.4831,  0.5368,  0.5905,  0.6442,  0.6979,  0.7516,  0.8052,  0.8589,
         0.9126,  1.2884,  1.9326], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  35
---------Q_out---------
None
name:  bert.encoder.layer.5.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0085,  0.0010,  0.0339,  ...,  0.0473, -0.0109,  0.0123],
        [-0.0391,  0.0080, -0.0889,  ...,  0.0764, -0.0024,  0.0354],
        [ 0.0255,  0.0335,  0.0309,  ...,  0.0515,  0.0003,  0.0151],
        ...,
        [-0.0118,  0.0518,  0.0148,  ..., -0.0492, -0.0295,  0.0240],
        [ 0.0075,  0.0114, -0.0347,  ..., -0.0419,  0.0075,  0.0507],
        [ 0.0413, -0.0986,  0.0466,  ...,  0.0614, -0.0103,  0.0964]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0087,  0.0000,  0.0347,  ...,  0.0463, -0.0116,  0.0116],
        [-0.0405,  0.0087, -0.0896,  ...,  0.0752, -0.0029,  0.0347],
        [ 0.0260,  0.0347,  0.0318,  ...,  0.0520,  0.0000,  0.0145],
        ...,
        [-0.0116,  0.0520,  0.0145,  ..., -0.0492, -0.0289,  0.0231],
        [ 0.0087,  0.0116, -0.0347,  ..., -0.0434,  0.0087,  0.0520],
        [ 0.0405, -0.0983,  0.0463,  ...,  0.0607, -0.0116,  0.0954]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3412, -0.2718, -0.2660, -0.2400, -0.2371, -0.2342, -0.2197, -0.2111,
        -0.2082, -0.2053, -0.1995, -0.1966, -0.1937, -0.1908, -0.1879, -0.1850,
        -0.1821, -0.1793, -0.1764, -0.1735, -0.1706, -0.1677, -0.1648, -0.1619,
        -0.1590, -0.1561, -0.1532, -0.1503, -0.1475, -0.1446, -0.1417, -0.1388,
        -0.1359, -0.1330, -0.1301, -0.1272, -0.1243, -0.1214, -0.1185, -0.1157,
        -0.1128, -0.1099, -0.1070, -0.1041, -0.1012, -0.0983, -0.0954, -0.0925,
        -0.0896, -0.0867, -0.0838, -0.0810, -0.0781, -0.0752, -0.0723, -0.0694,
        -0.0665, -0.0636, -0.0607, -0.0578, -0.0549, -0.0520, -0.0492, -0.0463,
        -0.0434, -0.0405, -0.0376, -0.0347, -0.0318, -0.0289, -0.0260, -0.0231,
        -0.0202, -0.0173, -0.0145, -0.0116, -0.0087, -0.0058, -0.0029, -0.0000,
         0.0029,  0.0058,  0.0087,  0.0116,  0.0145,  0.0173,  0.0202,  0.0231,
         0.0260,  0.0289,  0.0318,  0.0347,  0.0376,  0.0405,  0.0434,  0.0463,
         0.0492,  0.0520,  0.0549,  0.0578,  0.0607,  0.0636,  0.0665,  0.0694,
         0.0723,  0.0752,  0.0781,  0.0810,  0.0838,  0.0867,  0.0896,  0.0925,
         0.0954,  0.0983,  0.1012,  0.1041,  0.1070,  0.1099,  0.1128,  0.1157,
         0.1185,  0.1214,  0.1243,  0.1272,  0.1301,  0.1330,  0.1359,  0.1388,
         0.1417,  0.1446,  0.1475,  0.1503,  0.1532,  0.1561,  0.1590,  0.1619,
         0.1648,  0.1677,  0.1706,  0.1735,  0.1764,  0.1793,  0.1821,  0.1850,
         0.1879,  0.1908,  0.1937,  0.1966,  0.1995,  0.2024,  0.2053,  0.2082,
         0.2111,  0.2140,  0.2168,  0.2226,  0.2255,  0.2284,  0.2313,  0.2342,
         0.2458,  0.2544,  0.2573,  0.2602,  0.2949,  0.3094,  0.3180,  0.3441,
         0.3672], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  169
---------Q_out---------
tensor([[[ 0.2586, -0.1293, -1.8101,  ...,  0.5818, -1.6808, -0.2586],
         [-0.3879,  0.7111,  0.1293,  ...,  2.5859, -2.3273,  0.5818],
         [ 0.9051, -0.3232,  0.0646,  ...,  2.8445, -3.6202, -0.1293],
         ...,
         [ 1.3576, -0.1293, -0.1939,  ...,  1.2929, -3.0384, -0.3232],
         [ 1.6808, -0.1939, -0.0646,  ...,  0.9051, -2.1980,  0.0000],
         [ 1.8748,  0.5818,  0.0646,  ...,  0.8404, -2.7798, -1.0990]],

        [[ 0.3232,  0.7758, -2.0040,  ...,  0.2586, -1.6162, -0.2586],
         [-0.1293,  1.4222, -0.0646,  ...,  0.9697, -3.1030,  1.0990],
         [ 0.0646,  0.6465,  0.3232,  ...,  1.2929, -2.9737,  1.2929],
         ...,
         [ 1.5515,  0.3232,  0.0646,  ...,  0.3879, -3.3616, -0.9697],
         [ 1.1636,  0.1939, -1.2283,  ..., -0.5172, -2.2626, -0.5818],
         [-0.3879,  0.0646, -0.0646,  ..., -0.3879, -2.2626, -0.1939]],

        [[ 0.2586,  0.1293, -1.7455,  ...,  1.0343, -1.3576, -0.5818],
         [ 0.2586, -0.0646, -1.6162,  ...,  1.4222, -1.8101, -0.4525],
         [-0.1939,  0.3879,  0.3879,  ..., -0.7111, -3.6849, -0.4525],
         ...,
         [ 1.0343, -0.0646, -0.1939,  ...,  1.0343, -1.6162,  0.1939],
         [ 1.9394,  0.3879, -0.1939,  ...,  0.9697, -2.0040, -0.9051],
         [ 2.2626,  0.3879,  0.1293,  ...,  0.4525, -2.0687, -0.6465]],

        ...,

        [[-0.0646,  0.1939, -1.8101,  ...,  0.0000, -1.2283, -0.5172],
         [-1.1636,  0.4525, -0.1293,  ...,  0.8404, -5.0424,  1.1636],
         [-1.6162,  0.0000, -0.5172,  ..., -0.7111, -4.9131,  1.0343],
         ...,
         [ 1.4869,  0.9051,  0.0646,  ...,  0.3879, -2.6505,  0.3232],
         [ 1.6808,  0.8404,  0.0646,  ...,  1.0990, -2.0687,  0.4525],
         [ 1.5515,  0.7758,  0.2586,  ...,  0.3232, -2.5859, -0.5818]],

        [[ 0.1939,  0.3232, -1.8748,  ...,  0.3232, -1.8101, -0.5818],
         [ 0.0000,  2.1333, -0.7758,  ...,  1.4869, -3.6849,  2.4566],
         [ 0.3879,  0.3879, -1.4222,  ...,  0.1293, -3.1030,  0.1939],
         ...,
         [ 1.3576, -0.0646, -0.1293,  ...,  0.8404, -1.8748,  0.2586],
         [ 1.4222, -0.1939, -0.5172,  ...,  0.8404, -1.4222,  0.5172],
         [ 1.6162, -0.1939, -0.3879,  ...,  0.3879, -2.5859, -0.4525]],

        [[-0.3232,  0.3232, -1.6162,  ...,  0.5172, -1.4222, -0.4525],
         [-0.2586,  0.1939, -0.5172,  ..., -1.3576, -2.3919,  2.2626],
         [ 0.7758,  1.0343,  0.5172,  ...,  0.5172, -3.5556,  1.0990],
         ...,
         [ 1.0343,  0.0646,  0.4525,  ...,  1.1636, -2.7798, -0.1939],
         [ 2.3273,  0.5818,  0.1939,  ...,  0.8404, -2.5212,  0.1939],
         [ 1.8748,  0.3232,  0.0000,  ...,  0.7758, -2.5212, -0.9051]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.6930, -7.4990, -7.4344, -7.3051, -7.1758, -7.1111, -7.0465, -6.9818,
        -6.9172, -6.8525, -6.7879, -6.7233, -6.6586, -6.5940, -6.5293, -6.4647,
        -6.4000, -6.3354, -6.2707, -6.2061, -6.1414, -6.0768, -6.0121, -5.9475,
        -5.8828, -5.8182, -5.7536, -5.6889, -5.6243, -5.5596, -5.4950, -5.4303,
        -5.3657, -5.3010, -5.2364, -5.1717, -5.1071, -5.0424, -4.9778, -4.9131,
        -4.8485, -4.7839, -4.7192, -4.6546, -4.5899, -4.5253, -4.4606, -4.3960,
        -4.3313, -4.2667, -4.2020, -4.1374, -4.0727, -4.0081, -3.9434, -3.8788,
        -3.8142, -3.7495, -3.6849, -3.6202, -3.5556, -3.4909, -3.4263, -3.3616,
        -3.2970, -3.2323, -3.1677, -3.1030, -3.0384, -2.9737, -2.9091, -2.8445,
        -2.7798, -2.7152, -2.6505, -2.5859, -2.5212, -2.4566, -2.3919, -2.3273,
        -2.2626, -2.1980, -2.1333, -2.0687, -2.0040, -1.9394, -1.8748, -1.8101,
        -1.7455, -1.6808, -1.6162, -1.5515, -1.4869, -1.4222, -1.3576, -1.2929,
        -1.2283, -1.1636, -1.0990, -1.0343, -0.9697, -0.9051, -0.8404, -0.7758,
        -0.7111, -0.6465, -0.5818, -0.5172, -0.4525, -0.3879, -0.3232, -0.2586,
        -0.1939, -0.1293, -0.0646, -0.0000,  0.0646,  0.1293,  0.1939,  0.2586,
         0.3232,  0.3879,  0.4525,  0.5172,  0.5818,  0.6465,  0.7111,  0.7758,
         0.8404,  0.9051,  0.9697,  1.0343,  1.0990,  1.1636,  1.2283,  1.2929,
         1.3576,  1.4222,  1.4869,  1.5515,  1.6162,  1.6808,  1.7455,  1.8101,
         1.8748,  1.9394,  2.0040,  2.0687,  2.1333,  2.1980,  2.2626,  2.3273,
         2.3919,  2.4566,  2.5212,  2.5859,  2.6505,  2.7152,  2.7798,  2.8445,
         2.9091,  2.9737,  3.0384,  3.1030,  3.1677,  3.2323,  3.2970,  3.3616,
         3.4263,  3.4909,  3.5556,  3.6202,  3.6849,  3.7495,  3.8142,  3.8788,
         3.9434,  4.0081,  4.0727,  4.1374,  4.2020,  4.2667,  4.3313,  4.3960,
         4.4606,  4.5253,  4.5899,  4.6546,  4.7192,  4.7839,  4.8485,  4.9131,
         4.9778,  5.0424,  5.1071,  5.1717,  5.2364,  5.3010,  5.3657,  5.4303,
         5.4950,  5.5596,  5.6243,  5.6889,  5.7536,  5.8182,  5.8828,  5.9475,
         6.0121,  6.0768,  6.1414,  6.2061,  6.2707,  6.3354,  6.4000,  6.4647,
         6.5293,  6.5940,  6.6586,  6.7233,  6.7879,  6.8525,  6.9172,  6.9818,
         7.0465,  7.1111,  7.1758,  7.2404,  7.3051,  7.3697,  7.4344,  7.6283,
         7.6930,  7.8869,  7.9515,  8.0808,  8.1455], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  237
name:  bert.encoder.layer.5.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0034,  0.0259, -0.0482,  ..., -0.0006,  0.0190,  0.0260],
        [ 0.0234, -0.0923, -0.0535,  ...,  0.0255,  0.0035, -0.0291],
        [-0.0038,  0.0401, -0.0242,  ...,  0.0313,  0.0296,  0.0240],
        ...,
        [-0.0316, -0.0161, -0.0153,  ...,  0.0316, -0.0102,  0.0278],
        [ 0.0212,  0.0142, -0.0032,  ...,  0.0172, -0.0527, -0.0105],
        [ 0.0440,  0.0149,  0.0594,  ..., -0.0163,  0.0192, -0.0046]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0028,  0.0255, -0.0481,  ..., -0.0000,  0.0198,  0.0255],
        [ 0.0226, -0.0934, -0.0538,  ...,  0.0255,  0.0028, -0.0283],
        [-0.0028,  0.0396, -0.0255,  ...,  0.0311,  0.0283,  0.0226],
        ...,
        [-0.0311, -0.0170, -0.0141,  ...,  0.0311, -0.0113,  0.0283],
        [ 0.0198,  0.0141, -0.0028,  ...,  0.0170, -0.0538, -0.0113],
        [ 0.0453,  0.0141,  0.0594,  ..., -0.0170,  0.0198, -0.0057]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3226, -0.3028, -0.2858, -0.2575, -0.2490, -0.2434, -0.2405, -0.2292,
        -0.2264, -0.2235, -0.2207, -0.2179, -0.2151, -0.2122, -0.2094, -0.2066,
        -0.2037, -0.2009, -0.1981, -0.1952, -0.1924, -0.1896, -0.1868, -0.1839,
        -0.1811, -0.1783, -0.1754, -0.1726, -0.1698, -0.1670, -0.1641, -0.1613,
        -0.1585, -0.1556, -0.1528, -0.1500, -0.1471, -0.1443, -0.1415, -0.1387,
        -0.1358, -0.1330, -0.1302, -0.1273, -0.1245, -0.1217, -0.1188, -0.1160,
        -0.1132, -0.1104, -0.1075, -0.1047, -0.1019, -0.0990, -0.0962, -0.0934,
        -0.0906, -0.0877, -0.0849, -0.0821, -0.0792, -0.0764, -0.0736, -0.0707,
        -0.0679, -0.0651, -0.0623, -0.0594, -0.0566, -0.0538, -0.0509, -0.0481,
        -0.0453, -0.0424, -0.0396, -0.0368, -0.0340, -0.0311, -0.0283, -0.0255,
        -0.0226, -0.0198, -0.0170, -0.0141, -0.0113, -0.0085, -0.0057, -0.0028,
        -0.0000,  0.0028,  0.0057,  0.0085,  0.0113,  0.0141,  0.0170,  0.0198,
         0.0226,  0.0255,  0.0283,  0.0311,  0.0340,  0.0368,  0.0396,  0.0424,
         0.0453,  0.0481,  0.0509,  0.0538,  0.0566,  0.0594,  0.0623,  0.0651,
         0.0679,  0.0707,  0.0736,  0.0764,  0.0792,  0.0821,  0.0849,  0.0877,
         0.0906,  0.0934,  0.0962,  0.0990,  0.1019,  0.1047,  0.1075,  0.1104,
         0.1132,  0.1160,  0.1188,  0.1217,  0.1245,  0.1273,  0.1302,  0.1330,
         0.1358,  0.1387,  0.1415,  0.1443,  0.1471,  0.1500,  0.1528,  0.1556,
         0.1585,  0.1613,  0.1641,  0.1670,  0.1698,  0.1726,  0.1754,  0.1783,
         0.1811,  0.1839,  0.1868,  0.1896,  0.1924,  0.1952,  0.1981,  0.2009,
         0.2037,  0.2094,  0.2122,  0.2151,  0.2207,  0.2235,  0.2264,  0.2292,
         0.2320,  0.2349,  0.2377,  0.2547,  0.2575,  0.2603,  0.2688,  0.2773,
         0.2830,  0.2858,  0.2886,  0.2971,  0.3113,  0.3594], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  182
---------Q_out---------
tensor([[[ 0.0600,  1.5012,  0.3603,  ...,  0.9608, -2.6422,  0.3002],
         [-1.2010,  0.8407, -0.9007,  ...,  1.2010,  2.0417, -0.7806],
         [-1.4412,  0.1801, -0.6605,  ...,  1.5012,  1.2610, -0.4804],
         ...,
         [ 0.2402,  1.5613,  1.5012,  ...,  0.1801,  0.7806,  0.4804],
         [ 0.3603,  1.6213,  0.3002,  ...,  0.5404,  0.6605,  1.1409],
         [ 1.8015,  2.3419,  0.7206,  ...,  1.9216,  1.3811,  0.4804]],

        [[-0.5404,  1.3211,  0.2402,  ...,  0.4203, -2.9424,  0.3603],
         [-1.6814,  0.7206, -1.9216,  ...,  1.4412,  1.9816, -0.4203],
         [-1.6814,  1.5613, -2.0417,  ...,  1.8015,  2.5821,  0.8407],
         ...,
         [-0.0000,  0.4203,  0.9007,  ..., -0.5404,  1.6213,  1.8615],
         [-0.4203,  1.5012,  0.0600,  ..., -1.2610,  1.2610, -0.6005],
         [-0.3002, -0.1201, -0.3002,  ..., -0.3603, -3.4829, -0.5404]],

        [[-0.1801,  1.1409,  0.3603,  ...,  0.3603, -3.1826, -0.6605],
         [-1.9816,  1.0208, -1.2010,  ...,  0.0600,  0.6005, -0.7206],
         [-0.7206,  0.3603, -1.3211,  ...,  0.0600,  1.4412, -0.4203],
         ...,
         [ 0.2402,  1.5012,  0.7806,  ...,  0.0000,  0.3603,  0.0600],
         [ 0.4804,  1.8615,  1.0208,  ...,  0.6605,  1.7414, -0.1801],
         [ 1.3211,  2.2218,  0.1201,  ...,  1.2010,  1.7414, -0.4203]],

        ...,

        [[-0.1801,  0.7806,  0.5404,  ...,  0.5404, -2.7623, -0.3603],
         [-2.5221,  0.6005, -1.5012,  ...,  0.7206,  1.4412, -0.7206],
         [-1.2010,  0.6605, -1.7414,  ...,  0.8407,  1.2010,  0.2402],
         ...,
         [ 0.4203,  2.1017,  0.6605,  ...,  0.1201,  2.5821,  0.8407],
         [ 0.1201,  1.6814, -0.0000,  ...,  0.5404,  1.7414,  1.0208],
         [ 0.8407,  1.9216,  0.6605,  ...,  1.5012,  1.3811,  0.9007]],

        [[-0.4804,  1.2610,  0.0600,  ...,  0.5404, -2.8223,  0.1801],
         [-1.9816,  1.6213, -1.7414,  ...,  0.9608,  1.8615, -0.9007],
         [-1.6213,  1.5613, -0.4804,  ...,  1.3211,  1.2610,  0.4804],
         ...,
         [-0.3603,  1.0809, -0.4804,  ...,  1.0208,  1.6814,  0.3603],
         [-0.7206,  0.7206, -0.9007,  ...,  0.7206,  1.8615,  0.3603],
         [ 0.7806,  1.9216,  0.1201,  ...,  0.6005,  1.8015,  0.9608]],

        [[-0.0000,  1.3211,  0.1801,  ...,  0.6005, -2.8223, -0.1201],
         [-1.0208,  1.5012, -1.1409,  ...,  2.2218,  1.3211,  0.9007],
         [-0.3002,  1.6213, -0.4203,  ...,  1.6213,  3.0625,  0.6005],
         ...,
         [ 0.7806,  1.5012,  1.1409,  ...,  0.4203,  0.3002,  1.0809],
         [ 1.0809,  1.6814,  1.2610,  ...,  1.3211,  1.2010,  0.6605],
         [ 1.5613,  1.4412,  0.2402,  ...,  1.6213,  1.5012,  0.6005]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.6263, -7.0258, -6.9657, -6.7856, -6.6054, -6.5454, -6.4253, -6.3652,
        -6.3052, -6.2451, -6.1851, -6.1250, -6.0650, -6.0049, -5.9449, -5.8848,
        -5.8248, -5.7647, -5.7047, -5.6446, -5.5846, -5.5245, -5.4645, -5.4044,
        -5.3444, -5.2843, -5.2243, -5.1642, -5.1042, -5.0441, -4.9841, -4.9240,
        -4.8640, -4.8039, -4.7439, -4.6838, -4.6238, -4.5637, -4.5037, -4.4436,
        -4.3836, -4.3235, -4.2635, -4.2034, -4.1434, -4.0833, -4.0233, -3.9633,
        -3.9032, -3.8432, -3.7831, -3.7231, -3.6630, -3.6030, -3.5429, -3.4829,
        -3.4228, -3.3628, -3.3027, -3.2427, -3.1826, -3.1226, -3.0625, -3.0025,
        -2.9424, -2.8824, -2.8223, -2.7623, -2.7022, -2.6422, -2.5821, -2.5221,
        -2.4620, -2.4020, -2.3419, -2.2819, -2.2218, -2.1618, -2.1017, -2.0417,
        -1.9816, -1.9216, -1.8615, -1.8015, -1.7414, -1.6814, -1.6213, -1.5613,
        -1.5012, -1.4412, -1.3811, -1.3211, -1.2610, -1.2010, -1.1409, -1.0809,
        -1.0208, -0.9608, -0.9007, -0.8407, -0.7806, -0.7206, -0.6605, -0.6005,
        -0.5404, -0.4804, -0.4203, -0.3603, -0.3002, -0.2402, -0.1801, -0.1201,
        -0.0600, -0.0000,  0.0600,  0.1201,  0.1801,  0.2402,  0.3002,  0.3603,
         0.4203,  0.4804,  0.5404,  0.6005,  0.6605,  0.7206,  0.7806,  0.8407,
         0.9007,  0.9608,  1.0208,  1.0809,  1.1409,  1.2010,  1.2610,  1.3211,
         1.3811,  1.4412,  1.5012,  1.5613,  1.6213,  1.6814,  1.7414,  1.8015,
         1.8615,  1.9216,  1.9816,  2.0417,  2.1017,  2.1618,  2.2218,  2.2819,
         2.3419,  2.4020,  2.4620,  2.5221,  2.5821,  2.6422,  2.7022,  2.7623,
         2.8223,  2.8824,  2.9424,  3.0025,  3.0625,  3.1226,  3.1826,  3.2427,
         3.3027,  3.3628,  3.4228,  3.4829,  3.5429,  3.6030,  3.6630,  3.7231,
         3.7831,  3.8432,  3.9032,  3.9633,  4.0233,  4.0833,  4.1434,  4.2034,
         4.2635,  4.3235,  4.3836,  4.4436,  4.5037,  4.5637,  4.6238,  4.6838,
         4.7439,  4.8039,  4.8640,  4.9240,  4.9841,  5.0441,  5.1042,  5.1642,
         5.2243,  5.2843,  5.3444,  5.4044,  5.4645,  5.5245,  5.5846,  5.6446,
         5.7047,  5.7647,  5.8248,  5.8848,  5.9449,  6.0049,  6.0650,  6.1250,
         6.1851,  6.2451,  6.3052,  6.3652,  6.4253,  6.4853,  6.5454,  6.6054,
         6.6655,  6.7255,  6.7856,  6.8456,  6.9057,  6.9657,  7.0258,  7.1459,
         7.2059,  7.6263], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  234
name:  bert.encoder.layer.5.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0613, -0.0042, -0.0337,  ..., -0.0061,  0.0063, -0.0365],
        [ 0.0425, -0.0309,  0.0215,  ...,  0.0131,  0.0446,  0.0047],
        [ 0.0280, -0.0178,  0.0444,  ...,  0.0077,  0.0249, -0.0616],
        ...,
        [ 0.0218, -0.0553,  0.0379,  ...,  0.0216,  0.0373,  0.0019],
        [ 0.0141,  0.0003,  0.0555,  ...,  0.0299,  0.0130,  0.0267],
        [-0.0519,  0.0020, -0.0184,  ..., -0.0097, -0.0375,  0.0331]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0609, -0.0035, -0.0331,  ..., -0.0052,  0.0070, -0.0365],
        [ 0.0418, -0.0313,  0.0209,  ...,  0.0139,  0.0453,  0.0052],
        [ 0.0278, -0.0174,  0.0453,  ...,  0.0070,  0.0244, -0.0609],
        ...,
        [ 0.0226, -0.0557,  0.0383,  ...,  0.0209,  0.0365,  0.0017],
        [ 0.0139,  0.0000,  0.0557,  ...,  0.0296,  0.0122,  0.0261],
        [-0.0522,  0.0017, -0.0191,  ..., -0.0104, -0.0383,  0.0331]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2210, -0.2106, -0.2036, -0.2019, -0.1967, -0.1949, -0.1880, -0.1862,
        -0.1827, -0.1810, -0.1775, -0.1758, -0.1740, -0.1723, -0.1706, -0.1688,
        -0.1671, -0.1653, -0.1636, -0.1619, -0.1601, -0.1584, -0.1566, -0.1549,
        -0.1532, -0.1514, -0.1497, -0.1479, -0.1462, -0.1445, -0.1427, -0.1410,
        -0.1392, -0.1375, -0.1358, -0.1340, -0.1323, -0.1305, -0.1288, -0.1271,
        -0.1253, -0.1236, -0.1218, -0.1201, -0.1184, -0.1166, -0.1149, -0.1131,
        -0.1114, -0.1096, -0.1079, -0.1062, -0.1044, -0.1027, -0.1009, -0.0992,
        -0.0975, -0.0957, -0.0940, -0.0922, -0.0905, -0.0888, -0.0870, -0.0853,
        -0.0835, -0.0818, -0.0801, -0.0783, -0.0766, -0.0748, -0.0731, -0.0714,
        -0.0696, -0.0679, -0.0661, -0.0644, -0.0627, -0.0609, -0.0592, -0.0574,
        -0.0557, -0.0540, -0.0522, -0.0505, -0.0487, -0.0470, -0.0453, -0.0435,
        -0.0418, -0.0400, -0.0383, -0.0365, -0.0348, -0.0331, -0.0313, -0.0296,
        -0.0278, -0.0261, -0.0244, -0.0226, -0.0209, -0.0191, -0.0174, -0.0157,
        -0.0139, -0.0122, -0.0104, -0.0087, -0.0070, -0.0052, -0.0035, -0.0017,
        -0.0000,  0.0017,  0.0035,  0.0052,  0.0070,  0.0087,  0.0104,  0.0122,
         0.0139,  0.0157,  0.0174,  0.0191,  0.0209,  0.0226,  0.0244,  0.0261,
         0.0278,  0.0296,  0.0313,  0.0331,  0.0348,  0.0365,  0.0383,  0.0400,
         0.0418,  0.0435,  0.0453,  0.0470,  0.0487,  0.0505,  0.0522,  0.0540,
         0.0557,  0.0574,  0.0592,  0.0609,  0.0627,  0.0644,  0.0661,  0.0679,
         0.0696,  0.0714,  0.0731,  0.0748,  0.0766,  0.0783,  0.0801,  0.0818,
         0.0835,  0.0853,  0.0870,  0.0888,  0.0905,  0.0922,  0.0940,  0.0957,
         0.0975,  0.0992,  0.1009,  0.1027,  0.1044,  0.1062,  0.1079,  0.1096,
         0.1114,  0.1131,  0.1149,  0.1166,  0.1184,  0.1201,  0.1218,  0.1236,
         0.1253,  0.1271,  0.1288,  0.1305,  0.1323,  0.1340,  0.1358,  0.1375,
         0.1392,  0.1410,  0.1427,  0.1445,  0.1462,  0.1479,  0.1497,  0.1514,
         0.1532,  0.1549,  0.1566,  0.1584,  0.1601,  0.1636,  0.1653,  0.1671,
         0.1740,  0.1810,  0.1845,  0.1880,  0.1932,  0.1949,  0.1967,  0.1984,
         0.2002,  0.2089,  0.2106,  0.2141], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  220
---------Q_out---------
None
name:  bert.encoder.layer.5.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0073,  0.0245, -0.0379,  ..., -0.0110,  0.0472,  0.0181],
        [ 0.0533,  0.0235, -0.0549,  ...,  0.0095,  0.0709, -0.0208],
        [-0.0373, -0.0097, -0.0172,  ...,  0.0087,  0.0389,  0.0198],
        ...,
        [-0.0164,  0.0042, -0.0010,  ..., -0.0313,  0.0451, -0.0562],
        [ 0.0006,  0.0157, -0.0285,  ..., -0.0029, -0.0736,  0.0089],
        [ 0.0145,  0.0046,  0.0231,  ..., -0.0063, -0.0122, -0.0126]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0076,  0.0253, -0.0380,  ..., -0.0101,  0.0481,  0.0177],
        [ 0.0532,  0.0228, -0.0557,  ...,  0.0101,  0.0709, -0.0203],
        [-0.0380, -0.0101, -0.0177,  ...,  0.0076,  0.0380,  0.0203],
        ...,
        [-0.0152,  0.0051, -0.0000,  ..., -0.0304,  0.0456, -0.0557],
        [ 0.0000,  0.0152, -0.0278,  ..., -0.0025, -0.0734,  0.0101],
        [ 0.0152,  0.0051,  0.0228,  ..., -0.0076, -0.0127, -0.0127]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2987, -0.2810, -0.2455, -0.2076, -0.2000, -0.1949, -0.1924, -0.1873,
        -0.1848, -0.1823, -0.1772, -0.1747, -0.1721, -0.1696, -0.1671, -0.1645,
        -0.1620, -0.1595, -0.1569, -0.1544, -0.1519, -0.1494, -0.1468, -0.1443,
        -0.1418, -0.1392, -0.1367, -0.1342, -0.1316, -0.1291, -0.1266, -0.1240,
        -0.1215, -0.1190, -0.1164, -0.1139, -0.1114, -0.1088, -0.1063, -0.1038,
        -0.1013, -0.0987, -0.0962, -0.0937, -0.0911, -0.0886, -0.0861, -0.0835,
        -0.0810, -0.0785, -0.0759, -0.0734, -0.0709, -0.0683, -0.0658, -0.0633,
        -0.0608, -0.0582, -0.0557, -0.0532, -0.0506, -0.0481, -0.0456, -0.0430,
        -0.0405, -0.0380, -0.0354, -0.0329, -0.0304, -0.0278, -0.0253, -0.0228,
        -0.0203, -0.0177, -0.0152, -0.0127, -0.0101, -0.0076, -0.0051, -0.0025,
        -0.0000,  0.0025,  0.0051,  0.0076,  0.0101,  0.0127,  0.0152,  0.0177,
         0.0203,  0.0228,  0.0253,  0.0278,  0.0304,  0.0329,  0.0354,  0.0380,
         0.0405,  0.0430,  0.0456,  0.0481,  0.0506,  0.0532,  0.0557,  0.0582,
         0.0608,  0.0633,  0.0658,  0.0683,  0.0709,  0.0734,  0.0759,  0.0785,
         0.0810,  0.0835,  0.0861,  0.0886,  0.0911,  0.0937,  0.0962,  0.0987,
         0.1013,  0.1038,  0.1063,  0.1088,  0.1114,  0.1139,  0.1164,  0.1190,
         0.1215,  0.1240,  0.1266,  0.1291,  0.1316,  0.1342,  0.1367,  0.1392,
         0.1418,  0.1443,  0.1468,  0.1494,  0.1519,  0.1544,  0.1569,  0.1595,
         0.1620,  0.1645,  0.1671,  0.1696,  0.1721,  0.1747,  0.1772,  0.1823,
         0.1873,  0.1899,  0.1924,  0.1949,  0.1974,  0.2202,  0.2304,  0.2329,
         0.2455,  0.2936,  0.3215], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  163
---------Q_out---------
None
name:  bert.encoder.layer.5.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0324,  0.0625,  0.0484,  ...,  0.0725,  0.0494,  0.0147],
        [ 0.0724,  0.0316, -0.0793,  ..., -0.0009, -0.0626,  0.0326],
        [-0.0819,  0.0311,  0.0014,  ...,  0.0171, -0.0028,  0.0117],
        ...,
        [-0.0101, -0.0140,  0.0108,  ..., -0.0068, -0.0051,  0.0051],
        [ 0.0355, -0.0159, -0.0278,  ...,  0.0048,  0.0297,  0.0390],
        [-0.0046, -0.0314, -0.0347,  ...,  0.1282,  0.0101, -0.0499]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0304,  0.0608,  0.0478,  ...,  0.0739,  0.0478,  0.0130],
        [ 0.0739,  0.0304, -0.0782,  ..., -0.0000, -0.0608,  0.0348],
        [-0.0825,  0.0304,  0.0000,  ...,  0.0174, -0.0043,  0.0130],
        ...,
        [-0.0087, -0.0130,  0.0087,  ..., -0.0087, -0.0043,  0.0043],
        [ 0.0348, -0.0174, -0.0261,  ...,  0.0043,  0.0304,  0.0391],
        [-0.0043, -0.0304, -0.0348,  ...,  0.1303,  0.0087, -0.0478]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3519, -0.2998, -0.2650, -0.2607, -0.2563, -0.2520, -0.2476, -0.2433,
        -0.2389, -0.2346, -0.2302, -0.2259, -0.2216, -0.2172, -0.2129, -0.2085,
        -0.2042, -0.1998, -0.1955, -0.1911, -0.1868, -0.1825, -0.1781, -0.1738,
        -0.1694, -0.1651, -0.1607, -0.1564, -0.1521, -0.1477, -0.1434, -0.1390,
        -0.1347, -0.1303, -0.1260, -0.1216, -0.1173, -0.1130, -0.1086, -0.1043,
        -0.0999, -0.0956, -0.0912, -0.0869, -0.0825, -0.0782, -0.0739, -0.0695,
        -0.0652, -0.0608, -0.0565, -0.0521, -0.0478, -0.0434, -0.0391, -0.0348,
        -0.0304, -0.0261, -0.0217, -0.0174, -0.0130, -0.0087, -0.0043, -0.0000,
         0.0043,  0.0087,  0.0130,  0.0174,  0.0217,  0.0261,  0.0304,  0.0348,
         0.0391,  0.0434,  0.0478,  0.0521,  0.0565,  0.0608,  0.0652,  0.0695,
         0.0739,  0.0782,  0.0825,  0.0869,  0.0912,  0.0956,  0.0999,  0.1043,
         0.1086,  0.1130,  0.1173,  0.1216,  0.1260,  0.1303,  0.1347,  0.1390,
         0.1434,  0.1477,  0.1521,  0.1564,  0.1607,  0.1651,  0.1694,  0.1738,
         0.1781,  0.1825,  0.1868,  0.1911,  0.1955,  0.1998,  0.2042,  0.2085,
         0.2129,  0.2172,  0.2216,  0.2259,  0.2302,  0.2346,  0.2389,  0.2433,
         0.2476,  0.2520,  0.2563,  0.2693,  0.2737,  0.2824,  0.2867,  0.2998,
         0.3475,  0.3562,  0.3649,  0.4475,  0.5344,  0.5517], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  134
---------Q_out---------
None
name:  bert.encoder.layer.5.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0641, -0.0480,  0.0528,  ...,  0.0213,  0.0187, -0.0280],
        [-0.0173,  0.1157,  0.0063,  ...,  0.0202, -0.0514, -0.0003],
        [-0.0509,  0.0075, -0.0316,  ..., -0.0583,  0.0156, -0.0605],
        ...,
        [-0.0035,  0.0353, -0.0195,  ..., -0.0025, -0.0294, -0.0038],
        [-0.0571,  0.0402, -0.0448,  ...,  0.0057, -0.0467, -0.0232],
        [-0.0078,  0.0751,  0.0115,  ..., -0.0273,  0.0300,  0.0107]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0474, -0.0474,  0.0474,  ...,  0.0000,  0.0000, -0.0474],
        [-0.0000,  0.0949,  0.0000,  ...,  0.0000, -0.0474, -0.0000],
        [-0.0474,  0.0000, -0.0474,  ..., -0.0474,  0.0000, -0.0474],
        ...,
        [-0.0000,  0.0474, -0.0000,  ..., -0.0000, -0.0474, -0.0000],
        [-0.0474,  0.0474, -0.0474,  ...,  0.0000, -0.0474, -0.0000],
        [-0.0000,  0.0949,  0.0000,  ..., -0.0474,  0.0474,  0.0000]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-6.0236, -0.8063, -0.6166, -0.5692, -0.4743, -0.4269, -0.3794, -0.3320,
        -0.2846, -0.2371, -0.1897, -0.1423, -0.0949, -0.0474, -0.0000,  0.0474,
         0.0949,  0.1423,  0.1897,  0.2371,  0.2846,  0.3320,  0.3794,  0.4269,
         0.4743,  0.5217,  0.5692,  0.7114,  0.8063,  0.8537,  2.1343,  2.5138],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  32
---------Q_out---------
None
name:  bert.encoder.layer.6.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0254,  0.0490, -0.0148,  ...,  0.0431, -0.0253,  0.0014],
        [-0.0142, -0.0465, -0.0020,  ..., -0.0073, -0.0295,  0.0207],
        [ 0.0147, -0.0400, -0.0345,  ..., -0.0698, -0.1239, -0.0030],
        ...,
        [-0.0215, -0.0160, -0.0679,  ...,  0.0233, -0.0360,  0.0356],
        [-0.0296,  0.0270,  0.0465,  ...,  0.0690,  0.0294,  0.0096],
        [-0.0013, -0.0402,  0.0344,  ..., -0.0741,  0.0372,  0.0420]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0252,  0.0479, -0.0151,  ...,  0.0429, -0.0252,  0.0025],
        [-0.0151, -0.0454, -0.0025,  ..., -0.0076, -0.0303,  0.0202],
        [ 0.0151, -0.0403, -0.0353,  ..., -0.0706, -0.1235, -0.0025],
        ...,
        [-0.0227, -0.0151, -0.0681,  ...,  0.0227, -0.0353,  0.0353],
        [-0.0303,  0.0277,  0.0454,  ...,  0.0681,  0.0303,  0.0101],
        [-0.0025, -0.0403,  0.0353,  ..., -0.0731,  0.0378,  0.0429]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2723, -0.2698, -0.2269, -0.2244, -0.2219, -0.2143, -0.2118, -0.2067,
        -0.2042, -0.2017, -0.1966, -0.1941, -0.1916, -0.1891, -0.1866, -0.1840,
        -0.1815, -0.1790, -0.1765, -0.1740, -0.1714, -0.1689, -0.1664, -0.1639,
        -0.1613, -0.1588, -0.1563, -0.1538, -0.1513, -0.1487, -0.1462, -0.1437,
        -0.1412, -0.1387, -0.1361, -0.1336, -0.1311, -0.1286, -0.1261, -0.1235,
        -0.1210, -0.1185, -0.1160, -0.1134, -0.1109, -0.1084, -0.1059, -0.1034,
        -0.1008, -0.0983, -0.0958, -0.0933, -0.0908, -0.0882, -0.0857, -0.0832,
        -0.0807, -0.0782, -0.0756, -0.0731, -0.0706, -0.0681, -0.0655, -0.0630,
        -0.0605, -0.0580, -0.0555, -0.0529, -0.0504, -0.0479, -0.0454, -0.0429,
        -0.0403, -0.0378, -0.0353, -0.0328, -0.0303, -0.0277, -0.0252, -0.0227,
        -0.0202, -0.0176, -0.0151, -0.0126, -0.0101, -0.0076, -0.0050, -0.0025,
        -0.0000,  0.0025,  0.0050,  0.0076,  0.0101,  0.0126,  0.0151,  0.0176,
         0.0202,  0.0227,  0.0252,  0.0277,  0.0303,  0.0328,  0.0353,  0.0378,
         0.0403,  0.0429,  0.0454,  0.0479,  0.0504,  0.0529,  0.0555,  0.0580,
         0.0605,  0.0630,  0.0655,  0.0681,  0.0706,  0.0731,  0.0756,  0.0782,
         0.0807,  0.0832,  0.0857,  0.0882,  0.0908,  0.0933,  0.0958,  0.0983,
         0.1008,  0.1034,  0.1059,  0.1084,  0.1109,  0.1134,  0.1160,  0.1185,
         0.1210,  0.1235,  0.1261,  0.1286,  0.1311,  0.1336,  0.1361,  0.1387,
         0.1412,  0.1437,  0.1462,  0.1487,  0.1513,  0.1538,  0.1563,  0.1588,
         0.1613,  0.1639,  0.1664,  0.1689,  0.1714,  0.1740,  0.1765,  0.1790,
         0.1815,  0.1840,  0.1866,  0.1891,  0.1916,  0.1941,  0.1966,  0.1992,
         0.2042,  0.2092,  0.2143,  0.2168,  0.2193,  0.2319,  0.2395,  0.2496,
         0.2698,  0.2798,  0.3202], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  179
---------Q_out---------
tensor([[[ 0.4539, -0.7133,  1.5564,  ..., -0.1297,  0.1945, -1.0376],
         [-1.1673,  1.8806,  1.2321,  ...,  0.2594,  1.2970, -4.9934],
         [-0.6485,  3.2425,  1.2321,  ...,  0.6485,  0.1297, -4.8637],
         ...,
         [ 0.6485,  0.1297,  0.8430,  ...,  1.1024, -2.5291, -1.6861],
         [ 0.5836, -0.9079,  1.3618,  ...,  0.7133, -2.3994, -2.1400],
         [ 0.0648,  0.0648,  2.1400,  ...,  0.3891, -3.4370, -2.7885]],

        [[ 0.3891, -0.2594,  2.0752,  ..., -0.3891,  0.1297, -1.3618],
         [-2.5291,  3.2425,  3.1128,  ..., -0.9727,  3.0479, -4.2801],
         [-3.0479,  2.1400,  3.5667,  ..., -1.1673,  0.9727, -2.9831],
         ...,
         [-0.3891, -0.0000,  2.3346,  ..., -1.8158, -0.5188, -1.1024],
         [-0.3891,  0.8430,  1.6212,  ...,  0.5188, -1.2970, -2.7237],
         [-0.8430,  1.3618,  1.2970,  ...,  0.2594,  0.5836, -1.8806]],

        [[-0.0648, -0.0000,  0.9079,  ..., -0.1945,  0.8430, -1.1673],
         [-1.1673,  2.3994,  1.5564,  ...,  0.1945,  1.8158, -5.2528],
         [-2.2697,  2.7885,  1.4915,  ..., -0.7133, -1.3618, -4.2152],
         ...,
         [ 0.3242, -0.1945,  1.1673,  ...,  0.7782, -0.6485, -2.9831],
         [-0.0000,  0.7133,  2.3346,  ...,  0.5836, -2.3994, -3.0479],
         [-0.0000,  0.2594,  1.5564,  ...,  1.0376, -4.0855, -2.7237]],

        ...,

        [[ 0.2594, -0.1945,  1.8806,  ..., -0.1945,  0.1945, -1.4915],
         [-2.6588,  2.2049,  3.5019,  ..., -0.8430,  2.7885, -4.2152],
         [-2.7885,  2.9182,  2.3346,  ..., -0.5188,  0.0648, -3.2425],
         ...,
         [ 0.3242, -0.4539,  1.2321,  ...,  0.6485, -1.7509, -2.4643],
         [ 0.7782,  0.2594,  1.2970,  ...,  0.9079, -2.3994, -2.9182],
         [ 0.0648,  0.1945,  2.1400,  ...,  0.2594, -2.9182, -2.7885]],

        [[ 0.4539, -0.5188,  1.5564,  ..., -0.5188,  0.6485, -1.4267],
         [-1.2970,  2.2697,  3.6964,  ..., -2.1400,  1.2970, -4.4098],
         [-0.1297,  1.4267,  2.3346,  ...,  0.5188, -0.3242, -3.4370],
         ...,
         [ 0.1945, -0.1945,  2.0103,  ...,  0.1297, -0.3242, -3.8261],
         [ 0.1297,  0.3891,  1.8806,  ...,  0.1297, -0.5836, -3.6316],
         [ 0.0648,  0.0000,  2.2049,  ...,  0.5188, -2.3346, -3.0479]],

        [[ 0.2594, -0.3891,  1.8158,  ..., -0.2594,  0.3891, -0.9079],
         [-2.3346,  2.6588,  1.5564,  ..., -0.7782,  2.0103, -5.0583],
         [ 0.3242,  2.6588,  2.1400,  ..., -0.1297, -0.1945, -4.6692],
         ...,
         [ 0.4539, -0.0648,  1.8158,  ...,  1.7509, -2.3994, -1.1673],
         [-0.0000, -0.2594,  1.6212,  ...,  1.3618, -2.8534, -2.9831],
         [ 0.0000, -0.1945,  2.3994,  ...,  0.0000, -2.5940, -3.5019]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-8.2359, -8.1062, -8.0413, -7.9765, -7.9116, -7.8468, -7.7171, -7.6522,
        -7.5874, -7.5225, -7.4577, -7.3929, -7.3280, -7.2632, -7.1983, -7.1335,
        -7.0686, -7.0038, -6.9389, -6.8741, -6.8092, -6.7444, -6.6795, -6.6147,
        -6.5498, -6.4850, -6.4201, -6.3553, -6.2904, -6.2256, -6.1607, -6.0959,
        -6.0310, -5.9662, -5.9013, -5.8365, -5.7716, -5.7068, -5.6419, -5.5771,
        -5.5122, -5.4474, -5.3825, -5.3177, -5.2528, -5.1880, -5.1231, -5.0583,
        -4.9934, -4.9286, -4.8637, -4.7989, -4.7340, -4.6692, -4.6043, -4.5395,
        -4.4746, -4.4098, -4.3449, -4.2801, -4.2152, -4.1504, -4.0855, -4.0207,
        -3.9558, -3.8910, -3.8261, -3.7613, -3.6964, -3.6316, -3.5667, -3.5019,
        -3.4370, -3.3722, -3.3073, -3.2425, -3.1776, -3.1128, -3.0479, -2.9831,
        -2.9182, -2.8534, -2.7885, -2.7237, -2.6588, -2.5940, -2.5291, -2.4643,
        -2.3994, -2.3346, -2.2697, -2.2049, -2.1400, -2.0752, -2.0103, -1.9455,
        -1.8806, -1.8158, -1.7509, -1.6861, -1.6212, -1.5564, -1.4915, -1.4267,
        -1.3618, -1.2970, -1.2321, -1.1673, -1.1024, -1.0376, -0.9727, -0.9079,
        -0.8430, -0.7782, -0.7133, -0.6485, -0.5836, -0.5188, -0.4539, -0.3891,
        -0.3242, -0.2594, -0.1945, -0.1297, -0.0648, -0.0000,  0.0648,  0.1297,
         0.1945,  0.2594,  0.3242,  0.3891,  0.4539,  0.5188,  0.5836,  0.6485,
         0.7133,  0.7782,  0.8430,  0.9079,  0.9727,  1.0376,  1.1024,  1.1673,
         1.2321,  1.2970,  1.3618,  1.4267,  1.4915,  1.5564,  1.6212,  1.6861,
         1.7509,  1.8158,  1.8806,  1.9455,  2.0103,  2.0752,  2.1400,  2.2049,
         2.2697,  2.3346,  2.3994,  2.4643,  2.5291,  2.5940,  2.6588,  2.7237,
         2.7885,  2.8534,  2.9182,  2.9831,  3.0479,  3.1128,  3.1776,  3.2425,
         3.3073,  3.3722,  3.4370,  3.5019,  3.5667,  3.6316,  3.6964,  3.7613,
         3.8261,  3.8910,  3.9558,  4.0207,  4.0855,  4.1504,  4.2152,  4.2801,
         4.3449,  4.4098,  4.4746,  4.5395,  4.6043,  4.6692,  4.7340,  4.7989,
         4.8637,  4.9286,  4.9934,  5.0583,  5.1231,  5.1880,  5.2528,  5.3177,
         5.3825,  5.4474,  5.5122,  5.5771,  5.6419,  5.7068,  5.7716,  5.8365,
         5.9013,  5.9662,  6.0310,  6.0959,  6.1607,  6.2256,  6.2904,  6.3553,
         6.4201,  6.4850,  6.5498,  6.6147,  6.6795,  6.7444,  6.8092,  6.8741,
         6.9389,  7.0038,  7.0686,  7.1335,  7.1983,  7.2632,  7.3280,  7.3929,
         7.4577,  7.5225,  7.5874,  7.6522,  7.7171,  7.9116,  8.0413,  8.1062],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  248
name:  bert.encoder.layer.6.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0589,  0.0240, -0.0749,  ..., -0.0558, -0.0502, -0.0084],
        [ 0.0070, -0.0450,  0.0146,  ..., -0.0704, -0.0315,  0.0267],
        [ 0.0399, -0.0710,  0.0829,  ..., -0.0582, -0.0538, -0.0147],
        ...,
        [-0.0012, -0.0404, -0.0151,  ...,  0.0543,  0.0300,  0.0780],
        [ 0.0053,  0.0702,  0.0584,  ...,  0.0186, -0.0031,  0.0007],
        [ 0.0101, -0.0598,  0.0351,  ...,  0.0352,  0.0015,  0.0067]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0595,  0.0227, -0.0737,  ..., -0.0567, -0.0510, -0.0085],
        [ 0.0057, -0.0453,  0.0142,  ..., -0.0709, -0.0312,  0.0255],
        [ 0.0397, -0.0709,  0.0822,  ..., -0.0595, -0.0539, -0.0142],
        ...,
        [-0.0000, -0.0397, -0.0142,  ...,  0.0539,  0.0312,  0.0794],
        [ 0.0057,  0.0709,  0.0595,  ...,  0.0198, -0.0028,  0.0000],
        [ 0.0113, -0.0595,  0.0340,  ...,  0.0340,  0.0028,  0.0057]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3345, -0.2891, -0.2664, -0.2608, -0.2579, -0.2494, -0.2466, -0.2409,
        -0.2381, -0.2324, -0.2296, -0.2267, -0.2239, -0.2182, -0.2154, -0.2126,
        -0.2097, -0.2069, -0.2041, -0.1984, -0.1956, -0.1927, -0.1899, -0.1871,
        -0.1842, -0.1814, -0.1786, -0.1757, -0.1729, -0.1701, -0.1672, -0.1644,
        -0.1616, -0.1587, -0.1559, -0.1531, -0.1502, -0.1474, -0.1446, -0.1417,
        -0.1389, -0.1360, -0.1332, -0.1304, -0.1275, -0.1247, -0.1219, -0.1190,
        -0.1162, -0.1134, -0.1105, -0.1077, -0.1049, -0.1020, -0.0992, -0.0964,
        -0.0935, -0.0907, -0.0879, -0.0850, -0.0822, -0.0794, -0.0765, -0.0737,
        -0.0709, -0.0680, -0.0652, -0.0624, -0.0595, -0.0567, -0.0539, -0.0510,
        -0.0482, -0.0453, -0.0425, -0.0397, -0.0368, -0.0340, -0.0312, -0.0283,
        -0.0255, -0.0227, -0.0198, -0.0170, -0.0142, -0.0113, -0.0085, -0.0057,
        -0.0028, -0.0000,  0.0028,  0.0057,  0.0085,  0.0113,  0.0142,  0.0170,
         0.0198,  0.0227,  0.0255,  0.0283,  0.0312,  0.0340,  0.0368,  0.0397,
         0.0425,  0.0453,  0.0482,  0.0510,  0.0539,  0.0567,  0.0595,  0.0624,
         0.0652,  0.0680,  0.0709,  0.0737,  0.0765,  0.0794,  0.0822,  0.0850,
         0.0879,  0.0907,  0.0935,  0.0964,  0.0992,  0.1020,  0.1049,  0.1077,
         0.1105,  0.1134,  0.1162,  0.1190,  0.1219,  0.1247,  0.1275,  0.1304,
         0.1332,  0.1360,  0.1389,  0.1417,  0.1446,  0.1474,  0.1502,  0.1531,
         0.1559,  0.1587,  0.1616,  0.1644,  0.1672,  0.1701,  0.1729,  0.1757,
         0.1786,  0.1814,  0.1842,  0.1871,  0.1899,  0.1927,  0.1984,  0.2012,
         0.2041,  0.2097,  0.2126,  0.2154,  0.2182,  0.2211,  0.2239,  0.2324,
         0.2353,  0.2438,  0.2466,  0.2523,  0.2664,  0.2749,  0.2778,  0.2806,
         0.2834,  0.3061,  0.3203,  0.3316,  0.3600], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  181
---------Q_out---------
tensor([[[-1.4024, -0.1275, -0.5737,  ..., -2.1674, -0.9562, -0.0637],
         [-0.2550,  0.8287,  0.3187,  ..., -0.8924,  0.5100,  0.5100],
         [-0.0637,  0.8287,  0.1275,  ...,  0.0637, -0.7012, -0.2550],
         ...,
         [-0.8287, -0.9562,  2.1674,  ...,  0.0637,  0.1275,  1.9761],
         [-1.0199, -0.6375,  2.1036,  ..., -0.3187, -0.3825,  1.4024],
         [-0.0637, -0.9562,  1.9761,  ..., -0.4462, -0.6375,  0.7012]],

        [[-1.2112, -0.3187, -0.1275,  ..., -1.0837, -0.5737, -0.7649],
         [-1.7211,  1.9124,  1.2749,  ..., -1.7211,  2.0399,  1.5936],
         [-2.5498,  1.2112,  0.8924,  ..., -1.5936,  1.4024,  2.9323],
         ...,
         [-0.1912,  0.1275,  1.7849,  ...,  0.3187, -0.8287,  2.0399],
         [-0.8924, -0.3187, -0.4462,  ..., -1.0199,  0.9562,  1.4024],
         [-1.0837,  1.2749,  1.4024,  ..., -0.0000,  0.9562, -3.4423]],

        [[-0.7012, -0.3187, -0.4462,  ..., -0.6375, -0.5100, -0.9562],
         [-0.3187,  0.5100,  0.6375,  ..., -0.2550,  1.1474, -0.3187],
         [-0.1275,  1.0837,  0.1275,  ..., -0.1275,  0.6375,  1.1474],
         ...,
         [-1.0199, -0.8924,  0.7649,  ..., -1.1474,  0.8924,  1.0837],
         [-1.4024, -1.7849,  2.1036,  ..., -0.5100, -0.7649,  1.2749],
         [-1.1474, -1.5299,  2.1674,  ..., -0.8924, -1.2112,  1.7849]],

        ...,

        [[-1.5936, -0.5100, -0.3825,  ..., -1.4024, -0.9562, -0.2550],
         [-1.2112,  1.8486,  1.1474,  ..., -1.4662,  0.8287,  2.0399],
         [-1.5299,  0.7012,  0.7649,  ...,  0.8287, -2.6136,  0.3825],
         ...,
         [-0.9562, -1.3387,  1.8486,  ...,  0.2550,  0.5737,  3.3785],
         [-1.0837, -1.1474,  2.1036,  ..., -0.3187,  0.0000,  2.5498],
         [-0.3187, -1.6574,  1.8486,  ..., -1.0199, -0.3825,  1.7211]],

        [[-0.8924, -0.9562, -1.0837,  ..., -1.5299, -0.1275, -0.8287],
         [-0.6375,  1.2112, -0.1275,  ..., -1.5299,  0.4462,  1.9761],
         [-1.7849, -0.0000, -0.1912,  ...,  2.6136, -0.8924,  1.5299],
         ...,
         [-1.2749, -1.2749,  0.6375,  ..., -1.4662,  0.7649,  1.0199],
         [-1.6574, -1.2112, -0.3187,  ..., -1.7849, -0.1275,  0.8287],
         [-1.0199, -1.8486,  0.3187,  ..., -0.9562, -1.5936,  1.3387]],

        [[-0.8924, -0.6375, -0.3187,  ..., -0.5737, -0.2550,  0.1275],
         [-0.2550, -0.1912, -0.2550,  ..., -1.5299,  0.4462,  1.4662],
         [ 0.9562,  1.8486, -0.7649,  ...,  0.2550,  1.1474,  1.1474],
         ...,
         [-0.7012, -1.2112,  2.6136,  ...,  0.2550, -0.1275,  2.3586],
         [-0.7649, -1.6574,  2.4223,  ..., -0.8924, -0.8924,  1.9761],
         [-1.0837, -1.6574,  2.3586,  ..., -0.6375, -0.6375,  1.9761]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.7132, -7.3945, -7.0758, -6.9483, -6.8845, -6.8208, -6.6933, -6.6296,
        -6.5658, -6.5021, -6.4383, -6.3746, -6.2471, -6.1833, -6.1196, -6.0559,
        -5.9921, -5.9284, -5.8646, -5.8009, -5.7371, -5.6734, -5.6096, -5.5459,
        -5.4821, -5.4184, -5.3546, -5.2909, -5.2272, -5.1634, -5.0997, -5.0359,
        -4.9722, -4.9084, -4.8447, -4.7809, -4.7172, -4.6534, -4.5897, -4.5260,
        -4.4622, -4.3985, -4.3347, -4.2710, -4.2072, -4.1435, -4.0797, -4.0160,
        -3.9522, -3.8885, -3.8247, -3.7610, -3.6973, -3.6335, -3.5698, -3.5060,
        -3.4423, -3.3785, -3.3148, -3.2510, -3.1873, -3.1235, -3.0598, -2.9961,
        -2.9323, -2.8686, -2.8048, -2.7411, -2.6773, -2.6136, -2.5498, -2.4861,
        -2.4223, -2.3586, -2.2948, -2.2311, -2.1674, -2.1036, -2.0399, -1.9761,
        -1.9124, -1.8486, -1.7849, -1.7211, -1.6574, -1.5936, -1.5299, -1.4662,
        -1.4024, -1.3387, -1.2749, -1.2112, -1.1474, -1.0837, -1.0199, -0.9562,
        -0.8924, -0.8287, -0.7649, -0.7012, -0.6375, -0.5737, -0.5100, -0.4462,
        -0.3825, -0.3187, -0.2550, -0.1912, -0.1275, -0.0637, -0.0000,  0.0637,
         0.1275,  0.1912,  0.2550,  0.3187,  0.3825,  0.4462,  0.5100,  0.5737,
         0.6375,  0.7012,  0.7649,  0.8287,  0.8924,  0.9562,  1.0199,  1.0837,
         1.1474,  1.2112,  1.2749,  1.3387,  1.4024,  1.4662,  1.5299,  1.5936,
         1.6574,  1.7211,  1.7849,  1.8486,  1.9124,  1.9761,  2.0399,  2.1036,
         2.1674,  2.2311,  2.2948,  2.3586,  2.4223,  2.4861,  2.5498,  2.6136,
         2.6773,  2.7411,  2.8048,  2.8686,  2.9323,  2.9961,  3.0598,  3.1235,
         3.1873,  3.2510,  3.3148,  3.3785,  3.4423,  3.5060,  3.5698,  3.6335,
         3.6973,  3.7610,  3.8247,  3.8885,  3.9522,  4.0160,  4.0797,  4.1435,
         4.2072,  4.2710,  4.3347,  4.3985,  4.4622,  4.5260,  4.5897,  4.6534,
         4.7172,  4.7809,  4.8447,  4.9084,  4.9722,  5.0359,  5.0997,  5.1634,
         5.2272,  5.2909,  5.3546,  5.4184,  5.4821,  5.5459,  5.6096,  5.6734,
         5.7371,  5.8009,  5.8646,  5.9284,  5.9921,  6.0559,  6.1196,  6.1833,
         6.2471,  6.3108,  6.3746,  6.4383,  6.5021,  6.5658,  6.6296,  6.6933,
         6.7571,  6.8208,  6.8845,  6.9483,  7.0120,  7.0758,  7.1395,  7.2670,
         7.5220,  7.5858,  7.6495,  7.7770,  8.0320,  8.0957], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  230
name:  bert.encoder.layer.6.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0173,  0.0054, -0.0514,  ..., -0.0333,  0.0493,  0.0369],
        [ 0.0117,  0.0166,  0.0305,  ..., -0.0425, -0.0074, -0.0425],
        [ 0.0236,  0.0043, -0.0109,  ...,  0.0239,  0.0123, -0.0037],
        ...,
        [ 0.0372, -0.0043, -0.0054,  ..., -0.0219,  0.0104, -0.0038],
        [-0.0173,  0.0194, -0.0226,  ..., -0.0223,  0.0022,  0.0299],
        [ 0.0418, -0.0129,  0.0065,  ..., -0.0476,  0.0052, -0.0130]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0168,  0.0061, -0.0519,  ..., -0.0336,  0.0488,  0.0366],
        [ 0.0122,  0.0168,  0.0305,  ..., -0.0427, -0.0076, -0.0427],
        [ 0.0229,  0.0046, -0.0107,  ...,  0.0244,  0.0122, -0.0031],
        ...,
        [ 0.0366, -0.0046, -0.0061,  ..., -0.0214,  0.0107, -0.0031],
        [-0.0168,  0.0198, -0.0229,  ..., -0.0229,  0.0015,  0.0305],
        [ 0.0412, -0.0122,  0.0061,  ..., -0.0473,  0.0046, -0.0137]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.1939, -0.1786, -0.1771, -0.1740, -0.1694, -0.1664, -0.1649, -0.1603,
        -0.1588, -0.1572, -0.1557, -0.1542, -0.1526, -0.1511, -0.1496, -0.1481,
        -0.1465, -0.1450, -0.1435, -0.1420, -0.1404, -0.1389, -0.1374, -0.1359,
        -0.1343, -0.1328, -0.1313, -0.1297, -0.1282, -0.1267, -0.1252, -0.1236,
        -0.1221, -0.1206, -0.1191, -0.1175, -0.1160, -0.1145, -0.1130, -0.1114,
        -0.1099, -0.1084, -0.1069, -0.1053, -0.1038, -0.1023, -0.1007, -0.0992,
        -0.0977, -0.0962, -0.0946, -0.0931, -0.0916, -0.0901, -0.0885, -0.0870,
        -0.0855, -0.0840, -0.0824, -0.0809, -0.0794, -0.0778, -0.0763, -0.0748,
        -0.0733, -0.0717, -0.0702, -0.0687, -0.0672, -0.0656, -0.0641, -0.0626,
        -0.0611, -0.0595, -0.0580, -0.0565, -0.0550, -0.0534, -0.0519, -0.0504,
        -0.0488, -0.0473, -0.0458, -0.0443, -0.0427, -0.0412, -0.0397, -0.0382,
        -0.0366, -0.0351, -0.0336, -0.0321, -0.0305, -0.0290, -0.0275, -0.0259,
        -0.0244, -0.0229, -0.0214, -0.0198, -0.0183, -0.0168, -0.0153, -0.0137,
        -0.0122, -0.0107, -0.0092, -0.0076, -0.0061, -0.0046, -0.0031, -0.0015,
        -0.0000,  0.0015,  0.0031,  0.0046,  0.0061,  0.0076,  0.0092,  0.0107,
         0.0122,  0.0137,  0.0153,  0.0168,  0.0183,  0.0198,  0.0214,  0.0229,
         0.0244,  0.0259,  0.0275,  0.0290,  0.0305,  0.0321,  0.0336,  0.0351,
         0.0366,  0.0382,  0.0397,  0.0412,  0.0427,  0.0443,  0.0458,  0.0473,
         0.0488,  0.0504,  0.0519,  0.0534,  0.0550,  0.0565,  0.0580,  0.0595,
         0.0611,  0.0626,  0.0641,  0.0656,  0.0672,  0.0687,  0.0702,  0.0717,
         0.0733,  0.0748,  0.0763,  0.0778,  0.0794,  0.0809,  0.0824,  0.0840,
         0.0855,  0.0870,  0.0885,  0.0901,  0.0916,  0.0931,  0.0946,  0.0962,
         0.0977,  0.0992,  0.1007,  0.1023,  0.1038,  0.1053,  0.1069,  0.1084,
         0.1099,  0.1114,  0.1130,  0.1145,  0.1160,  0.1175,  0.1191,  0.1206,
         0.1221,  0.1236,  0.1252,  0.1267,  0.1282,  0.1297,  0.1313,  0.1328,
         0.1343,  0.1359,  0.1374,  0.1389,  0.1404,  0.1420,  0.1435,  0.1450,
         0.1465,  0.1481,  0.1496,  0.1511,  0.1526,  0.1542,  0.1572,  0.1603,
         0.1618,  0.1633,  0.1694,  0.1710,  0.1725,  0.1740,  0.1847],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  223
---------Q_out---------
None
name:  bert.encoder.layer.6.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0324, -0.0275,  0.0121,  ..., -0.0079,  0.0169, -0.0072],
        [-0.0069,  0.0124,  0.0105,  ..., -0.0174,  0.0105, -0.0259],
        [ 0.0738,  0.0707,  0.0400,  ...,  0.0026, -0.0292, -0.0013],
        ...,
        [ 0.1028, -0.0427, -0.0197,  ...,  0.0240,  0.0061,  0.0010],
        [-0.0292,  0.0047, -0.0201,  ..., -0.0006,  0.0369, -0.0029],
        [ 0.0126,  0.0417, -0.0354,  ...,  0.0045, -0.0401,  0.0085]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0323, -0.0272,  0.0119,  ..., -0.0085,  0.0170, -0.0068],
        [-0.0068,  0.0119,  0.0102,  ..., -0.0170,  0.0102, -0.0255],
        [ 0.0732,  0.0715,  0.0391,  ...,  0.0034, -0.0289, -0.0017],
        ...,
        [ 0.1021, -0.0425, -0.0204,  ...,  0.0238,  0.0068,  0.0017],
        [-0.0289,  0.0051, -0.0204,  ..., -0.0000,  0.0374, -0.0034],
        [ 0.0119,  0.0425, -0.0357,  ...,  0.0051, -0.0408,  0.0085]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2161, -0.2042, -0.1923, -0.1872, -0.1838, -0.1804, -0.1770, -0.1702,
        -0.1685, -0.1668, -0.1651, -0.1634, -0.1617, -0.1600, -0.1583, -0.1566,
        -0.1549, -0.1531, -0.1514, -0.1497, -0.1480, -0.1463, -0.1446, -0.1429,
        -0.1412, -0.1395, -0.1378, -0.1361, -0.1344, -0.1327, -0.1310, -0.1293,
        -0.1276, -0.1259, -0.1242, -0.1225, -0.1208, -0.1191, -0.1174, -0.1157,
        -0.1140, -0.1123, -0.1106, -0.1089, -0.1072, -0.1055, -0.1038, -0.1021,
        -0.1004, -0.0987, -0.0970, -0.0953, -0.0936, -0.0919, -0.0902, -0.0885,
        -0.0868, -0.0851, -0.0834, -0.0817, -0.0800, -0.0783, -0.0766, -0.0749,
        -0.0732, -0.0715, -0.0698, -0.0681, -0.0664, -0.0647, -0.0630, -0.0613,
        -0.0596, -0.0579, -0.0562, -0.0545, -0.0528, -0.0510, -0.0493, -0.0476,
        -0.0459, -0.0442, -0.0425, -0.0408, -0.0391, -0.0374, -0.0357, -0.0340,
        -0.0323, -0.0306, -0.0289, -0.0272, -0.0255, -0.0238, -0.0221, -0.0204,
        -0.0187, -0.0170, -0.0153, -0.0136, -0.0119, -0.0102, -0.0085, -0.0068,
        -0.0051, -0.0034, -0.0017, -0.0000,  0.0017,  0.0034,  0.0051,  0.0068,
         0.0085,  0.0102,  0.0119,  0.0136,  0.0153,  0.0170,  0.0187,  0.0204,
         0.0221,  0.0238,  0.0255,  0.0272,  0.0289,  0.0306,  0.0323,  0.0340,
         0.0357,  0.0374,  0.0391,  0.0408,  0.0425,  0.0442,  0.0459,  0.0476,
         0.0493,  0.0510,  0.0528,  0.0545,  0.0562,  0.0579,  0.0596,  0.0613,
         0.0630,  0.0647,  0.0664,  0.0681,  0.0698,  0.0715,  0.0732,  0.0749,
         0.0766,  0.0783,  0.0800,  0.0817,  0.0834,  0.0851,  0.0868,  0.0885,
         0.0902,  0.0919,  0.0936,  0.0953,  0.0970,  0.0987,  0.1004,  0.1021,
         0.1038,  0.1055,  0.1072,  0.1089,  0.1106,  0.1123,  0.1140,  0.1157,
         0.1174,  0.1191,  0.1208,  0.1225,  0.1242,  0.1259,  0.1276,  0.1293,
         0.1310,  0.1327,  0.1344,  0.1361,  0.1378,  0.1395,  0.1412,  0.1429,
         0.1446,  0.1463,  0.1480,  0.1497,  0.1514,  0.1531,  0.1549,  0.1566,
         0.1583,  0.1617,  0.1634,  0.1651,  0.1668,  0.1685,  0.1702,  0.1719,
         0.1753,  0.1770,  0.1787,  0.1906,  0.1923,  0.1940,  0.2008],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  215
---------Q_out---------
None
name:  bert.encoder.layer.6.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0450, -0.0488, -0.0153,  ..., -0.0514,  0.0607,  0.0172],
        [ 0.0516,  0.0293,  0.0056,  ...,  0.0754, -0.0406,  0.0052],
        [ 0.0905, -0.0406, -0.0324,  ...,  0.0019,  0.0265, -0.0144],
        ...,
        [-0.0266,  0.0113, -0.0500,  ...,  0.0178,  0.0132,  0.0159],
        [-0.0460,  0.1039,  0.0174,  ..., -0.0041,  0.0068, -0.0156],
        [ 0.0225, -0.0158,  0.0807,  ..., -0.0209,  0.0575,  0.0511]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0442, -0.0497, -0.0166,  ..., -0.0525,  0.0608,  0.0166],
        [ 0.0525,  0.0304,  0.0055,  ...,  0.0746, -0.0414,  0.0055],
        [ 0.0912, -0.0414, -0.0332,  ...,  0.0028,  0.0276, -0.0138],
        ...,
        [-0.0276,  0.0111, -0.0497,  ...,  0.0166,  0.0138,  0.0166],
        [-0.0470,  0.1050,  0.0166,  ..., -0.0028,  0.0055, -0.0166],
        [ 0.0221, -0.0166,  0.0801,  ..., -0.0221,  0.0580,  0.0497]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2984, -0.2901, -0.2819, -0.2736, -0.2680, -0.2570, -0.2542, -0.2515,
        -0.2459, -0.2432, -0.2376, -0.2321, -0.2294, -0.2266, -0.2238, -0.2211,
        -0.2155, -0.2128, -0.2100, -0.2072, -0.2045, -0.2017, -0.1990, -0.1962,
        -0.1934, -0.1907, -0.1879, -0.1851, -0.1824, -0.1796, -0.1769, -0.1741,
        -0.1713, -0.1686, -0.1658, -0.1630, -0.1603, -0.1575, -0.1547, -0.1520,
        -0.1492, -0.1465, -0.1437, -0.1409, -0.1382, -0.1354, -0.1326, -0.1299,
        -0.1271, -0.1243, -0.1216, -0.1188, -0.1161, -0.1133, -0.1105, -0.1078,
        -0.1050, -0.1022, -0.0995, -0.0967, -0.0940, -0.0912, -0.0884, -0.0857,
        -0.0829, -0.0801, -0.0774, -0.0746, -0.0718, -0.0691, -0.0663, -0.0636,
        -0.0608, -0.0580, -0.0553, -0.0525, -0.0497, -0.0470, -0.0442, -0.0414,
        -0.0387, -0.0359, -0.0332, -0.0304, -0.0276, -0.0249, -0.0221, -0.0193,
        -0.0166, -0.0138, -0.0111, -0.0083, -0.0055, -0.0028, -0.0000,  0.0028,
         0.0055,  0.0083,  0.0111,  0.0138,  0.0166,  0.0193,  0.0221,  0.0249,
         0.0276,  0.0304,  0.0332,  0.0359,  0.0387,  0.0414,  0.0442,  0.0470,
         0.0497,  0.0525,  0.0553,  0.0580,  0.0608,  0.0636,  0.0663,  0.0691,
         0.0718,  0.0746,  0.0774,  0.0801,  0.0829,  0.0857,  0.0884,  0.0912,
         0.0940,  0.0967,  0.0995,  0.1022,  0.1050,  0.1078,  0.1105,  0.1133,
         0.1161,  0.1188,  0.1216,  0.1243,  0.1271,  0.1299,  0.1326,  0.1354,
         0.1382,  0.1409,  0.1437,  0.1465,  0.1492,  0.1520,  0.1547,  0.1575,
         0.1603,  0.1630,  0.1658,  0.1686,  0.1713,  0.1741,  0.1769,  0.1796,
         0.1824,  0.1851,  0.1879,  0.1907,  0.1934,  0.1962,  0.1990,  0.2017,
         0.2045,  0.2072,  0.2100,  0.2128,  0.2155,  0.2183,  0.2211,  0.2238,
         0.2266,  0.2294,  0.2321,  0.2376,  0.2404,  0.2432,  0.2515,  0.2542,
         0.2598,  0.2680,  0.2708,  0.2763,  0.2819,  0.2846,  0.2874,  0.2957,
         0.2984,  0.3150,  0.3509], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  195
---------Q_out---------
None
name:  bert.encoder.layer.6.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0008,  0.0008, -0.0147,  ..., -0.0149, -0.0126,  0.0617],
        [-0.0097,  0.0150,  0.0102,  ..., -0.0334,  0.0092,  0.0407],
        [ 0.0069, -0.0008, -0.0739,  ..., -0.0504,  0.0725,  0.0093],
        ...,
        [ 0.0282,  0.0442,  0.0258,  ...,  0.0676,  0.0494,  0.1267],
        [ 0.0142, -0.0012,  0.0494,  ..., -0.0493, -0.0103,  0.0290],
        [ 0.0008,  0.0373, -0.0182,  ..., -0.0334,  0.0099,  0.0220]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0000,  0.0000, -0.0000,  ..., -0.0000, -0.0000,  0.0757],
        [-0.0000,  0.0000,  0.0000,  ..., -0.0379,  0.0000,  0.0379],
        [ 0.0000, -0.0000, -0.0757,  ..., -0.0379,  0.0757,  0.0000],
        ...,
        [ 0.0379,  0.0379,  0.0379,  ...,  0.0757,  0.0379,  0.1136],
        [ 0.0000, -0.0000,  0.0379,  ..., -0.0379, -0.0000,  0.0379],
        [ 0.0000,  0.0379, -0.0000,  ..., -0.0379,  0.0000,  0.0379]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-4.8091, -1.0981, -0.7573, -0.7195, -0.5301, -0.4923, -0.4165, -0.3787,
        -0.3408, -0.3029, -0.2651, -0.2272, -0.1893, -0.1515, -0.1136, -0.0757,
        -0.0379, -0.0000,  0.0379,  0.0757,  0.1136,  0.1515,  0.1893,  0.2272,
         0.2651,  0.3029,  0.3408,  0.3787,  0.4165,  0.4544,  0.4923,  0.5680,
         0.6059,  0.7195,  0.8709,  1.6283], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  36
---------Q_out---------
None
name:  bert.encoder.layer.7.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0008, -0.0777, -0.0346,  ..., -0.0128, -0.0086,  0.0287],
        [-0.0149, -0.0271, -0.0251,  ..., -0.0537, -0.0198, -0.0695],
        [ 0.0408, -0.0107, -0.0435,  ..., -0.0075,  0.0339, -0.0097],
        ...,
        [-0.0180,  0.0074,  0.0135,  ..., -0.0588, -0.0277,  0.0416],
        [-0.0016,  0.0464, -0.0192,  ...,  0.0132,  0.0923,  0.0024],
        [-0.0636, -0.0009,  0.0354,  ...,  0.0523,  0.0081, -0.0208]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0000, -0.0780, -0.0338,  ..., -0.0130, -0.0078,  0.0286],
        [-0.0156, -0.0260, -0.0260,  ..., -0.0546, -0.0208, -0.0702],
        [ 0.0416, -0.0104, -0.0442,  ..., -0.0078,  0.0338, -0.0104],
        ...,
        [-0.0182,  0.0078,  0.0130,  ..., -0.0598, -0.0286,  0.0416],
        [-0.0026,  0.0468, -0.0182,  ...,  0.0130,  0.0910,  0.0026],
        [-0.0624, -0.0000,  0.0364,  ...,  0.0520,  0.0078, -0.0208]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2679, -0.2575, -0.2523, -0.2445, -0.2263, -0.2107, -0.2003, -0.1951,
        -0.1899, -0.1873, -0.1847, -0.1821, -0.1795, -0.1769, -0.1743, -0.1717,
        -0.1691, -0.1665, -0.1639, -0.1613, -0.1587, -0.1561, -0.1535, -0.1509,
        -0.1483, -0.1457, -0.1431, -0.1405, -0.1379, -0.1353, -0.1327, -0.1300,
        -0.1274, -0.1248, -0.1222, -0.1196, -0.1170, -0.1144, -0.1118, -0.1092,
        -0.1066, -0.1040, -0.1014, -0.0988, -0.0962, -0.0936, -0.0910, -0.0884,
        -0.0858, -0.0832, -0.0806, -0.0780, -0.0754, -0.0728, -0.0702, -0.0676,
        -0.0650, -0.0624, -0.0598, -0.0572, -0.0546, -0.0520, -0.0494, -0.0468,
        -0.0442, -0.0416, -0.0390, -0.0364, -0.0338, -0.0312, -0.0286, -0.0260,
        -0.0234, -0.0208, -0.0182, -0.0156, -0.0130, -0.0104, -0.0078, -0.0052,
        -0.0026, -0.0000,  0.0026,  0.0052,  0.0078,  0.0104,  0.0130,  0.0156,
         0.0182,  0.0208,  0.0234,  0.0260,  0.0286,  0.0312,  0.0338,  0.0364,
         0.0390,  0.0416,  0.0442,  0.0468,  0.0494,  0.0520,  0.0546,  0.0572,
         0.0598,  0.0624,  0.0650,  0.0676,  0.0702,  0.0728,  0.0754,  0.0780,
         0.0806,  0.0832,  0.0858,  0.0884,  0.0910,  0.0936,  0.0962,  0.0988,
         0.1014,  0.1040,  0.1066,  0.1092,  0.1118,  0.1144,  0.1170,  0.1196,
         0.1222,  0.1248,  0.1274,  0.1300,  0.1327,  0.1353,  0.1379,  0.1405,
         0.1431,  0.1457,  0.1483,  0.1509,  0.1535,  0.1561,  0.1587,  0.1613,
         0.1639,  0.1665,  0.1691,  0.1717,  0.1743,  0.1769,  0.1795,  0.1821,
         0.1847,  0.1873,  0.1899,  0.1925,  0.1951,  0.1977,  0.2003,  0.2029,
         0.2081,  0.2133,  0.2159,  0.2289,  0.2315,  0.2549,  0.2757,  0.2835,
         0.3277,  0.3303], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  170
---------Q_out---------
tensor([[[ 0.2206,  1.8382,  2.3529,  ..., -0.9559, -0.4412, -0.5882],
         [ 0.8088,  0.5882,  1.9852,  ..., -1.1764,  0.0735, -0.2941],
         [-0.1471, -0.0735,  2.6470,  ..., -1.9117,  0.3676, -1.0294],
         ...,
         [ 0.4412, -0.3676,  2.7205,  ..., -1.1764,  0.2941,  0.2941],
         [ 0.0000,  0.2941,  2.5735,  ...,  0.4412,  0.2941,  1.0294],
         [ 0.2206, -0.2941,  2.5735,  ...,  0.1471, -1.0294,  0.8823]],

        [[-0.0735,  1.2500,  1.7647,  ..., -0.9559,  0.2206, -0.8823],
         [-0.4412, -0.0000,  2.1323,  ..., -2.7205,  1.1764, -0.0000],
         [ 0.0000,  0.6617,  1.8382,  ..., -2.4999,  0.0735, -0.3676],
         ...,
         [ 0.5882,  0.7353,  1.4705,  ..., -0.2206, -0.0735, -0.6617],
         [ 0.4412,  1.3235,  2.0588,  ...,  0.4412,  0.5147,  1.9117],
         [ 0.9559,  0.0735,  2.5735,  ..., -0.9559,  0.2206, -0.2206]],

        [[-0.4412,  1.2500,  1.6911,  ..., -0.8088, -0.0735, -1.1029],
         [-0.1471,  0.8823,  2.2058,  ..., -0.9559,  0.3676, -0.4412],
         [-0.8823,  0.3676,  1.4705,  ..., -1.1029, -0.0735, -0.8823],
         ...,
         [-0.5882,  0.2206,  2.2793,  ..., -0.5882,  0.2206,  1.2500],
         [-0.2206, -0.2206,  2.4999,  ..., -0.2941,  1.1764,  0.6617],
         [ 0.2941, -0.0000,  3.1617,  ...,  0.2206,  1.6176,  1.0294]],

        ...,

        [[-0.5147,  1.4705,  1.6911,  ..., -0.2941, -0.0735, -0.8088],
         [-1.1764,  0.3676,  0.9559,  ..., -2.4999, -0.8823, -1.1029],
         [-0.4412, -0.0735,  1.9852,  ..., -2.0588,  0.7353, -0.8823],
         ...,
         [ 0.1471, -0.0000,  3.1617,  ..., -0.5147,  1.2500,  0.3676],
         [-0.0735,  0.5147,  2.5735,  ..., -0.4412,  0.6617,  1.6911],
         [ 0.2206, -0.2206,  2.9411,  ...,  0.6617, -0.5147,  1.0294]],

        [[-0.2941,  1.6176,  2.3529,  ..., -0.5882, -0.8088, -1.5441],
         [ 0.2206,  0.3676,  2.4999,  ..., -0.6617,  0.0735, -1.1029],
         [-0.4412, -0.6617,  2.2058,  ..., -1.3970, -0.2206,  0.2941],
         ...,
         [ 0.3676,  0.4412,  3.5293,  ..., -0.7353,  0.5882,  0.4412],
         [ 0.1471,  0.2941,  2.9411,  ..., -0.6617,  0.6617, -0.3676],
         [ 0.1471,  0.2206,  3.6764,  ..., -0.0735,  0.1471,  1.2500]],

        [[ 0.4412,  1.1764,  2.2793,  ..., -0.4412,  0.0735, -1.3970],
         [-0.9559,  0.8088,  1.4705,  ..., -1.0294,  1.1029, -0.7353],
         [ 0.4412,  0.1471,  0.6617,  ..., -1.1029,  2.1323, -1.8382],
         ...,
         [-0.2206,  0.5147,  3.3087,  ...,  0.4412,  1.6911, -0.0735],
         [ 0.0735, -0.4412,  2.6470,  ...,  0.1471,  0.0735,  1.2500],
         [ 0.4412, -0.2941,  3.2352,  ...,  0.1471,  0.8088,  0.2206]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-9.1174, -8.9703, -8.7497, -8.6027, -8.5292, -8.3821, -8.3086, -8.2350,
        -8.1615, -8.0880, -8.0145, -7.9409, -7.8674, -7.7939, -7.7204, -7.6468,
        -7.5733, -7.4998, -7.4262, -7.3527, -7.2792, -7.2057, -7.1321, -7.0586,
        -6.9851, -6.9116, -6.8380, -6.7645, -6.6910, -6.6174, -6.5439, -6.4704,
        -6.3969, -6.3233, -6.2498, -6.1763, -6.1028, -6.0292, -5.9557, -5.8822,
        -5.8086, -5.7351, -5.6616, -5.5881, -5.5145, -5.4410, -5.3675, -5.2940,
        -5.2204, -5.1469, -5.0734, -4.9998, -4.9263, -4.8528, -4.7793, -4.7057,
        -4.6322, -4.5587, -4.4852, -4.4116, -4.3381, -4.2646, -4.1910, -4.1175,
        -4.0440, -3.9705, -3.8969, -3.8234, -3.7499, -3.6764, -3.6028, -3.5293,
        -3.4558, -3.3823, -3.3087, -3.2352, -3.1617, -3.0881, -3.0146, -2.9411,
        -2.8676, -2.7940, -2.7205, -2.6470, -2.5735, -2.4999, -2.4264, -2.3529,
        -2.2793, -2.2058, -2.1323, -2.0588, -1.9852, -1.9117, -1.8382, -1.7647,
        -1.6911, -1.6176, -1.5441, -1.4705, -1.3970, -1.3235, -1.2500, -1.1764,
        -1.1029, -1.0294, -0.9559, -0.8823, -0.8088, -0.7353, -0.6617, -0.5882,
        -0.5147, -0.4412, -0.3676, -0.2941, -0.2206, -0.1471, -0.0735, -0.0000,
         0.0735,  0.1471,  0.2206,  0.2941,  0.3676,  0.4412,  0.5147,  0.5882,
         0.6617,  0.7353,  0.8088,  0.8823,  0.9559,  1.0294,  1.1029,  1.1764,
         1.2500,  1.3235,  1.3970,  1.4705,  1.5441,  1.6176,  1.6911,  1.7647,
         1.8382,  1.9117,  1.9852,  2.0588,  2.1323,  2.2058,  2.2793,  2.3529,
         2.4264,  2.4999,  2.5735,  2.6470,  2.7205,  2.7940,  2.8676,  2.9411,
         3.0146,  3.0881,  3.1617,  3.2352,  3.3087,  3.3823,  3.4558,  3.5293,
         3.6028,  3.6764,  3.7499,  3.8234,  3.8969,  3.9705,  4.0440,  4.1175,
         4.1910,  4.2646,  4.3381,  4.4116,  4.4852,  4.5587,  4.6322,  4.7057,
         4.7793,  4.8528,  4.9263,  4.9998,  5.0734,  5.1469,  5.2204,  5.2940,
         5.3675,  5.4410,  5.5145,  5.5881,  5.6616,  5.7351,  5.8086,  5.8822,
         5.9557,  6.0292,  6.1028,  6.1763,  6.2498,  6.3233,  6.3969,  6.4704,
         6.5439,  6.6174,  6.6910,  6.7645,  6.8380,  6.9116,  6.9851,  7.0586,
         7.1321,  7.2057,  7.2792,  7.3527,  7.4262,  7.4998,  7.5733,  7.7204,
         7.7939], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  225
name:  bert.encoder.layer.7.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0279, -0.0051,  0.0579,  ..., -0.0175,  0.0170,  0.0087],
        [-0.0091, -0.0578, -0.0641,  ..., -0.0496, -0.0359, -0.0224],
        [ 0.0213,  0.0067, -0.0164,  ...,  0.0395, -0.0706,  0.0535],
        ...,
        [-0.0252,  0.0191,  0.0054,  ...,  0.0370,  0.0104, -0.0350],
        [-0.0871, -0.0597, -0.0149,  ...,  0.0918,  0.0060,  0.0283],
        [-0.0057,  0.0488,  0.0709,  ..., -0.0652, -0.0684,  0.0007]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0274, -0.0034,  0.0582,  ..., -0.0171,  0.0171,  0.0103],
        [-0.0103, -0.0582, -0.0650,  ..., -0.0479, -0.0376, -0.0239],
        [ 0.0205,  0.0068, -0.0171,  ...,  0.0410, -0.0718,  0.0547],
        ...,
        [-0.0239,  0.0205,  0.0068,  ...,  0.0376,  0.0103, -0.0342],
        [-0.0855, -0.0582, -0.0137,  ...,  0.0924,  0.0068,  0.0274],
        [-0.0068,  0.0479,  0.0718,  ..., -0.0650, -0.0684,  0.0000]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3865, -0.3626, -0.3215, -0.3044, -0.3010, -0.2908, -0.2736, -0.2702,
        -0.2600, -0.2531, -0.2463, -0.2394, -0.2292, -0.2258, -0.2223, -0.2155,
        -0.2121, -0.2087, -0.2052, -0.1984, -0.1950, -0.1916, -0.1881, -0.1847,
        -0.1813, -0.1779, -0.1745, -0.1710, -0.1676, -0.1642, -0.1608, -0.1573,
        -0.1539, -0.1505, -0.1471, -0.1437, -0.1402, -0.1368, -0.1334, -0.1300,
        -0.1266, -0.1231, -0.1197, -0.1163, -0.1129, -0.1095, -0.1060, -0.1026,
        -0.0992, -0.0958, -0.0924, -0.0889, -0.0855, -0.0821, -0.0787, -0.0753,
        -0.0718, -0.0684, -0.0650, -0.0616, -0.0582, -0.0547, -0.0513, -0.0479,
        -0.0445, -0.0410, -0.0376, -0.0342, -0.0308, -0.0274, -0.0239, -0.0205,
        -0.0171, -0.0137, -0.0103, -0.0068, -0.0034, -0.0000,  0.0034,  0.0068,
         0.0103,  0.0137,  0.0171,  0.0205,  0.0239,  0.0274,  0.0308,  0.0342,
         0.0376,  0.0410,  0.0445,  0.0479,  0.0513,  0.0547,  0.0582,  0.0616,
         0.0650,  0.0684,  0.0718,  0.0753,  0.0787,  0.0821,  0.0855,  0.0889,
         0.0924,  0.0958,  0.0992,  0.1026,  0.1060,  0.1095,  0.1129,  0.1163,
         0.1197,  0.1231,  0.1266,  0.1300,  0.1334,  0.1368,  0.1402,  0.1437,
         0.1471,  0.1505,  0.1539,  0.1573,  0.1608,  0.1642,  0.1676,  0.1710,
         0.1745,  0.1779,  0.1813,  0.1847,  0.1881,  0.1916,  0.1950,  0.1984,
         0.2018,  0.2052,  0.2087,  0.2121,  0.2189,  0.2223,  0.2258,  0.2326,
         0.2360,  0.2394,  0.2429,  0.2497,  0.2668,  0.2736,  0.2805,  0.2839,
         0.2873,  0.3147,  0.3215,  0.3250,  0.3352,  0.3694,  0.4344],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  159
---------Q_out---------
tensor([[[-0.9892,  0.9274,  1.7929,  ..., -0.8655,  0.3709, -0.0000],
         [-0.6801, -0.6182, -2.5348,  ..., -1.2365, -0.6182, -2.0402],
         [-1.5456, -0.3709, -2.7203,  ...,  0.3091, -0.8655, -0.4946],
         ...,
         [ 0.3091, -1.2365,  0.4328,  ..., -0.4328, -0.1236, -0.6801],
         [ 0.3091, -0.5564,  0.9892,  ...,  0.3709, -0.6182,  0.1236],
         [-0.1236, -1.4220,  0.0618,  ...,  0.6801,  0.2473, -0.2473]],

        [[-0.2473,  0.8037,  1.9165,  ..., -0.3091, -0.5564, -0.1855],
         [-1.7929, -0.6182, -3.3385,  ..., -1.9784, -1.0510, -2.5348],
         [-2.0402, -0.1855, -2.5966,  ..., -2.0402, -1.5456, -2.1638],
         ...,
         [ 0.6801,  1.4838, -1.2365,  ...,  0.7419, -0.6801,  2.3493],
         [ 0.5564,  0.3709, -0.0618,  ...,  0.4328, -0.7419,  0.0618],
         [ 0.4946, -0.1855,  4.1422,  ..., -0.9892,  0.4946, -0.2473]],

        [[-1.2365,  0.3709,  1.2983,  ..., -0.9892,  0.8655, -0.1236],
         [-0.5564, -0.1855, -2.2875,  ..., -1.4838, -0.6182, -2.8439],
         [-1.1747, -0.6801, -1.9165,  ..., -1.3601, -0.0618, -1.6074],
         ...,
         [-0.1855, -0.6182,  1.6074,  ..., -0.8655,  0.4946, -1.8547],
         [-0.2473, -0.1855,  0.3091,  ..., -0.1855,  0.1855, -0.4328],
         [-0.1236, -0.9892,  0.2473,  ..., -0.3709, -0.1236, -0.1855]],

        ...,

        [[-1.0510,  0.9892,  2.4111,  ..., -0.1855, -0.1855, -0.3709],
         [-0.4328, -1.9784, -2.9676,  ..., -1.7929, -0.4946, -1.6074],
         [-1.7311, -1.0510, -1.6692,  ..., -2.0402, -0.2473, -0.8655],
         ...,
         [ 0.4328, -0.3709, -0.2473,  ..., -0.6182,  0.6801, -0.4946],
         [ 0.1236, -0.4946,  0.5564,  ..., -0.1236,  0.6182,  0.4946],
         [ 0.6801, -0.3091,  0.4946,  ...,  0.0000,  0.4328, -0.1236]],

        [[ 0.2473,  1.3601,  1.2365,  ..., -1.2983, -0.3091,  0.3091],
         [-0.0618, -0.7419, -3.0912,  ..., -1.1747,  0.5564, -1.2983],
         [-0.4328, -0.9274, -3.2767,  ...,  0.0618,  0.9892, -1.7311],
         ...,
         [ 0.1236, -0.6182,  0.3091,  ..., -1.7311,  0.2473, -0.9892],
         [-0.0618, -0.6801,  0.1855,  ..., -1.7311,  0.1236, -1.5456],
         [ 0.4328,  0.0618,  0.9274,  ..., -0.3709, -0.4328,  0.0618]],

        [[-0.3709,  1.0510,  1.6692,  ..., -1.0510,  0.2473, -0.0618],
         [-0.6182,  0.1236, -3.1530,  ..., -1.0510,  0.8655, -2.2257],
         [ 0.0000,  0.1236, -3.7713,  ..., -1.6074, -0.0618, -2.2875],
         ...,
         [-0.1236, -0.1855, -0.0618,  ...,  0.3709, -0.1236,  0.0000],
         [-0.4946, -0.4946,  0.6801,  ...,  0.3091,  0.5564,  0.6801],
         [ 0.4946, -0.5564,  0.1236,  ..., -0.3709, -0.2473,  0.0618]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.6044, -7.3571, -7.1098, -6.8006, -6.7388, -6.6152, -6.4915, -6.4297,
        -6.3679, -6.3060, -6.2442, -6.1824, -6.1206, -6.0588, -5.9969, -5.9351,
        -5.8733, -5.8115, -5.7496, -5.6878, -5.6260, -5.5642, -5.5023, -5.4405,
        -5.3787, -5.3169, -5.2550, -5.1932, -5.1314, -5.0696, -5.0077, -4.9459,
        -4.8841, -4.8223, -4.7604, -4.6986, -4.6368, -4.5750, -4.5132, -4.4513,
        -4.3895, -4.3277, -4.2659, -4.2040, -4.1422, -4.0804, -4.0186, -3.9567,
        -3.8949, -3.8331, -3.7713, -3.7094, -3.6476, -3.5858, -3.5240, -3.4621,
        -3.4003, -3.3385, -3.2767, -3.2148, -3.1530, -3.0912, -3.0294, -2.9676,
        -2.9057, -2.8439, -2.7821, -2.7203, -2.6584, -2.5966, -2.5348, -2.4730,
        -2.4111, -2.3493, -2.2875, -2.2257, -2.1638, -2.1020, -2.0402, -1.9784,
        -1.9165, -1.8547, -1.7929, -1.7311, -1.6692, -1.6074, -1.5456, -1.4838,
        -1.4220, -1.3601, -1.2983, -1.2365, -1.1747, -1.1128, -1.0510, -0.9892,
        -0.9274, -0.8655, -0.8037, -0.7419, -0.6801, -0.6182, -0.5564, -0.4946,
        -0.4328, -0.3709, -0.3091, -0.2473, -0.1855, -0.1236, -0.0618, -0.0000,
         0.0618,  0.1236,  0.1855,  0.2473,  0.3091,  0.3709,  0.4328,  0.4946,
         0.5564,  0.6182,  0.6801,  0.7419,  0.8037,  0.8655,  0.9274,  0.9892,
         1.0510,  1.1128,  1.1747,  1.2365,  1.2983,  1.3601,  1.4220,  1.4838,
         1.5456,  1.6074,  1.6692,  1.7311,  1.7929,  1.8547,  1.9165,  1.9784,
         2.0402,  2.1020,  2.1638,  2.2257,  2.2875,  2.3493,  2.4111,  2.4730,
         2.5348,  2.5966,  2.6584,  2.7203,  2.7821,  2.8439,  2.9057,  2.9676,
         3.0294,  3.0912,  3.1530,  3.2148,  3.2767,  3.3385,  3.4003,  3.4621,
         3.5240,  3.5858,  3.6476,  3.7094,  3.7713,  3.8331,  3.8949,  3.9567,
         4.0186,  4.0804,  4.1422,  4.2040,  4.2659,  4.3277,  4.3895,  4.4513,
         4.5132,  4.5750,  4.6368,  4.6986,  4.7604,  4.8223,  4.8841,  4.9459,
         5.0077,  5.0696,  5.1314,  5.1932,  5.2550,  5.3169,  5.3787,  5.4405,
         5.5023,  5.5642,  5.6260,  5.6878,  5.7496,  5.8115,  5.8733,  5.9351,
         5.9969,  6.0588,  6.1206,  6.1824,  6.2442,  6.3060,  6.3679,  6.4297,
         6.5533,  6.6152,  6.6770,  6.7388,  6.8006,  6.9861,  7.0479,  7.1098,
         7.2952,  7.4807,  7.7898], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  227
name:  bert.encoder.layer.7.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0107,  0.0324, -0.0397,  ...,  0.0480, -0.0542,  0.0252],
        [-0.0348, -0.0096, -0.0143,  ..., -0.0131,  0.0102, -0.0173],
        [ 0.0318,  0.0083, -0.0023,  ...,  0.0721, -0.0554, -0.0395],
        ...,
        [-0.0123,  0.0279, -0.0338,  ..., -0.0104,  0.0596,  0.0071],
        [ 0.0044,  0.0311, -0.0018,  ...,  0.0631,  0.0054, -0.0358],
        [ 0.0224, -0.0178, -0.0173,  ..., -0.0015,  0.0244, -0.0927]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0107,  0.0322, -0.0399,  ...,  0.0476, -0.0537,  0.0245],
        [-0.0353, -0.0092, -0.0138,  ..., -0.0138,  0.0107, -0.0169],
        [ 0.0322,  0.0077, -0.0015,  ...,  0.0721, -0.0552, -0.0399],
        ...,
        [-0.0123,  0.0276, -0.0338,  ..., -0.0107,  0.0598,  0.0077],
        [ 0.0046,  0.0307, -0.0015,  ...,  0.0629,  0.0061, -0.0353],
        [ 0.0230, -0.0184, -0.0169,  ..., -0.0015,  0.0245, -0.0920]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.1948, -0.1902, -0.1872, -0.1734, -0.1626, -0.1611, -0.1595, -0.1580,
        -0.1565, -0.1549, -0.1519, -0.1503, -0.1488, -0.1473, -0.1442, -0.1427,
        -0.1411, -0.1396, -0.1381, -0.1365, -0.1350, -0.1335, -0.1319, -0.1304,
        -0.1289, -0.1273, -0.1258, -0.1243, -0.1227, -0.1212, -0.1197, -0.1181,
        -0.1166, -0.1151, -0.1135, -0.1120, -0.1105, -0.1089, -0.1074, -0.1059,
        -0.1043, -0.1028, -0.1013, -0.0997, -0.0982, -0.0966, -0.0951, -0.0936,
        -0.0920, -0.0905, -0.0890, -0.0874, -0.0859, -0.0844, -0.0828, -0.0813,
        -0.0798, -0.0782, -0.0767, -0.0752, -0.0736, -0.0721, -0.0706, -0.0690,
        -0.0675, -0.0660, -0.0644, -0.0629, -0.0614, -0.0598, -0.0583, -0.0568,
        -0.0552, -0.0537, -0.0522, -0.0506, -0.0491, -0.0476, -0.0460, -0.0445,
        -0.0430, -0.0414, -0.0399, -0.0384, -0.0368, -0.0353, -0.0338, -0.0322,
        -0.0307, -0.0291, -0.0276, -0.0261, -0.0245, -0.0230, -0.0215, -0.0199,
        -0.0184, -0.0169, -0.0153, -0.0138, -0.0123, -0.0107, -0.0092, -0.0077,
        -0.0061, -0.0046, -0.0031, -0.0015, -0.0000,  0.0015,  0.0031,  0.0046,
         0.0061,  0.0077,  0.0092,  0.0107,  0.0123,  0.0138,  0.0153,  0.0169,
         0.0184,  0.0199,  0.0215,  0.0230,  0.0245,  0.0261,  0.0276,  0.0291,
         0.0307,  0.0322,  0.0338,  0.0353,  0.0368,  0.0384,  0.0399,  0.0414,
         0.0430,  0.0445,  0.0460,  0.0476,  0.0491,  0.0506,  0.0522,  0.0537,
         0.0552,  0.0568,  0.0583,  0.0598,  0.0614,  0.0629,  0.0644,  0.0660,
         0.0675,  0.0690,  0.0706,  0.0721,  0.0736,  0.0752,  0.0767,  0.0782,
         0.0798,  0.0813,  0.0828,  0.0844,  0.0859,  0.0874,  0.0890,  0.0905,
         0.0920,  0.0936,  0.0951,  0.0966,  0.0982,  0.0997,  0.1013,  0.1028,
         0.1043,  0.1059,  0.1074,  0.1089,  0.1105,  0.1120,  0.1135,  0.1151,
         0.1166,  0.1181,  0.1197,  0.1212,  0.1227,  0.1243,  0.1258,  0.1273,
         0.1289,  0.1304,  0.1319,  0.1335,  0.1350,  0.1365,  0.1381,  0.1396,
         0.1411,  0.1427,  0.1442,  0.1457,  0.1473,  0.1503,  0.1549,  0.1565,
         0.1595,  0.1611,  0.1688], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  211
---------Q_out---------
None
name:  bert.encoder.layer.7.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0549,  0.0256, -0.0145,  ..., -0.0431,  0.0537, -0.0171],
        [ 0.0101, -0.0155,  0.0108,  ...,  0.0380,  0.0296, -0.0144],
        [ 0.0220, -0.0560, -0.0183,  ..., -0.0105,  0.0720,  0.0122],
        ...,
        [ 0.0729,  0.0478, -0.0461,  ..., -0.0149,  0.0040, -0.0542],
        [-0.0264, -0.0266, -0.0192,  ...,  0.0353,  0.0349,  0.0251],
        [ 0.0124, -0.0244, -0.0092,  ..., -0.0081,  0.0517,  0.0173]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0544,  0.0252, -0.0136,  ..., -0.0427,  0.0544, -0.0175],
        [ 0.0097, -0.0155,  0.0117,  ...,  0.0388,  0.0291, -0.0136],
        [ 0.0214, -0.0563, -0.0175,  ..., -0.0097,  0.0718,  0.0117],
        ...,
        [ 0.0738,  0.0485, -0.0466,  ..., -0.0155,  0.0039, -0.0544],
        [-0.0272, -0.0272, -0.0194,  ...,  0.0350,  0.0350,  0.0252],
        [ 0.0117, -0.0252, -0.0097,  ..., -0.0078,  0.0524,  0.0175]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2447, -0.1883, -0.1825, -0.1748, -0.1689, -0.1670, -0.1650, -0.1631,
        -0.1612, -0.1592, -0.1573, -0.1553, -0.1534, -0.1515, -0.1495, -0.1476,
        -0.1456, -0.1437, -0.1417, -0.1398, -0.1379, -0.1359, -0.1340, -0.1320,
        -0.1301, -0.1282, -0.1262, -0.1243, -0.1223, -0.1204, -0.1184, -0.1165,
        -0.1146, -0.1126, -0.1107, -0.1087, -0.1068, -0.1049, -0.1029, -0.1010,
        -0.0990, -0.0971, -0.0951, -0.0932, -0.0913, -0.0893, -0.0874, -0.0854,
        -0.0835, -0.0816, -0.0796, -0.0777, -0.0757, -0.0738, -0.0718, -0.0699,
        -0.0680, -0.0660, -0.0641, -0.0621, -0.0602, -0.0583, -0.0563, -0.0544,
        -0.0524, -0.0505, -0.0485, -0.0466, -0.0447, -0.0427, -0.0408, -0.0388,
        -0.0369, -0.0350, -0.0330, -0.0311, -0.0291, -0.0272, -0.0252, -0.0233,
        -0.0214, -0.0194, -0.0175, -0.0155, -0.0136, -0.0117, -0.0097, -0.0078,
        -0.0058, -0.0039, -0.0019, -0.0000,  0.0019,  0.0039,  0.0058,  0.0078,
         0.0097,  0.0117,  0.0136,  0.0155,  0.0175,  0.0194,  0.0214,  0.0233,
         0.0252,  0.0272,  0.0291,  0.0311,  0.0330,  0.0350,  0.0369,  0.0388,
         0.0408,  0.0427,  0.0447,  0.0466,  0.0485,  0.0505,  0.0524,  0.0544,
         0.0563,  0.0583,  0.0602,  0.0621,  0.0641,  0.0660,  0.0680,  0.0699,
         0.0718,  0.0738,  0.0757,  0.0777,  0.0796,  0.0816,  0.0835,  0.0854,
         0.0874,  0.0893,  0.0913,  0.0932,  0.0951,  0.0971,  0.0990,  0.1010,
         0.1029,  0.1049,  0.1068,  0.1087,  0.1107,  0.1126,  0.1146,  0.1165,
         0.1184,  0.1204,  0.1223,  0.1243,  0.1262,  0.1282,  0.1301,  0.1320,
         0.1340,  0.1359,  0.1379,  0.1398,  0.1417,  0.1437,  0.1456,  0.1476,
         0.1495,  0.1553,  0.1573,  0.1612,  0.1650,  0.1709,  0.1728,  0.1767,
         0.1786,  0.1825,  0.1845,  0.2155,  0.2466], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  181
---------Q_out---------
None
name:  bert.encoder.layer.7.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0361, -0.0048, -0.0167,  ...,  0.0376, -0.0446,  0.0110],
        [-0.1222, -0.0474,  0.0137,  ...,  0.0090, -0.0010,  0.0240],
        [ 0.0303,  0.0255, -0.0532,  ..., -0.0556, -0.0427, -0.0167],
        ...,
        [-0.0059, -0.0156, -0.0002,  ..., -0.0195,  0.0034,  0.0173],
        [ 0.0430, -0.0101,  0.0281,  ...,  0.0096,  0.0716,  0.0189],
        [-0.0351, -0.0316, -0.0614,  ...,  0.0151,  0.0182, -0.0189]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0376, -0.0063, -0.0157,  ...,  0.0376, -0.0438,  0.0125],
        [-0.1221, -0.0470,  0.0125,  ...,  0.0094, -0.0000,  0.0250],
        [ 0.0313,  0.0250, -0.0532,  ..., -0.0564, -0.0438, -0.0157],
        ...,
        [-0.0063, -0.0157, -0.0000,  ..., -0.0188,  0.0031,  0.0188],
        [ 0.0438, -0.0094,  0.0282,  ...,  0.0094,  0.0720,  0.0188],
        [-0.0344, -0.0313, -0.0626,  ...,  0.0157,  0.0188, -0.0188]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3193, -0.3162, -0.2786, -0.2724, -0.2630, -0.2567, -0.2536, -0.2505,
        -0.2317, -0.2285, -0.2223, -0.2191, -0.2160, -0.2129, -0.2098, -0.2066,
        -0.2035, -0.2004, -0.1972, -0.1941, -0.1910, -0.1878, -0.1847, -0.1816,
        -0.1784, -0.1753, -0.1722, -0.1691, -0.1659, -0.1628, -0.1597, -0.1565,
        -0.1534, -0.1503, -0.1471, -0.1440, -0.1409, -0.1377, -0.1346, -0.1315,
        -0.1284, -0.1252, -0.1221, -0.1190, -0.1158, -0.1127, -0.1096, -0.1064,
        -0.1033, -0.1002, -0.0970, -0.0939, -0.0908, -0.0877, -0.0845, -0.0814,
        -0.0783, -0.0751, -0.0720, -0.0689, -0.0657, -0.0626, -0.0595, -0.0564,
        -0.0532, -0.0501, -0.0470, -0.0438, -0.0407, -0.0376, -0.0344, -0.0313,
        -0.0282, -0.0250, -0.0219, -0.0188, -0.0157, -0.0125, -0.0094, -0.0063,
        -0.0031, -0.0000,  0.0031,  0.0063,  0.0094,  0.0125,  0.0157,  0.0188,
         0.0219,  0.0250,  0.0282,  0.0313,  0.0344,  0.0376,  0.0407,  0.0438,
         0.0470,  0.0501,  0.0532,  0.0564,  0.0595,  0.0626,  0.0657,  0.0689,
         0.0720,  0.0751,  0.0783,  0.0814,  0.0845,  0.0877,  0.0908,  0.0939,
         0.0970,  0.1002,  0.1033,  0.1064,  0.1096,  0.1127,  0.1158,  0.1190,
         0.1221,  0.1252,  0.1284,  0.1315,  0.1346,  0.1377,  0.1409,  0.1440,
         0.1471,  0.1503,  0.1534,  0.1565,  0.1597,  0.1628,  0.1659,  0.1691,
         0.1722,  0.1753,  0.1784,  0.1816,  0.1847,  0.1878,  0.1910,  0.1941,
         0.1972,  0.2004,  0.2035,  0.2066,  0.2129,  0.2160,  0.2191,  0.2223,
         0.2254,  0.2285,  0.2317,  0.2348,  0.2379,  0.2442,  0.2505,  0.2567,
         0.2630,  0.2661,  0.2692,  0.2849,  0.3005,  0.3225,  0.3256,  0.3976],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  168
---------Q_out---------
None
name:  bert.encoder.layer.7.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-2.1992e-02, -1.1965e-01,  8.6214e-03,  ..., -3.8491e-02,
          7.3150e-03,  5.9484e-03],
        [-2.2700e-02, -2.8286e-02,  2.1415e-02,  ...,  2.2163e-02,
         -3.4502e-02,  1.0215e-02],
        [ 3.2862e-02,  4.4804e-02,  9.3634e-05,  ..., -6.2634e-02,
          6.6018e-02,  1.5757e-02],
        ...,
        [-1.1545e-01,  5.4823e-02, -1.0641e-02,  ..., -2.1837e-02,
          8.2366e-02,  1.4478e-02],
        [-1.2734e-02,  3.6414e-02,  1.9589e-02,  ..., -1.3002e-02,
          1.1071e-01, -1.8742e-02],
        [-4.5867e-02,  1.1043e-02, -1.3296e-02,  ..., -1.1185e-02,
          4.3985e-02,  1.2282e-02]], device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0280, -0.1122,  0.0000,  ..., -0.0280,  0.0000,  0.0000],
        [-0.0280, -0.0280,  0.0280,  ...,  0.0280, -0.0280,  0.0000],
        [ 0.0280,  0.0561,  0.0000,  ..., -0.0561,  0.0561,  0.0280],
        ...,
        [-0.1122,  0.0561, -0.0000,  ..., -0.0280,  0.0841,  0.0280],
        [-0.0000,  0.0280,  0.0280,  ..., -0.0000,  0.1122, -0.0280],
        [-0.0561,  0.0000, -0.0000,  ..., -0.0000,  0.0561,  0.0000]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-3.5619, -2.4961, -0.8975, -0.7853, -0.6731, -0.5890, -0.4487, -0.4207,
        -0.3926, -0.3646, -0.3366, -0.3085, -0.2805, -0.2524, -0.2244, -0.1963,
        -0.1683, -0.1402, -0.1122, -0.0841, -0.0561, -0.0280, -0.0000,  0.0280,
         0.0561,  0.0841,  0.1122,  0.1402,  0.1683,  0.1963,  0.2244,  0.2524,
         0.2805,  0.3085,  0.3366,  0.3646,  0.3926,  0.4207,  0.4487,  0.5048,
         0.5329,  0.7573,  0.9536], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  43
---------Q_out---------
None
name:  bert.encoder.layer.8.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0448,  0.0628,  0.0168,  ..., -0.0453, -0.0251,  0.0062],
        [ 0.0230, -0.0180,  0.0191,  ...,  0.0315, -0.0021,  0.0480],
        [ 0.0307, -0.1262,  0.0069,  ..., -0.0443,  0.0412, -0.0344],
        ...,
        [-0.0030, -0.0355, -0.0323,  ..., -0.0217, -0.0044, -0.0324],
        [ 0.0048, -0.0319, -0.0534,  ...,  0.0861, -0.0552,  0.0022],
        [-0.0212,  0.0549,  0.0542,  ...,  0.0029,  0.0173,  0.0269]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0448,  0.0628,  0.0179,  ..., -0.0448, -0.0239,  0.0060],
        [ 0.0239, -0.0179,  0.0179,  ...,  0.0329, -0.0030,  0.0478],
        [ 0.0299, -0.1255,  0.0060,  ..., -0.0448,  0.0418, -0.0329],
        ...,
        [-0.0030, -0.0359, -0.0329,  ..., -0.0209, -0.0030, -0.0329],
        [ 0.0060, -0.0329, -0.0538,  ...,  0.0867, -0.0538,  0.0030],
        [-0.0209,  0.0538,  0.0538,  ...,  0.0030,  0.0179,  0.0269]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3796, -0.2481, -0.2391, -0.2361, -0.2301, -0.2242, -0.2152, -0.2122,
        -0.2092, -0.2062, -0.2003, -0.1973, -0.1943, -0.1913, -0.1883, -0.1853,
        -0.1823, -0.1793, -0.1763, -0.1734, -0.1704, -0.1674, -0.1644, -0.1614,
        -0.1584, -0.1554, -0.1524, -0.1494, -0.1465, -0.1435, -0.1405, -0.1375,
        -0.1345, -0.1315, -0.1285, -0.1255, -0.1225, -0.1196, -0.1166, -0.1136,
        -0.1106, -0.1076, -0.1046, -0.1016, -0.0986, -0.0956, -0.0927, -0.0897,
        -0.0867, -0.0837, -0.0807, -0.0777, -0.0747, -0.0717, -0.0687, -0.0658,
        -0.0628, -0.0598, -0.0568, -0.0538, -0.0508, -0.0478, -0.0448, -0.0418,
        -0.0389, -0.0359, -0.0329, -0.0299, -0.0269, -0.0239, -0.0209, -0.0179,
        -0.0149, -0.0120, -0.0090, -0.0060, -0.0030, -0.0000,  0.0030,  0.0060,
         0.0090,  0.0120,  0.0149,  0.0179,  0.0209,  0.0239,  0.0269,  0.0299,
         0.0329,  0.0359,  0.0389,  0.0418,  0.0448,  0.0478,  0.0508,  0.0538,
         0.0568,  0.0598,  0.0628,  0.0658,  0.0687,  0.0717,  0.0747,  0.0777,
         0.0807,  0.0837,  0.0867,  0.0897,  0.0927,  0.0956,  0.0986,  0.1016,
         0.1046,  0.1076,  0.1106,  0.1136,  0.1166,  0.1196,  0.1225,  0.1255,
         0.1285,  0.1315,  0.1345,  0.1375,  0.1405,  0.1435,  0.1465,  0.1494,
         0.1524,  0.1554,  0.1584,  0.1614,  0.1644,  0.1674,  0.1704,  0.1734,
         0.1763,  0.1793,  0.1823,  0.1853,  0.1883,  0.1913,  0.1943,  0.1973,
         0.2003,  0.2032,  0.2062,  0.2122,  0.2152,  0.2212,  0.2272,  0.2331,
         0.2391,  0.2421,  0.2481,  0.2600,  0.2720,  0.2750,  0.2810,  0.3497],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  160
---------Q_out---------
tensor([[[-0.7180, -0.7778,  0.5385,  ..., -1.0171,  1.3761, -2.6924],
         [-1.1966, -0.2393,  0.7180,  ..., -0.5983, -0.1795, -0.6581],
         [ 0.0598,  0.2992,  0.7778,  ...,  0.5983, -0.4786, -0.7180],
         ...,
         [-0.0000, -0.4188,  0.4188,  ...,  0.0000, -1.0770, -2.0343],
         [-0.3590, -0.8376, -0.5385,  ..., -0.4786, -0.2992, -1.0770],
         [-0.4786, -0.4188, -0.1197,  ..., -0.6581, -0.2992, -1.4958]],

        [[-0.6581, -0.2393,  0.5385,  ..., -1.3163,  1.0770, -2.8121],
         [ 1.1966,  1.4359,  1.0770,  ...,  1.1966, -1.6753, -2.3334],
         [ 1.6154,  1.2565,  0.3590,  ...,  0.2992, -2.2138, -2.6924],
         ...,
         [-0.1795, -1.1966,  2.6924,  ..., -0.7778,  0.5983, -1.2565],
         [-0.2992, -0.3590, -0.2992,  ..., -0.2393,  0.4188, -2.6924],
         [-0.1795, -0.7778,  0.0598,  ...,  0.2992, -0.3590, -0.7180]],

        [[-0.5983, -0.3590,  0.3590,  ..., -1.1368,  1.0770, -2.8719],
         [-1.1368,  0.4188, -0.1197,  ..., -1.4958, -0.1197, -1.7351],
         [-0.3590,  0.6581,  0.5385,  ...,  0.2393, -1.0770, -0.9573],
         ...,
         [ 0.2992, -1.0171, -0.0598,  ..., -0.5385,  1.3163, -1.5556],
         [-0.5983, -0.1197, -0.4188,  ..., -0.3590, -1.0171, -1.3761],
         [-0.5983, -0.1795, -0.1795,  ..., -0.5983, -2.0941, -1.3163]],

        ...,

        [[-0.6581, -0.4786,  0.8975,  ..., -1.0770,  1.3163, -2.8121],
         [ 1.6154,  0.2393,  0.2992,  ...,  0.0598, -1.3761, -1.6753],
         [ 2.1539,  1.0770,  1.4958,  ...,  0.2393, -2.4531, -2.0343],
         ...,
         [ 0.0598, -0.7180, -1.1368,  ...,  0.5385, -1.0171, -2.3334],
         [-0.5385, -0.6581, -1.0171,  ..., -0.0598, -0.6581, -1.6753],
         [-1.1368, -0.1197, -0.1795,  ..., -1.0171, -0.0000, -1.4359]],

        [[-0.9573, -0.2992,  0.4188,  ..., -1.6753,  1.2565, -2.7522],
         [ 1.3761,  0.5983,  0.7180,  ...,  0.6581, -0.1197, -2.6326],
         [ 1.5556,  1.5556,  0.5385,  ...,  0.7180, -0.4786, -1.4958],
         ...,
         [-0.5983, -0.4188, -1.0770,  ..., -0.3590, -0.2992, -1.7949],
         [-0.8376, -0.1795, -1.2565,  ..., -0.1197, -0.2992, -2.3334],
         [-0.2393, -0.7778,  0.1197,  ..., -0.7778, -0.3590, -2.1539]],

        [[-0.8975, -0.8376,  0.1795,  ..., -0.8975,  1.3163, -2.9916],
         [ 2.1539,  1.2565,  0.8975,  ...,  2.0941, -0.8376, -1.0770],
         [ 0.6581,  1.9744, -0.2393,  ...,  3.3505,  0.7180, -2.3334],
         ...,
         [-0.5983, -1.3761,  0.2992,  ..., -1.0171, -0.6581, -2.5129],
         [-0.9573, -0.4188,  0.2992,  ..., -1.4359,  0.0598, -0.7180],
         [-1.3761, -1.0171, -0.3590,  ..., -1.0171, -0.5385, -1.7949]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.5985, -7.4191, -7.3592, -7.2396, -7.1199, -6.9404, -6.7609, -6.7011,
        -6.6413, -6.5814, -6.5216, -6.4618, -6.4019, -6.3421, -6.2823, -6.2224,
        -6.1626, -6.1028, -6.0429, -5.9831, -5.9233, -5.8634, -5.8036, -5.7438,
        -5.6840, -5.6241, -5.5643, -5.5045, -5.4446, -5.3848, -5.3250, -5.2651,
        -5.2053, -5.1455, -5.0856, -5.0258, -4.9660, -4.9061, -4.8463, -4.7865,
        -4.7267, -4.6668, -4.6070, -4.5472, -4.4873, -4.4275, -4.3677, -4.3078,
        -4.2480, -4.1882, -4.1283, -4.0685, -4.0087, -3.9489, -3.8890, -3.8292,
        -3.7694, -3.7095, -3.6497, -3.5899, -3.5300, -3.4702, -3.4104, -3.3505,
        -3.2907, -3.2309, -3.1710, -3.1112, -3.0514, -2.9916, -2.9317, -2.8719,
        -2.8121, -2.7522, -2.6924, -2.6326, -2.5727, -2.5129, -2.4531, -2.3932,
        -2.3334, -2.2736, -2.2138, -2.1539, -2.0941, -2.0343, -1.9744, -1.9146,
        -1.8548, -1.7949, -1.7351, -1.6753, -1.6154, -1.5556, -1.4958, -1.4359,
        -1.3761, -1.3163, -1.2565, -1.1966, -1.1368, -1.0770, -1.0171, -0.9573,
        -0.8975, -0.8376, -0.7778, -0.7180, -0.6581, -0.5983, -0.5385, -0.4786,
        -0.4188, -0.3590, -0.2992, -0.2393, -0.1795, -0.1197, -0.0598, -0.0000,
         0.0598,  0.1197,  0.1795,  0.2393,  0.2992,  0.3590,  0.4188,  0.4786,
         0.5385,  0.5983,  0.6581,  0.7180,  0.7778,  0.8376,  0.8975,  0.9573,
         1.0171,  1.0770,  1.1368,  1.1966,  1.2565,  1.3163,  1.3761,  1.4359,
         1.4958,  1.5556,  1.6154,  1.6753,  1.7351,  1.7949,  1.8548,  1.9146,
         1.9744,  2.0343,  2.0941,  2.1539,  2.2138,  2.2736,  2.3334,  2.3932,
         2.4531,  2.5129,  2.5727,  2.6326,  2.6924,  2.7522,  2.8121,  2.8719,
         2.9317,  2.9916,  3.0514,  3.1112,  3.1710,  3.2309,  3.2907,  3.3505,
         3.4104,  3.4702,  3.5300,  3.5899,  3.6497,  3.7095,  3.7694,  3.8292,
         3.8890,  3.9489,  4.0087,  4.0685,  4.1283,  4.1882,  4.2480,  4.3078,
         4.3677,  4.4275,  4.4873,  4.5472,  4.6070,  4.6668,  4.7267,  4.7865,
         4.8463,  4.9061,  4.9660,  5.0258,  5.0856,  5.1455,  5.2053,  5.2651,
         5.3250,  5.3848,  5.4446,  5.5045,  5.5643,  5.6241,  5.6840,  5.7438,
         5.8036,  5.8634,  5.9233,  5.9831,  6.0429,  6.1028,  6.1626,  6.2224,
         6.2823,  6.3421,  6.4019,  6.4618,  6.5216,  6.5814,  6.6413,  6.7011,
         6.7609,  6.8207,  6.8806,  7.3592], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  236
name:  bert.encoder.layer.8.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0199, -0.0007,  0.0490,  ...,  0.0125,  0.0142, -0.0053],
        [ 0.0071, -0.0560,  0.0617,  ..., -0.0308, -0.0218,  0.0287],
        [ 0.0395,  0.0394,  0.0221,  ..., -0.0357,  0.1048,  0.0117],
        ...,
        [ 0.1151, -0.0857, -0.0310,  ..., -0.0281,  0.0004, -0.0193],
        [-0.0265, -0.0669, -0.0385,  ..., -0.0815,  0.0204, -0.0708],
        [-0.0820,  0.0152,  0.1041,  ..., -0.0557,  0.0497,  0.0257]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0190, -0.0000,  0.0475,  ...,  0.0127,  0.0127, -0.0063],
        [ 0.0063, -0.0569,  0.0601,  ..., -0.0316, -0.0221,  0.0285],
        [ 0.0380,  0.0380,  0.0221,  ..., -0.0348,  0.1044,  0.0127],
        ...,
        [ 0.1139, -0.0854, -0.0316,  ..., -0.0285,  0.0000, -0.0190],
        [-0.0253, -0.0664, -0.0380,  ..., -0.0823,  0.0190, -0.0696],
        [-0.0823,  0.0158,  0.1044,  ..., -0.0569,  0.0506,  0.0253]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.4018, -0.3701, -0.3448, -0.3385, -0.3195, -0.3037, -0.2974, -0.2942,
        -0.2879, -0.2815, -0.2784, -0.2752, -0.2657, -0.2594, -0.2499, -0.2404,
        -0.2341, -0.2278, -0.2214, -0.2183, -0.2088, -0.2056, -0.2025, -0.1993,
        -0.1961, -0.1930, -0.1898, -0.1866, -0.1835, -0.1803, -0.1772, -0.1740,
        -0.1708, -0.1677, -0.1645, -0.1613, -0.1582, -0.1550, -0.1518, -0.1487,
        -0.1455, -0.1424, -0.1392, -0.1360, -0.1329, -0.1297, -0.1265, -0.1234,
        -0.1202, -0.1170, -0.1139, -0.1107, -0.1076, -0.1044, -0.1012, -0.0981,
        -0.0949, -0.0917, -0.0886, -0.0854, -0.0823, -0.0791, -0.0759, -0.0728,
        -0.0696, -0.0664, -0.0633, -0.0601, -0.0569, -0.0538, -0.0506, -0.0475,
        -0.0443, -0.0411, -0.0380, -0.0348, -0.0316, -0.0285, -0.0253, -0.0221,
        -0.0190, -0.0158, -0.0127, -0.0095, -0.0063, -0.0032, -0.0000,  0.0032,
         0.0063,  0.0095,  0.0127,  0.0158,  0.0190,  0.0221,  0.0253,  0.0285,
         0.0316,  0.0348,  0.0380,  0.0411,  0.0443,  0.0475,  0.0506,  0.0538,
         0.0569,  0.0601,  0.0633,  0.0664,  0.0696,  0.0728,  0.0759,  0.0791,
         0.0823,  0.0854,  0.0886,  0.0917,  0.0949,  0.0981,  0.1012,  0.1044,
         0.1076,  0.1107,  0.1139,  0.1170,  0.1202,  0.1234,  0.1265,  0.1297,
         0.1329,  0.1360,  0.1392,  0.1424,  0.1455,  0.1487,  0.1518,  0.1550,
         0.1582,  0.1613,  0.1645,  0.1677,  0.1708,  0.1740,  0.1772,  0.1803,
         0.1835,  0.1866,  0.1898,  0.1930,  0.1961,  0.2025,  0.2056,  0.2088,
         0.2120,  0.2151,  0.2183,  0.2214,  0.2246,  0.2309,  0.2341,  0.2373,
         0.2404,  0.2436,  0.2499,  0.2562,  0.2657,  0.2689,  0.2910,  0.2942,
         0.2974,  0.3005,  0.3163,  0.3195,  0.3290,  0.3448,  0.3480,  0.3575,
         0.3733,  0.3796,  0.3828], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  179
---------Q_out---------
tensor([[[ 0.8368,  0.7322,  1.2552,  ..., -0.4184,  0.1569, -0.2092],
         [ 0.2092,  0.3138,  1.5167,  ..., -0.2092, -1.2029, -0.1569],
         [ 0.9414,  0.9937,  0.9414,  ..., -0.5230, -1.0983,  0.0523],
         ...,
         [ 0.1569,  1.3075,  0.7322,  ...,  0.6276, -1.6737, -0.6276],
         [ 0.4707,  1.8829,  0.9937,  ..., -0.0000, -1.2029, -1.5167],
         [ 0.5230,  1.8829,  0.2615,  ..., -0.1569, -0.9414, -2.0398]],

        [[-0.1046,  0.6799,  1.1506,  ..., -0.3138,  0.4184, -0.2615],
         [ 0.4707,  2.4582,  1.3075,  ..., -0.9937, -1.2552, -0.1569],
         [ 0.3661,  2.0398,  0.8891,  ..., -0.4707, -1.7260,  0.0523],
         ...,
         [-0.6276, -1.3598,  1.3598,  ...,  0.3661, -0.0000, -0.3138],
         [-1.8829, -1.3075,  0.9937,  ...,  0.4707,  0.7845, -0.5230],
         [-0.6799, -1.0983,  0.3138,  ...,  0.5230, -0.2615, -1.2552]],

        [[ 0.6276,  0.3138,  0.9937,  ..., -0.9937, -0.2092,  0.2092],
         [ 0.1569,  1.3075,  0.4707,  ...,  1.1506,  0.0523,  0.3661],
         [-0.0523,  1.5167, -1.2029,  ...,  0.3138, -0.0000, -0.2615],
         ...,
         [-0.9414,  0.2615,  0.6276,  ..., -1.0460, -0.4184, -2.3013],
         [ 0.2092,  2.0398,  1.4121,  ..., -0.9414, -0.9937, -2.4582],
         [ 0.9414,  2.1444,  1.0983,  ..., -0.4707,  0.0523, -2.4059]],

        ...,

        [[ 0.2615,  0.5753,  1.3075,  ..., -1.0983,  0.1569, -0.0000],
         [-0.7322,  1.8306,  0.7845,  ..., -1.6214, -1.4121,  0.5230],
         [-0.1569,  0.5753, -0.8891,  ..., -1.2029, -0.8368,  0.4184],
         ...,
         [ 0.0000,  1.5167,  1.9875,  ..., -0.5753, -1.0460, -2.3013],
         [ 0.4184,  1.1506,  1.2552,  ..., -0.9937, -0.7845, -2.3013],
         [ 0.6276,  0.9414,  0.6276,  ..., -0.1569, -0.1569, -2.2490]],

        [[ 0.7845,  0.5230,  1.4644,  ..., -0.2092,  0.1046,  0.0523],
         [-0.4707,  2.5105, -0.4184,  ...,  0.1569, -1.4644,  0.6799],
         [-0.7845,  2.1967, -1.4121,  ...,  0.2092, -2.2490, -0.3661],
         ...,
         [ 1.0460,  2.0921,  0.9414,  ..., -0.4707, -1.4121, -3.1904],
         [ 1.1506,  2.3536,  0.5753,  ..., -0.5753, -0.8891, -3.2427],
         [ 0.3661,  1.3598,  0.2615,  ...,  0.0523,  0.3138, -2.0921]],

        [[ 1.0460,  0.2615,  1.2029,  ..., -0.6799,  0.2092, -0.5753],
         [-0.3661,  1.4644,  0.0523,  ...,  0.1569, -1.8306, -0.4184],
         [-0.3138,  1.7260, -0.8891,  ..., -0.1046, -0.6799, -0.3661],
         ...,
         [-0.3138,  1.2552,  1.4644,  ..., -0.1046, -1.8306, -2.0398],
         [ 0.1569,  0.6276,  0.8891,  ...,  0.7322, -0.1046, -2.3536],
         [ 1.2552,  1.4644,  1.1506,  ...,  0.4707, -1.2552, -1.7260]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-6.6423, -6.5377, -6.0147, -5.8578, -5.8055, -5.7532, -5.7009, -5.6486,
        -5.5963, -5.5440, -5.4917, -5.4394, -5.3871, -5.3348, -5.2825, -5.2302,
        -5.1779, -5.1256, -5.0733, -5.0210, -4.9687, -4.9164, -4.8640, -4.8117,
        -4.7594, -4.7071, -4.6548, -4.6025, -4.5502, -4.4979, -4.4456, -4.3933,
        -4.3410, -4.2887, -4.2364, -4.1841, -4.1318, -4.0795, -4.0272, -3.9749,
        -3.9226, -3.8703, -3.8180, -3.7657, -3.7134, -3.6611, -3.6088, -3.5565,
        -3.5042, -3.4519, -3.3996, -3.3473, -3.2950, -3.2427, -3.1904, -3.1381,
        -3.0858, -3.0335, -2.9812, -2.9289, -2.8766, -2.8243, -2.7720, -2.7197,
        -2.6674, -2.6151, -2.5628, -2.5105, -2.4582, -2.4059, -2.3536, -2.3013,
        -2.2490, -2.1967, -2.1444, -2.0921, -2.0398, -1.9875, -1.9352, -1.8829,
        -1.8306, -1.7783, -1.7260, -1.6737, -1.6214, -1.5690, -1.5167, -1.4644,
        -1.4121, -1.3598, -1.3075, -1.2552, -1.2029, -1.1506, -1.0983, -1.0460,
        -0.9937, -0.9414, -0.8891, -0.8368, -0.7845, -0.7322, -0.6799, -0.6276,
        -0.5753, -0.5230, -0.4707, -0.4184, -0.3661, -0.3138, -0.2615, -0.2092,
        -0.1569, -0.1046, -0.0523, -0.0000,  0.0523,  0.1046,  0.1569,  0.2092,
         0.2615,  0.3138,  0.3661,  0.4184,  0.4707,  0.5230,  0.5753,  0.6276,
         0.6799,  0.7322,  0.7845,  0.8368,  0.8891,  0.9414,  0.9937,  1.0460,
         1.0983,  1.1506,  1.2029,  1.2552,  1.3075,  1.3598,  1.4121,  1.4644,
         1.5167,  1.5690,  1.6214,  1.6737,  1.7260,  1.7783,  1.8306,  1.8829,
         1.9352,  1.9875,  2.0398,  2.0921,  2.1444,  2.1967,  2.2490,  2.3013,
         2.3536,  2.4059,  2.4582,  2.5105,  2.5628,  2.6151,  2.6674,  2.7197,
         2.7720,  2.8243,  2.8766,  2.9289,  2.9812,  3.0335,  3.0858,  3.1381,
         3.1904,  3.2427,  3.2950,  3.3473,  3.3996,  3.4519,  3.5042,  3.5565,
         3.6088,  3.6611,  3.7134,  3.7657,  3.8180,  3.8703,  3.9226,  3.9749,
         4.0272,  4.0795,  4.1318,  4.1841,  4.2364,  4.2887,  4.3410,  4.3933,
         4.4456,  4.4979,  4.5502,  4.6025,  4.6548,  4.7071,  4.7594,  4.8117,
         4.8640,  4.9164,  4.9687,  5.0210,  5.0733,  5.1256,  5.1779,  5.2302,
         5.2825,  5.3348,  5.3871,  5.4394,  5.4917,  5.5440,  5.5963,  5.6486,
         5.7009,  5.7532,  5.8055,  5.8578,  5.9101,  5.9624,  6.0147,  6.1193,
         6.1716,  6.2762,  6.3285,  6.4331,  6.4854,  6.5900,  6.6423],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  239
name:  bert.encoder.layer.8.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0074, -0.0223, -0.0243,  ..., -0.0689, -0.0164, -0.0234],
        [-0.0179, -0.0068, -0.0102,  ...,  0.0199,  0.0064,  0.0335],
        [ 0.0011,  0.0470,  0.0619,  ..., -0.0513,  0.0904,  0.0141],
        ...,
        [-0.0732,  0.0115,  0.0016,  ...,  0.0591, -0.0015, -0.0481],
        [ 0.0284,  0.0189, -0.0158,  ...,  0.0215,  0.0163, -0.0160],
        [-0.0028, -0.0418, -0.0524,  ..., -0.0188, -0.0650, -0.0031]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0074, -0.0221, -0.0236,  ..., -0.0693, -0.0162, -0.0236],
        [-0.0177, -0.0074, -0.0103,  ...,  0.0192,  0.0059,  0.0339],
        [ 0.0015,  0.0472,  0.0619,  ..., -0.0516,  0.0899,  0.0147],
        ...,
        [-0.0737,  0.0118,  0.0015,  ...,  0.0590, -0.0015, -0.0486],
        [ 0.0280,  0.0192, -0.0162,  ...,  0.0221,  0.0162, -0.0162],
        [-0.0029, -0.0413, -0.0531,  ..., -0.0192, -0.0649, -0.0029]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.1872, -0.1798, -0.1769, -0.1754, -0.1739, -0.1710, -0.1695, -0.1680,
        -0.1666, -0.1651, -0.1636, -0.1592, -0.1563, -0.1548, -0.1533, -0.1518,
        -0.1504, -0.1489, -0.1474, -0.1459, -0.1445, -0.1430, -0.1415, -0.1400,
        -0.1386, -0.1371, -0.1356, -0.1341, -0.1327, -0.1312, -0.1297, -0.1282,
        -0.1268, -0.1253, -0.1238, -0.1224, -0.1209, -0.1194, -0.1179, -0.1165,
        -0.1150, -0.1135, -0.1120, -0.1106, -0.1091, -0.1076, -0.1061, -0.1047,
        -0.1032, -0.1017, -0.1002, -0.0988, -0.0973, -0.0958, -0.0943, -0.0929,
        -0.0914, -0.0899, -0.0884, -0.0870, -0.0855, -0.0840, -0.0826, -0.0811,
        -0.0796, -0.0781, -0.0767, -0.0752, -0.0737, -0.0722, -0.0708, -0.0693,
        -0.0678, -0.0663, -0.0649, -0.0634, -0.0619, -0.0604, -0.0590, -0.0575,
        -0.0560, -0.0545, -0.0531, -0.0516, -0.0501, -0.0486, -0.0472, -0.0457,
        -0.0442, -0.0427, -0.0413, -0.0398, -0.0383, -0.0369, -0.0354, -0.0339,
        -0.0324, -0.0310, -0.0295, -0.0280, -0.0265, -0.0251, -0.0236, -0.0221,
        -0.0206, -0.0192, -0.0177, -0.0162, -0.0147, -0.0133, -0.0118, -0.0103,
        -0.0088, -0.0074, -0.0059, -0.0044, -0.0029, -0.0015, -0.0000,  0.0015,
         0.0029,  0.0044,  0.0059,  0.0074,  0.0088,  0.0103,  0.0118,  0.0133,
         0.0147,  0.0162,  0.0177,  0.0192,  0.0206,  0.0221,  0.0236,  0.0251,
         0.0265,  0.0280,  0.0295,  0.0310,  0.0324,  0.0339,  0.0354,  0.0369,
         0.0383,  0.0398,  0.0413,  0.0427,  0.0442,  0.0457,  0.0472,  0.0486,
         0.0501,  0.0516,  0.0531,  0.0545,  0.0560,  0.0575,  0.0590,  0.0604,
         0.0619,  0.0634,  0.0649,  0.0663,  0.0678,  0.0693,  0.0708,  0.0722,
         0.0737,  0.0752,  0.0767,  0.0781,  0.0796,  0.0811,  0.0826,  0.0840,
         0.0855,  0.0870,  0.0884,  0.0899,  0.0914,  0.0929,  0.0943,  0.0958,
         0.0973,  0.0988,  0.1002,  0.1017,  0.1032,  0.1047,  0.1061,  0.1076,
         0.1091,  0.1106,  0.1120,  0.1135,  0.1150,  0.1165,  0.1179,  0.1194,
         0.1209,  0.1224,  0.1238,  0.1253,  0.1268,  0.1282,  0.1297,  0.1312,
         0.1327,  0.1341,  0.1356,  0.1371,  0.1386,  0.1400,  0.1415,  0.1430,
         0.1445,  0.1459,  0.1474,  0.1489,  0.1504,  0.1518,  0.1533,  0.1548,
         0.1563,  0.1577,  0.1592,  0.1607,  0.1636,  0.1651,  0.1666,  0.1680,
         0.1695,  0.1710,  0.1725,  0.1784], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  236
---------Q_out---------
None
name:  bert.encoder.layer.8.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0398, -0.0240,  0.0144,  ..., -0.0430,  0.0208, -0.0109],
        [ 0.0531,  0.0099, -0.0092,  ..., -0.0103, -0.0166, -0.0250],
        [-0.0131, -0.0447, -0.0696,  ..., -0.0032, -0.0191, -0.0316],
        ...,
        [-0.0361,  0.0352, -0.0671,  ...,  0.0118, -0.0047,  0.0088],
        [ 0.0359,  0.0202,  0.0098,  ..., -0.0421,  0.0271,  0.0254],
        [ 0.0142,  0.0112, -0.0126,  ...,  0.0123,  0.0119,  0.0475]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0384, -0.0256,  0.0128,  ..., -0.0427,  0.0213, -0.0128],
        [ 0.0512,  0.0085, -0.0085,  ..., -0.0085, -0.0171, -0.0256],
        [-0.0128, -0.0427, -0.0683,  ..., -0.0043, -0.0171, -0.0299],
        ...,
        [-0.0342,  0.0342, -0.0683,  ...,  0.0128, -0.0043,  0.0085],
        [ 0.0342,  0.0213,  0.0085,  ..., -0.0427,  0.0256,  0.0256],
        [ 0.0128,  0.0128, -0.0128,  ...,  0.0128,  0.0128,  0.0470]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.5422, -0.5294, -0.4654, -0.4312, -0.3672, -0.3586, -0.3458, -0.3159,
        -0.3031, -0.2903, -0.2860, -0.2775, -0.2434, -0.2263, -0.2135, -0.2049,
        -0.2007, -0.1921, -0.1836, -0.1793, -0.1750, -0.1708, -0.1665, -0.1622,
        -0.1580, -0.1537, -0.1494, -0.1452, -0.1409, -0.1366, -0.1324, -0.1281,
        -0.1238, -0.1195, -0.1153, -0.1110, -0.1067, -0.1025, -0.0982, -0.0939,
        -0.0897, -0.0854, -0.0811, -0.0768, -0.0726, -0.0683, -0.0640, -0.0598,
        -0.0555, -0.0512, -0.0470, -0.0427, -0.0384, -0.0342, -0.0299, -0.0256,
        -0.0213, -0.0171, -0.0128, -0.0085, -0.0043, -0.0000,  0.0043,  0.0085,
         0.0128,  0.0171,  0.0213,  0.0256,  0.0299,  0.0342,  0.0384,  0.0427,
         0.0470,  0.0512,  0.0555,  0.0598,  0.0640,  0.0683,  0.0726,  0.0768,
         0.0811,  0.0854,  0.0897,  0.0939,  0.0982,  0.1025,  0.1067,  0.1110,
         0.1153,  0.1195,  0.1238,  0.1281,  0.1324,  0.1366,  0.1409,  0.1452,
         0.1494,  0.1537,  0.1580,  0.1622,  0.1665,  0.1708,  0.1750,  0.1793,
         0.1836,  0.1921,  0.2007,  0.2049,  0.2092,  0.2263,  0.2348,  0.2391,
         0.2434,  0.2519,  0.2604,  0.2690,  0.2775,  0.2946,  0.2989,  0.3245,
         0.3544,  0.3800,  0.3885,  0.4099], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  124
---------Q_out---------
None
name:  bert.encoder.layer.8.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0197,  0.0377, -0.0377,  ...,  0.0438, -0.0122,  0.0722],
        [-0.0149, -0.0122, -0.0228,  ...,  0.0224,  0.0645,  0.0213],
        [-0.0230,  0.0389, -0.0498,  ..., -0.0071, -0.0313, -0.0273],
        ...,
        [ 0.0196,  0.0256,  0.0162,  ..., -0.0099,  0.0622,  0.0040],
        [ 0.0225,  0.0927, -0.0034,  ...,  0.0421, -0.0129,  0.0253],
        [-0.0048, -0.0317,  0.0238,  ..., -0.0478,  0.0726,  0.0032]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0205,  0.0376, -0.0376,  ...,  0.0444, -0.0137,  0.0717],
        [-0.0137, -0.0137, -0.0239,  ...,  0.0239,  0.0649,  0.0205],
        [-0.0239,  0.0376, -0.0512,  ..., -0.0068, -0.0307, -0.0273],
        ...,
        [ 0.0205,  0.0239,  0.0171,  ..., -0.0102,  0.0615,  0.0034],
        [ 0.0239,  0.0922, -0.0034,  ...,  0.0410, -0.0137,  0.0239],
        [-0.0034, -0.0307,  0.0239,  ..., -0.0478,  0.0717,  0.0034]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.4336, -0.4097, -0.3551, -0.3517, -0.3448, -0.3312, -0.3209, -0.3175,
        -0.3039, -0.2936, -0.2834, -0.2766, -0.2731, -0.2663, -0.2595, -0.2527,
        -0.2424, -0.2390, -0.2356, -0.2322, -0.2288, -0.2253, -0.2219, -0.2185,
        -0.2151, -0.2117, -0.2083, -0.2049, -0.2014, -0.1946, -0.1912, -0.1878,
        -0.1844, -0.1810, -0.1775, -0.1741, -0.1707, -0.1673, -0.1639, -0.1605,
        -0.1571, -0.1536, -0.1502, -0.1468, -0.1434, -0.1400, -0.1366, -0.1332,
        -0.1297, -0.1263, -0.1229, -0.1195, -0.1161, -0.1127, -0.1093, -0.1058,
        -0.1024, -0.0990, -0.0956, -0.0922, -0.0888, -0.0854, -0.0819, -0.0785,
        -0.0751, -0.0717, -0.0683, -0.0649, -0.0615, -0.0580, -0.0546, -0.0512,
        -0.0478, -0.0444, -0.0410, -0.0376, -0.0341, -0.0307, -0.0273, -0.0239,
        -0.0205, -0.0171, -0.0137, -0.0102, -0.0068, -0.0034, -0.0000,  0.0034,
         0.0068,  0.0102,  0.0137,  0.0171,  0.0205,  0.0239,  0.0273,  0.0307,
         0.0341,  0.0376,  0.0410,  0.0444,  0.0478,  0.0512,  0.0546,  0.0580,
         0.0615,  0.0649,  0.0683,  0.0717,  0.0751,  0.0785,  0.0819,  0.0854,
         0.0888,  0.0922,  0.0956,  0.0990,  0.1024,  0.1058,  0.1093,  0.1127,
         0.1161,  0.1195,  0.1229,  0.1263,  0.1297,  0.1332,  0.1366,  0.1400,
         0.1434,  0.1468,  0.1502,  0.1536,  0.1571,  0.1605,  0.1639,  0.1673,
         0.1707,  0.1741,  0.1775,  0.1810,  0.1844,  0.1878,  0.1912,  0.1946,
         0.1980,  0.2014,  0.2049,  0.2083,  0.2117,  0.2151,  0.2185,  0.2219,
         0.2253,  0.2288,  0.2356,  0.2390,  0.2424,  0.2458,  0.2492,  0.2561,
         0.2595,  0.2629,  0.2663,  0.2902,  0.2970,  0.3141,  0.3244,  0.3448,
         0.3619,  0.3756], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  170
---------Q_out---------
None
name:  bert.encoder.layer.8.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0469,  0.0276, -0.0291,  ..., -0.0074,  0.0206, -0.0081],
        [-0.0268,  0.0292, -0.0308,  ...,  0.0271,  0.0170,  0.0190],
        [ 0.0299,  0.0132,  0.0517,  ...,  0.0265,  0.0246,  0.0652],
        ...,
        [-0.0070,  0.0759, -0.0073,  ...,  0.0424, -0.0310,  0.0178],
        [-0.0297, -0.0164, -0.0149,  ...,  0.0421, -0.0585, -0.0531],
        [-0.0312,  0.0060,  0.0232,  ..., -0.0405,  0.0460,  0.0124]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0494,  0.0247, -0.0247,  ..., -0.0000,  0.0247, -0.0000],
        [-0.0247,  0.0247, -0.0247,  ...,  0.0247,  0.0247,  0.0247],
        [ 0.0247,  0.0247,  0.0494,  ...,  0.0247,  0.0247,  0.0741],
        ...,
        [-0.0000,  0.0741, -0.0000,  ...,  0.0494, -0.0247,  0.0247],
        [-0.0247, -0.0247, -0.0247,  ...,  0.0494, -0.0494, -0.0494],
        [-0.0247,  0.0000,  0.0247,  ..., -0.0494,  0.0494,  0.0247]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-3.1352, -2.5674, -1.8268, -1.7527, -1.2837, -0.9628, -0.7900, -0.7653,
        -0.6912, -0.6172, -0.5925, -0.5678, -0.5431, -0.5184, -0.4937, -0.4690,
        -0.4444, -0.4197, -0.3950, -0.3703, -0.3456, -0.3209, -0.2962, -0.2716,
        -0.2469, -0.2222, -0.1975, -0.1728, -0.1481, -0.1234, -0.0987, -0.0741,
        -0.0494, -0.0247, -0.0000,  0.0247,  0.0494,  0.0741,  0.0987,  0.1234,
         0.1481,  0.1728,  0.1975,  0.2222,  0.2469,  0.2716,  0.3209,  0.3703,
         0.3950,  0.4444,  0.4937,  0.5184,  0.5431,  0.5925,  0.6172,  0.7406,
         1.1109], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  57
---------Q_out---------
None
name:  bert.encoder.layer.9.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0800,  0.0056, -0.0041,  ..., -0.0355, -0.0039,  0.0070],
        [-0.0143, -0.0532, -0.0612,  ..., -0.0049, -0.0642,  0.0032],
        [-0.0423,  0.0086, -0.0153,  ..., -0.0500,  0.0146,  0.0230],
        ...,
        [-0.0032, -0.0187,  0.0253,  ..., -0.0898, -0.0988,  0.0353],
        [ 0.0278,  0.0577, -0.0265,  ...,  0.0038,  0.0110, -0.0411],
        [-0.0709,  0.0045,  0.0178,  ..., -0.0139, -0.0295, -0.0327]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0806,  0.0064, -0.0042,  ..., -0.0361, -0.0042,  0.0064],
        [-0.0148, -0.0530, -0.0615,  ..., -0.0042, -0.0636,  0.0042],
        [-0.0424,  0.0085, -0.0148,  ..., -0.0509,  0.0148,  0.0233],
        ...,
        [-0.0042, -0.0191,  0.0254,  ..., -0.0891, -0.0997,  0.0361],
        [ 0.0276,  0.0573, -0.0254,  ...,  0.0042,  0.0106, -0.0403],
        [-0.0700,  0.0042,  0.0170,  ..., -0.0148, -0.0297, -0.0318]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2460, -0.2418, -0.2333, -0.2311, -0.2290, -0.2142, -0.2121, -0.2099,
        -0.2078, -0.2057, -0.2015, -0.1993, -0.1951, -0.1930, -0.1909, -0.1887,
        -0.1866, -0.1845, -0.1824, -0.1803, -0.1781, -0.1760, -0.1739, -0.1718,
        -0.1697, -0.1675, -0.1654, -0.1633, -0.1612, -0.1590, -0.1569, -0.1548,
        -0.1527, -0.1506, -0.1484, -0.1463, -0.1442, -0.1421, -0.1400, -0.1378,
        -0.1357, -0.1336, -0.1315, -0.1294, -0.1272, -0.1251, -0.1230, -0.1209,
        -0.1188, -0.1166, -0.1145, -0.1124, -0.1103, -0.1082, -0.1060, -0.1039,
        -0.1018, -0.0997, -0.0975, -0.0954, -0.0933, -0.0912, -0.0891, -0.0869,
        -0.0848, -0.0827, -0.0806, -0.0785, -0.0763, -0.0742, -0.0721, -0.0700,
        -0.0679, -0.0657, -0.0636, -0.0615, -0.0594, -0.0573, -0.0551, -0.0530,
        -0.0509, -0.0488, -0.0467, -0.0445, -0.0424, -0.0403, -0.0382, -0.0361,
        -0.0339, -0.0318, -0.0297, -0.0276, -0.0254, -0.0233, -0.0212, -0.0191,
        -0.0170, -0.0148, -0.0127, -0.0106, -0.0085, -0.0064, -0.0042, -0.0021,
        -0.0000,  0.0021,  0.0042,  0.0064,  0.0085,  0.0106,  0.0127,  0.0148,
         0.0170,  0.0191,  0.0212,  0.0233,  0.0254,  0.0276,  0.0297,  0.0318,
         0.0339,  0.0361,  0.0382,  0.0403,  0.0424,  0.0445,  0.0467,  0.0488,
         0.0509,  0.0530,  0.0551,  0.0573,  0.0594,  0.0615,  0.0636,  0.0657,
         0.0679,  0.0700,  0.0721,  0.0742,  0.0763,  0.0785,  0.0806,  0.0827,
         0.0848,  0.0869,  0.0891,  0.0912,  0.0933,  0.0954,  0.0975,  0.0997,
         0.1018,  0.1039,  0.1060,  0.1082,  0.1103,  0.1124,  0.1145,  0.1166,
         0.1188,  0.1209,  0.1230,  0.1251,  0.1272,  0.1294,  0.1315,  0.1336,
         0.1357,  0.1378,  0.1400,  0.1421,  0.1442,  0.1463,  0.1484,  0.1506,
         0.1527,  0.1548,  0.1569,  0.1590,  0.1612,  0.1633,  0.1654,  0.1675,
         0.1697,  0.1718,  0.1739,  0.1760,  0.1781,  0.1803,  0.1824,  0.1845,
         0.1866,  0.1887,  0.1909,  0.1930,  0.1951,  0.1972,  0.1993,  0.2015,
         0.2057,  0.2099,  0.2184,  0.2205,  0.2290,  0.2333,  0.2481,  0.2524,
         0.2545,  0.2693], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  210
---------Q_out---------
tensor([[[-0.1917,  0.8308,  0.2556,  ..., -0.8947,  0.4474,  0.7030],
         [-0.5113,  2.1090,  0.4474,  ..., -0.7030,  0.1917, -0.4474],
         [ 0.9586,  1.8534,  0.7030,  ...,  0.1917, -0.3835,  0.1278],
         ...,
         [ 0.6391, -0.9586,  0.7030,  ..., -0.3195, -0.7030,  1.3421],
         [-0.2556, -0.7669,  0.3835,  ..., -0.6391, -0.4474,  1.0865],
         [ 0.5113, -1.4060, -0.6391,  ..., -0.8947, -0.7669,  0.3835]],

        [[ 0.0000,  0.7669, -0.1917,  ..., -0.4474,  0.3195,  1.3421],
         [ 1.6617,  0.9586,  0.0639,  ...,  0.0639,  0.3835, -0.9586],
         [ 0.3835,  0.1917,  0.3195,  ...,  0.0639,  0.5752, -0.1917],
         ...,
         [-0.0639,  0.8308, -0.3835,  ..., -0.6391, -0.4474,  0.1278],
         [-0.4474,  0.9586,  0.6391,  ..., -0.7030, -0.5113,  0.8947],
         [-0.6391,  1.0226, -1.4699,  ..., -0.2556,  0.2556,  0.2556]],

        [[-0.3835,  0.8308, -0.3195,  ..., -1.0865,  0.8308,  1.3421],
         [-0.6391,  1.9173,  0.7030,  ..., -0.3195,  0.6391,  0.8308],
         [-0.5752,  2.2368,  1.2782,  ...,  0.7030,  0.7030,  0.8308],
         ...,
         [-0.3195, -0.8947,  0.8308,  ..., -1.5977, -0.3195,  1.0226],
         [-0.5113, -1.1504, -1.0226,  ..., -0.1917, -0.9586,  0.5752],
         [-0.5113, -1.0865, -1.1504,  ..., -0.6391,  0.0000, -0.2556]],

        ...,

        [[ 0.0639,  1.3421,  0.3835,  ..., -1.1504,  0.5113,  0.8947],
         [-0.0639,  1.2143, -0.8947,  ..., -2.4286,  1.1504,  1.2143],
         [ 0.6391,  0.5752, -0.8308,  ..., -1.2782,  0.9586,  1.5977],
         ...,
         [ 0.1917, -1.0865,  0.4474,  ..., -0.7669, -0.4474,  1.1504],
         [ 0.2556, -0.8308,  0.1278,  ..., -1.0865, -0.7030,  0.5752],
         [ 0.3835, -0.4474, -0.0639,  ..., -0.6391, -0.9586,  0.6391]],

        [[-0.3195,  1.1504, -0.1278,  ..., -1.0865,  0.6391,  0.7669],
         [ 1.8534, -0.5752,  0.3195,  ...,  0.1278,  1.0865, -0.3195],
         [ 2.1090, -0.9586, -0.7030,  ...,  1.9173,  0.5752, -0.4474],
         ...,
         [ 0.0639, -0.2556,  0.0639,  ..., -0.6391, -0.1278,  0.5113],
         [ 0.3835,  0.1278,  0.7669,  ..., -0.3835, -0.5113,  0.0639],
         [ 0.3835, -1.9173,  0.6391,  ..., -1.0865, -0.7030,  1.0865]],

        [[-0.1917,  1.2143,  0.0000,  ..., -0.5752,  0.5752,  1.4060],
         [ 1.0865,  0.9586, -2.1090,  ..., -0.1278,  2.4925, -0.1917],
         [-0.6391,  0.3835, -0.5113,  ...,  0.1278,  0.3835,  0.3835],
         ...,
         [ 0.4474, -0.3195, -1.6617,  ..., -0.2556, -0.9586,  0.2556],
         [ 1.0865, -0.8308, -0.5752,  ..., -1.0865, -1.5977,  1.4060],
         [ 0.1917, -1.0226, -0.6391,  ..., -0.8308, -1.4699,  0.7030]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-8.0526, -7.9248, -7.6052, -7.4135, -7.2857, -7.2218, -7.1579, -7.0301,
        -6.9661, -6.9022, -6.8383, -6.7744, -6.7105, -6.6466, -6.5827, -6.5188,
        -6.4549, -6.3910, -6.3271, -6.2631, -6.1992, -6.1353, -6.0714, -6.0075,
        -5.9436, -5.8797, -5.8158, -5.7519, -5.6880, -5.6240, -5.5601, -5.4962,
        -5.4323, -5.3684, -5.3045, -5.2406, -5.1767, -5.1128, -5.0489, -4.9850,
        -4.9210, -4.8571, -4.7932, -4.7293, -4.6654, -4.6015, -4.5376, -4.4737,
        -4.4098, -4.3459, -4.2819, -4.2180, -4.1541, -4.0902, -4.0263, -3.9624,
        -3.8985, -3.8346, -3.7707, -3.7068, -3.6428, -3.5789, -3.5150, -3.4511,
        -3.3872, -3.3233, -3.2594, -3.1955, -3.1316, -3.0677, -3.0038, -2.9398,
        -2.8759, -2.8120, -2.7481, -2.6842, -2.6203, -2.5564, -2.4925, -2.4286,
        -2.3647, -2.3007, -2.2368, -2.1729, -2.1090, -2.0451, -1.9812, -1.9173,
        -1.8534, -1.7895, -1.7256, -1.6617, -1.5977, -1.5338, -1.4699, -1.4060,
        -1.3421, -1.2782, -1.2143, -1.1504, -1.0865, -1.0226, -0.9586, -0.8947,
        -0.8308, -0.7669, -0.7030, -0.6391, -0.5752, -0.5113, -0.4474, -0.3835,
        -0.3195, -0.2556, -0.1917, -0.1278, -0.0639, -0.0000,  0.0639,  0.1278,
         0.1917,  0.2556,  0.3195,  0.3835,  0.4474,  0.5113,  0.5752,  0.6391,
         0.7030,  0.7669,  0.8308,  0.8947,  0.9586,  1.0226,  1.0865,  1.1504,
         1.2143,  1.2782,  1.3421,  1.4060,  1.4699,  1.5338,  1.5977,  1.6617,
         1.7256,  1.7895,  1.8534,  1.9173,  1.9812,  2.0451,  2.1090,  2.1729,
         2.2368,  2.3007,  2.3647,  2.4286,  2.4925,  2.5564,  2.6203,  2.6842,
         2.7481,  2.8120,  2.8759,  2.9398,  3.0038,  3.0677,  3.1316,  3.1955,
         3.2594,  3.3233,  3.3872,  3.4511,  3.5150,  3.5789,  3.6428,  3.7068,
         3.7707,  3.8346,  3.8985,  3.9624,  4.0263,  4.0902,  4.1541,  4.2180,
         4.2819,  4.3459,  4.4098,  4.4737,  4.5376,  4.6015,  4.6654,  4.7293,
         4.7932,  4.8571,  4.9210,  4.9850,  5.0489,  5.1128,  5.1767,  5.2406,
         5.3045,  5.3684,  5.4323,  5.4962,  5.5601,  5.6240,  5.6880,  5.7519,
         5.8158,  5.8797,  5.9436,  6.0075,  6.0714,  6.1353,  6.1992,  6.2631,
         6.3271,  6.3910,  6.4549,  6.5827,  6.6466,  6.7105,  6.7744,  6.9661,
         7.0940,  7.2857], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  226
name:  bert.encoder.layer.9.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0135,  0.0075, -0.0528,  ...,  0.0408,  0.0476,  0.0062],
        [ 0.0062, -0.0475, -0.0587,  ...,  0.0381, -0.0663,  0.0154],
        [-0.0455, -0.0293,  0.0124,  ..., -0.0534, -0.0399, -0.0444],
        ...,
        [-0.0420,  0.0135, -0.0005,  ...,  0.0381,  0.1146, -0.0225],
        [-0.0431,  0.0020, -0.0378,  ...,  0.0292, -0.0587, -0.0411],
        [ 0.0316,  0.0288, -0.0461,  ...,  0.0061,  0.0167, -0.0081]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0148,  0.0099, -0.0543,  ...,  0.0395,  0.0494,  0.0049],
        [ 0.0049, -0.0494, -0.0592,  ...,  0.0395, -0.0642,  0.0148],
        [-0.0444, -0.0296,  0.0148,  ..., -0.0543, -0.0395, -0.0444],
        ...,
        [-0.0444,  0.0148, -0.0000,  ...,  0.0395,  0.1135, -0.0247],
        [-0.0444,  0.0000, -0.0395,  ...,  0.0296, -0.0592, -0.0395],
        [ 0.0296,  0.0296, -0.0444,  ...,  0.0049,  0.0148, -0.0099]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.5281, -0.5035, -0.4590, -0.4097, -0.3998, -0.3702, -0.3653, -0.3554,
        -0.3505, -0.3307, -0.3159, -0.3110, -0.3011, -0.2962, -0.2813, -0.2764,
        -0.2715, -0.2665, -0.2616, -0.2567, -0.2517, -0.2468, -0.2419, -0.2369,
        -0.2271, -0.2221, -0.2172, -0.2122, -0.2073, -0.2024, -0.1974, -0.1925,
        -0.1876, -0.1826, -0.1777, -0.1728, -0.1678, -0.1629, -0.1579, -0.1530,
        -0.1481, -0.1431, -0.1382, -0.1333, -0.1283, -0.1234, -0.1185, -0.1135,
        -0.1086, -0.1037, -0.0987, -0.0938, -0.0888, -0.0839, -0.0790, -0.0740,
        -0.0691, -0.0642, -0.0592, -0.0543, -0.0494, -0.0444, -0.0395, -0.0346,
        -0.0296, -0.0247, -0.0197, -0.0148, -0.0099, -0.0049, -0.0000,  0.0049,
         0.0099,  0.0148,  0.0197,  0.0247,  0.0296,  0.0346,  0.0395,  0.0444,
         0.0494,  0.0543,  0.0592,  0.0642,  0.0691,  0.0740,  0.0790,  0.0839,
         0.0888,  0.0938,  0.0987,  0.1037,  0.1086,  0.1135,  0.1185,  0.1234,
         0.1283,  0.1333,  0.1382,  0.1431,  0.1481,  0.1530,  0.1579,  0.1629,
         0.1678,  0.1728,  0.1777,  0.1826,  0.1876,  0.1925,  0.1974,  0.2024,
         0.2073,  0.2122,  0.2172,  0.2221,  0.2271,  0.2320,  0.2369,  0.2419,
         0.2468,  0.2567,  0.2616,  0.2764,  0.2813,  0.2863,  0.2912,  0.2962,
         0.3110,  0.3208,  0.3258,  0.3356,  0.3406,  0.3554,  0.3603,  0.3702,
         0.3751,  0.3850,  0.3899,  0.4245,  0.4492,  0.6269], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  142
---------Q_out---------
tensor([[[-0.5432, -0.3622,  0.9054,  ..., -0.9658, -0.1207,  1.6297],
         [ 0.7847,  1.3279,  0.8450,  ..., -0.1811, -1.0865, -0.2414],
         [-0.2414,  0.1811,  2.7766,  ...,  0.2414,  0.1207, -0.1207],
         ...,
         [-0.1207,  0.7243,  1.8108,  ...,  0.2414,  0.0604, -1.1468],
         [ 0.3622,  0.6640,  2.2937,  ..., -0.0604, -0.5432,  0.0604],
         [ 0.3622,  0.0604,  1.5694,  ...,  0.5432,  0.0604, -0.1207]],

        [[-0.8450,  0.3622,  0.3622,  ..., -1.6901,  0.5432,  1.5090],
         [ 0.5432, -0.9054,  0.3622,  ..., -1.8108, -0.4829,  2.2333],
         [ 0.1207, -1.3279,  0.2414,  ..., -1.5090, -0.7847,  0.7243],
         ...,
         [ 1.0261,  2.3540,  1.0865,  ..., -1.6901, -0.3018, -0.2414],
         [-0.0000,  2.5351,  1.3279,  ..., -1.0865,  0.3622, -0.6640],
         [-1.0261,  1.7504, -0.1811,  ..., -0.4829,  0.2414,  0.3018]],

        [[-0.6640,  0.0604,  0.2414,  ..., -1.0865, -0.7847,  0.9054],
         [ 0.1207,  1.8108, -0.0604,  ..., -1.3883, -2.0522, -0.9054],
         [-1.8712,  1.0261,  0.8450,  ..., -0.7243, -0.9658, -1.3883],
         ...,
         [ 0.3622,  0.9658,  1.9919,  ...,  0.9054, -0.6640,  0.1811],
         [-0.0000,  0.0604,  2.0522,  ..., -0.1811,  0.0000, -0.5432],
         [ 0.7243,  0.3018,  2.4144,  ..., -0.0604, -0.1207,  0.0000]],

        ...,

        [[-0.1811,  0.6640,  0.5432,  ..., -0.4829, -0.0000,  1.2072],
         [-0.6640, -0.7243, -0.3018,  ...,  0.7847,  0.4829, -0.3622],
         [-1.8108, -1.1468, -1.5694,  ..., -0.3018,  0.2414, -1.0865],
         ...,
         [ 0.9054,  0.5432,  2.1730,  ..., -0.1811, -0.6640, -0.4829],
         [ 1.3279, -0.1207,  1.8712,  ..., -0.0604, -0.3018, -0.1207],
         [ 0.2414,  0.2414,  1.8712,  ..., -0.0000,  0.4829,  0.1207]],

        [[-0.5432,  0.3622,  0.0604,  ..., -0.6036, -1.2072,  1.2676],
         [ 1.0865, -1.4486, -0.7243,  ..., -0.0604, -0.3018,  1.8712],
         [ 1.2072, -0.5432, -0.1811,  ..., -0.6640, -0.6036,  0.1811],
         ...,
         [ 1.0261, -0.3018,  2.3540,  ..., -0.0604, -2.5955, -0.6036],
         [ 0.8450, -0.1207,  2.1126,  ..., -0.1811, -2.1126,  0.2414],
         [ 1.2072,  0.7847,  2.0522,  ...,  1.2676, -0.5432, -0.3018]],

        [[-0.1811,  0.1811, -0.4225,  ..., -1.8108, -0.1207,  0.6640],
         [ 0.4225, -0.2414, -0.3622,  ..., -0.9658, -0.0604,  1.0865],
         [-0.6036,  0.3622, -0.3622,  ..., -0.8450, -0.7847,  1.8108],
         ...,
         [-0.4225,  0.1811,  1.4486,  ...,  0.0000, -0.4225, -0.5432],
         [ 1.6901, -0.5432,  1.9919,  ...,  0.6640, -0.6036, -0.3622],
         [-0.1811, -0.4225,  1.3279,  ...,  0.8450, -0.0000, -0.0604]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-6.6396, -6.4585, -6.0964, -5.9757, -5.8549, -5.7342, -5.6135, -5.5531,
        -5.4928, -5.4324, -5.3720, -5.3117, -5.2513, -5.1910, -5.1306, -5.0702,
        -5.0099, -4.9495, -4.8892, -4.8288, -4.7684, -4.7081, -4.6477, -4.5874,
        -4.5270, -4.4666, -4.4063, -4.3459, -4.2856, -4.2252, -4.1648, -4.1045,
        -4.0441, -3.9838, -3.9234, -3.8630, -3.8027, -3.7423, -3.6820, -3.6216,
        -3.5612, -3.5009, -3.4405, -3.3802, -3.3198, -3.2594, -3.1991, -3.1387,
        -3.0784, -3.0180, -2.9576, -2.8973, -2.8369, -2.7766, -2.7162, -2.6558,
        -2.5955, -2.5351, -2.4748, -2.4144, -2.3540, -2.2937, -2.2333, -2.1730,
        -2.1126, -2.0522, -1.9919, -1.9315, -1.8712, -1.8108, -1.7504, -1.6901,
        -1.6297, -1.5694, -1.5090, -1.4486, -1.3883, -1.3279, -1.2676, -1.2072,
        -1.1468, -1.0865, -1.0261, -0.9658, -0.9054, -0.8450, -0.7847, -0.7243,
        -0.6640, -0.6036, -0.5432, -0.4829, -0.4225, -0.3622, -0.3018, -0.2414,
        -0.1811, -0.1207, -0.0604, -0.0000,  0.0604,  0.1207,  0.1811,  0.2414,
         0.3018,  0.3622,  0.4225,  0.4829,  0.5432,  0.6036,  0.6640,  0.7243,
         0.7847,  0.8450,  0.9054,  0.9658,  1.0261,  1.0865,  1.1468,  1.2072,
         1.2676,  1.3279,  1.3883,  1.4486,  1.5090,  1.5694,  1.6297,  1.6901,
         1.7504,  1.8108,  1.8712,  1.9315,  1.9919,  2.0522,  2.1126,  2.1730,
         2.2333,  2.2937,  2.3540,  2.4144,  2.4748,  2.5351,  2.5955,  2.6558,
         2.7162,  2.7766,  2.8369,  2.8973,  2.9576,  3.0180,  3.0784,  3.1387,
         3.1991,  3.2594,  3.3198,  3.3802,  3.4405,  3.5009,  3.5612,  3.6216,
         3.6820,  3.7423,  3.8027,  3.8630,  3.9234,  3.9838,  4.0441,  4.1045,
         4.1648,  4.2252,  4.2856,  4.3459,  4.4063,  4.4666,  4.5270,  4.5874,
         4.6477,  4.7081,  4.7684,  4.8288,  4.8892,  4.9495,  5.0099,  5.0702,
         5.1306,  5.1910,  5.2513,  5.3117,  5.3720,  5.4324,  5.4928,  5.5531,
         5.6135,  5.6739,  5.7342,  5.7946,  5.8549,  5.9153,  5.9757,  6.0964,
         6.1567,  6.3378,  6.5189], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  203
name:  bert.encoder.layer.9.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0249,  0.0167,  0.0445,  ...,  0.0120,  0.0158,  0.0611],
        [-0.0303,  0.0345, -0.0627,  ...,  0.0241,  0.0536,  0.0080],
        [ 0.0099, -0.0343,  0.0250,  ...,  0.0589, -0.0179, -0.0413],
        ...,
        [-0.0010,  0.0506, -0.0197,  ...,  0.0399, -0.0159,  0.0113],
        [ 0.0251,  0.0058, -0.0075,  ..., -0.0653, -0.0188, -0.0564],
        [-0.0065,  0.0397, -0.0257,  ...,  0.0180, -0.0054,  0.0137]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0247,  0.0165,  0.0445,  ...,  0.0115,  0.0165,  0.0610],
        [-0.0297,  0.0346, -0.0627,  ...,  0.0247,  0.0544,  0.0082],
        [ 0.0099, -0.0346,  0.0247,  ...,  0.0594, -0.0181, -0.0412],
        ...,
        [-0.0016,  0.0511, -0.0198,  ...,  0.0396, -0.0165,  0.0115],
        [ 0.0247,  0.0066, -0.0082,  ..., -0.0660, -0.0181, -0.0561],
        [-0.0066,  0.0396, -0.0264,  ...,  0.0181, -0.0049,  0.0132]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.1979, -0.1896, -0.1748, -0.1699, -0.1682, -0.1666, -0.1633, -0.1616,
        -0.1600, -0.1583, -0.1567, -0.1550, -0.1534, -0.1517, -0.1501, -0.1484,
        -0.1468, -0.1451, -0.1435, -0.1418, -0.1402, -0.1385, -0.1369, -0.1352,
        -0.1336, -0.1319, -0.1303, -0.1286, -0.1270, -0.1253, -0.1237, -0.1220,
        -0.1204, -0.1187, -0.1171, -0.1154, -0.1138, -0.1121, -0.1105, -0.1088,
        -0.1072, -0.1055, -0.1039, -0.1022, -0.1006, -0.0989, -0.0973, -0.0956,
        -0.0940, -0.0923, -0.0907, -0.0890, -0.0874, -0.0858, -0.0841, -0.0825,
        -0.0808, -0.0792, -0.0775, -0.0759, -0.0742, -0.0726, -0.0709, -0.0693,
        -0.0676, -0.0660, -0.0643, -0.0627, -0.0610, -0.0594, -0.0577, -0.0561,
        -0.0544, -0.0528, -0.0511, -0.0495, -0.0478, -0.0462, -0.0445, -0.0429,
        -0.0412, -0.0396, -0.0379, -0.0363, -0.0346, -0.0330, -0.0313, -0.0297,
        -0.0280, -0.0264, -0.0247, -0.0231, -0.0214, -0.0198, -0.0181, -0.0165,
        -0.0148, -0.0132, -0.0115, -0.0099, -0.0082, -0.0066, -0.0049, -0.0033,
        -0.0016, -0.0000,  0.0016,  0.0033,  0.0049,  0.0066,  0.0082,  0.0099,
         0.0115,  0.0132,  0.0148,  0.0165,  0.0181,  0.0198,  0.0214,  0.0231,
         0.0247,  0.0264,  0.0280,  0.0297,  0.0313,  0.0330,  0.0346,  0.0363,
         0.0379,  0.0396,  0.0412,  0.0429,  0.0445,  0.0462,  0.0478,  0.0495,
         0.0511,  0.0528,  0.0544,  0.0561,  0.0577,  0.0594,  0.0610,  0.0627,
         0.0643,  0.0660,  0.0676,  0.0693,  0.0709,  0.0726,  0.0742,  0.0759,
         0.0775,  0.0792,  0.0808,  0.0825,  0.0841,  0.0858,  0.0874,  0.0890,
         0.0907,  0.0923,  0.0940,  0.0956,  0.0973,  0.0989,  0.1006,  0.1022,
         0.1039,  0.1055,  0.1072,  0.1088,  0.1105,  0.1121,  0.1138,  0.1154,
         0.1171,  0.1187,  0.1204,  0.1220,  0.1237,  0.1253,  0.1270,  0.1286,
         0.1303,  0.1319,  0.1336,  0.1352,  0.1369,  0.1385,  0.1402,  0.1418,
         0.1435,  0.1451,  0.1468,  0.1484,  0.1501,  0.1517,  0.1534,  0.1550,
         0.1567,  0.1583,  0.1600,  0.1616,  0.1633,  0.1682,  0.1847,  0.1962,
         0.1995,  0.2094], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  210
---------Q_out---------
None
name:  bert.encoder.layer.9.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0127,  0.0291, -0.0079,  ...,  0.0293,  0.0180, -0.0118],
        [ 0.0088, -0.0102, -0.0290,  ..., -0.0259, -0.0069, -0.0238],
        [ 0.0049, -0.0441, -0.0300,  ...,  0.0442,  0.0335,  0.0135],
        ...,
        [-0.0102, -0.0308,  0.0436,  ..., -0.0460, -0.0109, -0.0139],
        [-0.0027, -0.0193,  0.0013,  ..., -0.0261, -0.0114,  0.0131],
        [ 0.0198, -0.0465,  0.0027,  ...,  0.0130, -0.0087, -0.0089]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0118,  0.0294, -0.0088,  ...,  0.0294,  0.0177, -0.0118],
        [ 0.0088, -0.0088, -0.0294,  ..., -0.0265, -0.0059, -0.0235],
        [ 0.0059, -0.0441, -0.0294,  ...,  0.0441,  0.0324,  0.0147],
        ...,
        [-0.0088, -0.0294,  0.0441,  ..., -0.0471, -0.0118, -0.0147],
        [-0.0029, -0.0206,  0.0000,  ..., -0.0265, -0.0118,  0.0118],
        [ 0.0206, -0.0471,  0.0029,  ...,  0.0118, -0.0088, -0.0088]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3738, -0.2649, -0.2561, -0.2413, -0.2325, -0.2296, -0.2207, -0.2178,
        -0.2119, -0.2060, -0.2031, -0.2001, -0.1972, -0.1943, -0.1884, -0.1854,
        -0.1795, -0.1766, -0.1737, -0.1707, -0.1648, -0.1589, -0.1530, -0.1501,
        -0.1472, -0.1442, -0.1413, -0.1383, -0.1354, -0.1324, -0.1295, -0.1266,
        -0.1236, -0.1207, -0.1177, -0.1148, -0.1118, -0.1089, -0.1060, -0.1030,
        -0.1001, -0.0971, -0.0942, -0.0912, -0.0883, -0.0854, -0.0824, -0.0795,
        -0.0765, -0.0736, -0.0706, -0.0677, -0.0648, -0.0618, -0.0589, -0.0559,
        -0.0530, -0.0500, -0.0471, -0.0441, -0.0412, -0.0383, -0.0353, -0.0324,
        -0.0294, -0.0265, -0.0235, -0.0206, -0.0177, -0.0147, -0.0118, -0.0088,
        -0.0059, -0.0029, -0.0000,  0.0029,  0.0059,  0.0088,  0.0118,  0.0147,
         0.0177,  0.0206,  0.0235,  0.0265,  0.0294,  0.0324,  0.0353,  0.0383,
         0.0412,  0.0441,  0.0471,  0.0500,  0.0530,  0.0559,  0.0589,  0.0618,
         0.0648,  0.0677,  0.0706,  0.0736,  0.0765,  0.0795,  0.0824,  0.0854,
         0.0883,  0.0912,  0.0942,  0.0971,  0.1001,  0.1030,  0.1060,  0.1089,
         0.1118,  0.1148,  0.1177,  0.1207,  0.1236,  0.1266,  0.1295,  0.1324,
         0.1354,  0.1383,  0.1413,  0.1442,  0.1472,  0.1501,  0.1530,  0.1560,
         0.1619,  0.1648,  0.1678,  0.1707,  0.1737,  0.1795,  0.1854,  0.1884,
         0.1943,  0.2060,  0.2207,  0.2266,  0.2325,  0.2355,  0.2384,  0.2413,
         0.2708,  0.2796,  0.2943,  0.3385,  0.3591], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  149
---------Q_out---------
None
name:  bert.encoder.layer.9.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0629, -0.0077,  0.0021,  ...,  0.0211,  0.0409, -0.0592],
        [ 0.0203, -0.0268,  0.0203,  ..., -0.0100, -0.0123, -0.0223],
        [-0.0538, -0.0307,  0.0283,  ...,  0.0060,  0.0669, -0.0057],
        ...,
        [ 0.0030,  0.0108,  0.0723,  ...,  0.0184, -0.0548, -0.0084],
        [ 0.0004,  0.0598, -0.0425,  ...,  0.0171,  0.0827, -0.0586],
        [ 0.0307, -0.0490, -0.0272,  ..., -0.1159,  0.0399, -0.0009]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0648, -0.0065,  0.0000,  ...,  0.0194,  0.0389, -0.0583],
        [ 0.0194, -0.0259,  0.0194,  ..., -0.0130, -0.0130, -0.0194],
        [-0.0518, -0.0324,  0.0259,  ...,  0.0065,  0.0648, -0.0065],
        ...,
        [ 0.0000,  0.0130,  0.0713,  ...,  0.0194, -0.0518, -0.0065],
        [ 0.0000,  0.0583, -0.0453,  ...,  0.0194,  0.0842, -0.0583],
        [ 0.0324, -0.0518, -0.0259,  ..., -0.1166,  0.0389, -0.0000]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.8227, -0.7515, -0.7450, -0.6996, -0.6737, -0.6478, -0.6413, -0.5895,
        -0.5830, -0.5765, -0.5701, -0.5636, -0.5377, -0.5312, -0.5247, -0.5118,
        -0.5053, -0.4988, -0.4923, -0.4859, -0.4794, -0.4729, -0.4599, -0.4535,
        -0.4470, -0.4405, -0.4340, -0.4276, -0.4211, -0.4146, -0.4081, -0.4016,
        -0.3952, -0.3887, -0.3822, -0.3757, -0.3692, -0.3628, -0.3563, -0.3498,
        -0.3433, -0.3369, -0.3304, -0.3239, -0.3174, -0.3109, -0.3045, -0.2980,
        -0.2915, -0.2850, -0.2786, -0.2721, -0.2656, -0.2591, -0.2526, -0.2462,
        -0.2397, -0.2332, -0.2267, -0.2203, -0.2138, -0.2073, -0.2008, -0.1943,
        -0.1879, -0.1814, -0.1749, -0.1684, -0.1620, -0.1555, -0.1490, -0.1425,
        -0.1360, -0.1296, -0.1231, -0.1166, -0.1101, -0.1036, -0.0972, -0.0907,
        -0.0842, -0.0777, -0.0713, -0.0648, -0.0583, -0.0518, -0.0453, -0.0389,
        -0.0324, -0.0259, -0.0194, -0.0130, -0.0065, -0.0000,  0.0065,  0.0130,
         0.0194,  0.0259,  0.0324,  0.0389,  0.0453,  0.0518,  0.0583,  0.0648,
         0.0713,  0.0777,  0.0842,  0.0907,  0.0972,  0.1036,  0.1101,  0.1166,
         0.1231,  0.1296,  0.1360,  0.1425,  0.1490,  0.1555,  0.1620,  0.1684,
         0.1749,  0.1814,  0.1879,  0.1943,  0.2008,  0.2073,  0.2138,  0.2203,
         0.2267,  0.2332,  0.2397,  0.2462,  0.2526,  0.2591,  0.2656,  0.2721,
         0.2786,  0.2850,  0.2915,  0.2980,  0.3045,  0.3109,  0.3174,  0.3239,
         0.3304,  0.3369,  0.3433,  0.3498,  0.3563,  0.3628,  0.3692,  0.3757,
         0.3822,  0.3887,  0.3952,  0.4016,  0.4081,  0.4146,  0.4276,  0.4340,
         0.4405,  0.4470,  0.4535,  0.4599,  0.4664,  0.4729,  0.4794,  0.4859,
         0.4923,  0.4988,  0.5182,  0.5247,  0.5377,  0.5442,  0.5506,  0.5571,
         0.5636,  0.5701,  0.6089,  0.6154,  0.6284,  0.6413,  0.6996,  0.7385],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  184
---------Q_out---------
None
name:  bert.encoder.layer.9.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0549,  0.0073,  0.0137,  ..., -0.0441, -0.0058, -0.0271],
        [ 0.0164,  0.0395,  0.0088,  ..., -0.0045, -0.0284, -0.0266],
        [ 0.0043, -0.0237, -0.0001,  ...,  0.0163, -0.0186, -0.0064],
        ...,
        [ 0.0046, -0.0306, -0.0116,  ...,  0.0189, -0.0244, -0.0089],
        [-0.0031, -0.0036, -0.0306,  ..., -0.0121,  0.0417, -0.0260],
        [-0.0341, -0.0097, -0.0176,  ..., -0.0647,  0.0017,  0.0063]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0484,  0.0000,  0.0242,  ..., -0.0484, -0.0000, -0.0242],
        [ 0.0242,  0.0484,  0.0000,  ..., -0.0000, -0.0242, -0.0242],
        [ 0.0000, -0.0242, -0.0000,  ...,  0.0242, -0.0242, -0.0000],
        ...,
        [ 0.0000, -0.0242, -0.0000,  ...,  0.0242, -0.0242, -0.0000],
        [-0.0000, -0.0000, -0.0242,  ..., -0.0000,  0.0484, -0.0242],
        [-0.0242, -0.0000, -0.0242,  ..., -0.0726,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-3.0745, -2.8324, -2.6629, -2.5903, -2.5177, -2.2756, -2.1061, -1.9367,
        -1.7914, -1.7430, -1.7188, -1.3315, -1.3073, -1.2104, -1.1862, -1.1620,
        -1.1378, -1.1136, -1.0168, -0.9925, -0.8715, -0.7989, -0.7747, -0.6294,
        -0.6052, -0.5810, -0.5568, -0.5326, -0.5084, -0.4842, -0.4600, -0.4358,
        -0.4115, -0.3873, -0.3631, -0.3389, -0.3147, -0.2905, -0.2663, -0.2421,
        -0.2179, -0.1937, -0.1695, -0.1453, -0.1210, -0.0968, -0.0726, -0.0484,
        -0.0242, -0.0000,  0.0242,  0.0484,  0.0726,  0.0968,  0.1210,  0.1453,
         0.1695,  0.1937,  0.2179,  0.2421,  0.2663,  0.2905,  0.3147,  0.3389,
         0.3631,  0.3873,  0.4115,  0.4358,  0.4600,  0.4842,  0.5084,  0.6052,
         0.7263,  0.7747,  0.8715,  0.8957,  0.9683,  0.9925,  1.1378,  1.3799,
         1.6704,  2.1061,  2.1788], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  83
---------Q_out---------
None
name:  bert.encoder.layer.10.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 8.0765e-04,  3.7525e-02, -8.1769e-03,  ..., -6.3690e-02,
         -8.7442e-04,  9.3377e-03],
        [-8.4411e-03,  2.2000e-02,  3.7641e-02,  ..., -2.5657e-02,
          4.9983e-02,  1.4542e-02],
        [ 1.7675e-02, -4.4725e-02,  2.7368e-02,  ..., -3.3973e-05,
         -2.2218e-02, -2.1610e-02],
        ...,
        [-2.1641e-03, -1.3312e-02,  1.9141e-02,  ..., -7.8582e-02,
          3.4574e-02, -2.0281e-02],
        [-3.6263e-02,  2.1391e-02,  9.2538e-03,  ..., -2.3298e-02,
          4.4506e-02, -1.0833e-01],
        [ 7.1287e-02, -2.0219e-02,  5.8587e-02,  ...,  1.0124e-01,
          1.8682e-02, -6.1319e-02]], device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0000,  0.0365, -0.0073,  ..., -0.0621, -0.0000,  0.0110],
        [-0.0073,  0.0219,  0.0365,  ..., -0.0256,  0.0511,  0.0146],
        [ 0.0183, -0.0438,  0.0256,  ..., -0.0000, -0.0219, -0.0219],
        ...,
        [-0.0037, -0.0146,  0.0183,  ..., -0.0804,  0.0329, -0.0219],
        [-0.0365,  0.0219,  0.0110,  ..., -0.0219,  0.0438, -0.1096],
        [ 0.0731, -0.0219,  0.0584,  ...,  0.1023,  0.0183, -0.0621]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.4201, -0.4164, -0.3982, -0.3580, -0.3434, -0.3397, -0.3105, -0.3032,
        -0.2995, -0.2886, -0.2813, -0.2703, -0.2667, -0.2630, -0.2594, -0.2557,
        -0.2521, -0.2484, -0.2447, -0.2411, -0.2374, -0.2338, -0.2301, -0.2265,
        -0.2228, -0.2192, -0.2155, -0.2119, -0.2082, -0.2046, -0.2009, -0.1973,
        -0.1936, -0.1900, -0.1863, -0.1826, -0.1790, -0.1753, -0.1717, -0.1680,
        -0.1644, -0.1607, -0.1571, -0.1534, -0.1498, -0.1461, -0.1425, -0.1388,
        -0.1352, -0.1315, -0.1279, -0.1242, -0.1205, -0.1169, -0.1132, -0.1096,
        -0.1059, -0.1023, -0.0986, -0.0950, -0.0913, -0.0877, -0.0840, -0.0804,
        -0.0767, -0.0731, -0.0694, -0.0658, -0.0621, -0.0584, -0.0548, -0.0511,
        -0.0475, -0.0438, -0.0402, -0.0365, -0.0329, -0.0292, -0.0256, -0.0219,
        -0.0183, -0.0146, -0.0110, -0.0073, -0.0037, -0.0000,  0.0037,  0.0073,
         0.0110,  0.0146,  0.0183,  0.0219,  0.0256,  0.0292,  0.0329,  0.0365,
         0.0402,  0.0438,  0.0475,  0.0511,  0.0548,  0.0584,  0.0621,  0.0658,
         0.0694,  0.0731,  0.0767,  0.0804,  0.0840,  0.0877,  0.0913,  0.0950,
         0.0986,  0.1023,  0.1059,  0.1096,  0.1132,  0.1169,  0.1205,  0.1242,
         0.1279,  0.1315,  0.1352,  0.1388,  0.1425,  0.1461,  0.1498,  0.1534,
         0.1571,  0.1607,  0.1644,  0.1680,  0.1717,  0.1753,  0.1790,  0.1826,
         0.1863,  0.1900,  0.1936,  0.1973,  0.2009,  0.2046,  0.2082,  0.2119,
         0.2155,  0.2192,  0.2228,  0.2265,  0.2301,  0.2374,  0.2411,  0.2447,
         0.2484,  0.2521,  0.2557,  0.2594,  0.2630,  0.2667,  0.2703,  0.2740,
         0.2813,  0.2886,  0.3032,  0.3068,  0.3105,  0.3142,  0.3288,  0.3726,
         0.3872,  0.3945,  0.4128,  0.4384,  0.4639], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  173
---------Q_out---------
tensor([[[ 1.2697, -0.6348, -0.7759,  ..., -0.3527,  0.2116,  0.2821],
         [-0.7054,  0.7054,  0.6348,  ..., -1.4107, -0.0000, -0.2116],
         [ 0.6348,  0.2821,  0.3527,  ..., -2.1161, -0.4938, -0.2821],
         ...,
         [-0.0000,  0.2821,  0.9170,  ..., -1.3402,  0.6348,  1.0580],
         [-0.2821,  0.1411,  0.0705,  ..., -0.4938, -0.0705,  1.1286],
         [ 0.2821, -0.2821,  0.5643,  ..., -1.6929,  0.1411,  0.6348]],

        [[ 1.4107, -0.5643, -1.3402,  ..., -0.8464, -0.2821,  0.5643],
         [-0.7759, -1.4813, -0.0705,  ..., -3.2447, -1.0580, -0.5643],
         [-0.4938, -2.0456, -1.1286,  ..., -2.1161, -0.4232, -0.4232],
         ...,
         [ 0.1411,  0.5643, -1.6223,  ..., -1.6223,  0.1411, -0.2116],
         [ 0.1411, -0.2821,  0.6348,  ..., -1.7634, -0.9170,  1.2697],
         [-0.0000, -0.2116,  0.0705,  ..., -1.6929, -0.0000, -0.3527]],

        [[ 0.8464, -0.2116, -1.1991,  ..., -0.2116, -0.4938, -0.6348],
         [ 1.3402, -0.3527, -0.8464,  ..., -0.9170,  0.0000,  0.9875],
         [ 1.6223, -0.5643, -0.9170,  ..., -0.4938, -0.4938,  0.9170],
         ...,
         [ 0.3527,  0.2116,  1.4813,  ..., -2.5393, -0.9170,  0.9170],
         [-0.1411, -0.4232,  0.4232,  ..., -1.6929, -0.9875,  0.2821],
         [ 0.0705,  0.1411,  0.3527,  ..., -1.2697, -1.1286,  0.6348]],

        ...,

        [[-0.4232, -0.2821, -1.1991,  ...,  0.5643, -0.5643,  0.7054],
         [ 0.3527, -0.5643, -0.3527,  ..., -0.2821,  0.2116,  0.5643],
         [ 0.9170,  0.1411,  0.0705,  ..., -1.1286, -1.7634, -0.4938],
         ...,
         [-0.4938,  1.3402,  0.4938,  ..., -1.1991, -0.2821,  1.2697],
         [-1.2697,  0.7759,  0.6348,  ..., -0.4938, -0.4232,  1.6929],
         [-1.0580,  0.7054,  0.8464,  ..., -0.6348, -0.8464,  0.7759]],

        [[ 1.1991,  0.0705, -0.7054,  ...,  0.3527, -0.0705, -0.0705],
         [ 1.7634,  1.7634,  1.0580,  ..., -1.8339, -0.7759, -0.0705],
         [ 1.9045,  1.5518,  0.2116,  ..., -2.6099, -1.0580, -0.4232],
         ...,
         [ 0.4938,  0.7054,  0.6348,  ..., -0.9170, -0.9170,  0.9170],
         [ 0.9170,  0.7759,  0.4938,  ..., -1.3402, -1.0580,  0.2821],
         [-0.2116,  0.7759,  1.4107,  ..., -1.2697, -0.9875,  1.4107]],

        [[ 0.9875, -0.3527, -1.2697,  ...,  0.5643,  0.2116,  0.2821],
         [ 0.0705,  0.7759, -0.2821,  ..., -2.6099, -0.4232, -0.7759],
         [ 0.5643,  0.2116, -0.8464,  ..., -2.2572, -0.9170, -1.4813],
         ...,
         [ 0.1411, -0.6348,  0.8464,  ..., -1.6929, -0.0705, -1.1991],
         [-0.1411,  0.4232,  1.9045,  ..., -1.9750,  0.5643,  0.2821],
         [ 0.5643, -0.5643,  1.1286,  ..., -1.4813,  0.7054,  0.0000]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-8.9581, -8.8876, -8.6760, -8.6055, -8.3938, -8.3233, -8.2528, -8.1822,
        -8.1117, -8.0412, -7.9706, -7.9001, -7.8296, -7.7590, -7.6885, -7.6179,
        -7.5474, -7.4769, -7.4063, -7.3358, -7.2653, -7.1947, -7.1242, -7.0537,
        -6.9831, -6.9126, -6.8420, -6.7715, -6.7010, -6.6304, -6.5599, -6.4894,
        -6.4188, -6.3483, -6.2777, -6.2072, -6.1367, -6.0661, -5.9956, -5.9251,
        -5.8545, -5.7840, -5.7135, -5.6429, -5.5724, -5.5018, -5.4313, -5.3608,
        -5.2902, -5.2197, -5.1492, -5.0786, -5.0081, -4.9376, -4.8670, -4.7965,
        -4.7259, -4.6554, -4.5849, -4.5143, -4.4438, -4.3733, -4.3027, -4.2322,
        -4.1617, -4.0911, -4.0206, -3.9500, -3.8795, -3.8090, -3.7384, -3.6679,
        -3.5974, -3.5268, -3.4563, -3.3858, -3.3152, -3.2447, -3.1741, -3.1036,
        -3.0331, -2.9625, -2.8920, -2.8215, -2.7509, -2.6804, -2.6099, -2.5393,
        -2.4688, -2.3982, -2.3277, -2.2572, -2.1866, -2.1161, -2.0456, -1.9750,
        -1.9045, -1.8339, -1.7634, -1.6929, -1.6223, -1.5518, -1.4813, -1.4107,
        -1.3402, -1.2697, -1.1991, -1.1286, -1.0580, -0.9875, -0.9170, -0.8464,
        -0.7759, -0.7054, -0.6348, -0.5643, -0.4938, -0.4232, -0.3527, -0.2821,
        -0.2116, -0.1411, -0.0705, -0.0000,  0.0705,  0.1411,  0.2116,  0.2821,
         0.3527,  0.4232,  0.4938,  0.5643,  0.6348,  0.7054,  0.7759,  0.8464,
         0.9170,  0.9875,  1.0580,  1.1286,  1.1991,  1.2697,  1.3402,  1.4107,
         1.4813,  1.5518,  1.6223,  1.6929,  1.7634,  1.8339,  1.9045,  1.9750,
         2.0456,  2.1161,  2.1866,  2.2572,  2.3277,  2.3982,  2.4688,  2.5393,
         2.6099,  2.6804,  2.7509,  2.8215,  2.8920,  2.9625,  3.0331,  3.1036,
         3.1741,  3.2447,  3.3152,  3.3858,  3.4563,  3.5268,  3.5974,  3.6679,
         3.7384,  3.8090,  3.8795,  3.9500,  4.0206,  4.0911,  4.1617,  4.2322,
         4.3027,  4.3733,  4.4438,  4.5143,  4.5849,  4.6554,  4.7259,  4.7965,
         4.8670,  4.9376,  5.0081,  5.0786,  5.1492,  5.2197,  5.2902,  5.3608,
         5.4313,  5.5018,  5.5724,  5.6429,  5.7135,  5.7840,  5.8545,  5.9251,
         5.9956,  6.0661,  6.1367,  6.2072,  6.2777,  6.3483,  6.4188,  6.4894,
         6.5599,  6.6304,  6.7010,  6.7715,  6.8420,  6.9126,  6.9831,  7.0537,
         7.1242,  7.1947,  7.2653,  7.6179], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  228
name:  bert.encoder.layer.10.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0493, -0.0303,  0.0179,  ..., -0.0675, -0.0282,  0.0717],
        [ 0.0553, -0.0460,  0.0137,  ..., -0.0549,  0.0218,  0.0576],
        [-0.0247, -0.0289, -0.0606,  ...,  0.0338, -0.1068,  0.0231],
        ...,
        [-0.0458,  0.0601,  0.0167,  ...,  0.0461, -0.0163, -0.0458],
        [-0.0870,  0.0106,  0.0943,  ..., -0.0400,  0.0397, -0.0676],
        [-0.0318, -0.0725, -0.0395,  ...,  0.0227, -0.0563, -0.0246]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0482, -0.0304,  0.0178,  ..., -0.0685, -0.0279,  0.0710],
        [ 0.0558, -0.0457,  0.0127,  ..., -0.0558,  0.0228,  0.0583],
        [-0.0254, -0.0279, -0.0609,  ...,  0.0330, -0.1065,  0.0228],
        ...,
        [-0.0457,  0.0609,  0.0178,  ...,  0.0457, -0.0152, -0.0457],
        [-0.0862,  0.0101,  0.0939,  ..., -0.0406,  0.0406, -0.0685],
        [-0.0330, -0.0736, -0.0406,  ...,  0.0228, -0.0558, -0.0254]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3222, -0.2917, -0.2892, -0.2765, -0.2663, -0.2638, -0.2613, -0.2562,
        -0.2537, -0.2461, -0.2435, -0.2410, -0.2308, -0.2283, -0.2258, -0.2232,
        -0.2207, -0.2182, -0.2156, -0.2105, -0.2080, -0.2055, -0.2029, -0.1979,
        -0.1953, -0.1928, -0.1902, -0.1877, -0.1852, -0.1826, -0.1801, -0.1776,
        -0.1750, -0.1725, -0.1700, -0.1674, -0.1649, -0.1623, -0.1598, -0.1573,
        -0.1547, -0.1522, -0.1497, -0.1471, -0.1446, -0.1421, -0.1395, -0.1370,
        -0.1344, -0.1319, -0.1294, -0.1268, -0.1243, -0.1218, -0.1192, -0.1167,
        -0.1141, -0.1116, -0.1091, -0.1065, -0.1040, -0.1015, -0.0989, -0.0964,
        -0.0939, -0.0913, -0.0888, -0.0862, -0.0837, -0.0812, -0.0786, -0.0761,
        -0.0736, -0.0710, -0.0685, -0.0660, -0.0634, -0.0609, -0.0583, -0.0558,
        -0.0533, -0.0507, -0.0482, -0.0457, -0.0431, -0.0406, -0.0380, -0.0355,
        -0.0330, -0.0304, -0.0279, -0.0254, -0.0228, -0.0203, -0.0178, -0.0152,
        -0.0127, -0.0101, -0.0076, -0.0051, -0.0025, -0.0000,  0.0025,  0.0051,
         0.0076,  0.0101,  0.0127,  0.0152,  0.0178,  0.0203,  0.0228,  0.0254,
         0.0279,  0.0304,  0.0330,  0.0355,  0.0380,  0.0406,  0.0431,  0.0457,
         0.0482,  0.0507,  0.0533,  0.0558,  0.0583,  0.0609,  0.0634,  0.0660,
         0.0685,  0.0710,  0.0736,  0.0761,  0.0786,  0.0812,  0.0837,  0.0862,
         0.0888,  0.0913,  0.0939,  0.0964,  0.0989,  0.1015,  0.1040,  0.1065,
         0.1091,  0.1116,  0.1141,  0.1167,  0.1192,  0.1218,  0.1243,  0.1268,
         0.1294,  0.1319,  0.1344,  0.1370,  0.1395,  0.1421,  0.1446,  0.1471,
         0.1497,  0.1522,  0.1547,  0.1573,  0.1598,  0.1623,  0.1649,  0.1674,
         0.1700,  0.1725,  0.1750,  0.1776,  0.1801,  0.1826,  0.1852,  0.1877,
         0.1902,  0.1928,  0.1953,  0.1979,  0.2004,  0.2029,  0.2055,  0.2080,
         0.2105,  0.2156,  0.2182,  0.2207,  0.2232,  0.2258,  0.2283,  0.2308,
         0.2334,  0.2435,  0.2511,  0.2537,  0.2613], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  197
---------Q_out---------
tensor([[[-0.5468, -0.5468, -0.7290,  ...,  1.2758, -0.3645,  0.8505],
         [-1.1543,  0.4860, -0.3038,  ..., -0.4860, -0.0608, -0.8505],
         [-0.1823, -0.1215, -0.6683,  ...,  0.1215, -0.6683, -0.0608],
         ...,
         [ 0.1823, -1.2758,  0.8505,  ...,  0.5468, -2.0656,  0.2430],
         [-1.1543, -0.6075, -0.7898,  ...,  0.0608, -1.5796,  0.2430],
         [-0.0608, -0.9113, -0.5468,  ...,  0.4253, -2.2478, -0.3645]],

        [[-1.5188,  0.1215, -0.2430,  ...,  0.9720, -0.6075,  0.4253],
         [-1.7618, -0.7290,  1.3973,  ..., -0.5468, -2.3086,  2.1871],
         [-2.0048, -0.9720,  0.3645,  ..., -0.2430, -1.2758,  1.1543],
         ...,
         [-0.1215,  0.2430, -1.3365,  ...,  0.4253,  2.2478,  0.7290],
         [-0.4860, -0.4860,  0.3645,  ...,  0.3645, -0.5468,  0.2430],
         [-0.2430, -0.1823, -0.2430,  ..., -0.8505, -0.5468, -0.1823]],

        [[ 0.6075,  0.6683, -1.0328,  ...,  0.4860,  0.0608,  0.4860],
         [ 0.8505,  0.6683, -0.9113,  ...,  0.3645, -0.1215,  0.9113],
         [ 1.8833,  0.3645, -0.8505,  ...,  0.7898, -0.7290,  0.6075],
         ...,
         [ 0.4253, -0.0608,  0.2430,  ..., -0.4860, -1.1543,  0.2430],
         [ 0.1215,  0.1215, -0.1823,  ..., -0.6075, -1.6403,  0.0608],
         [ 0.4860, -0.1823, -0.6075,  ...,  0.6075, -2.3086,  0.1823]],

        ...,

        [[-0.9720, -0.7290, -0.9113,  ...,  1.3973, -0.0608,  1.2758],
         [-0.7898,  1.0935,  0.6075,  ..., -0.7898, -0.4860,  0.8505],
         [-1.4581,  0.9113,  0.1215,  ..., -0.6683, -0.7290,  2.0656],
         ...,
         [-0.8505, -0.3645,  0.3645,  ...,  0.0608, -2.9161,  0.2430],
         [-1.5188, -0.5468, -0.3038,  ...,  0.1215, -2.8554, -0.3038],
         [-0.9113, -0.9113, -0.1823,  ..., -0.3038, -2.7946,  0.4253]],

        [[ 0.4860,  0.3645, -0.7290,  ...,  1.3973, -0.6075,  0.3645],
         [ 0.4860,  2.0656,  0.7290,  ..., -0.4860, -0.4860,  0.6075],
         [ 0.1823,  0.2430,  0.6075,  ..., -0.1215, -1.3365,  0.9720],
         ...,
         [-0.4253,  0.0608,  0.4253,  ...,  0.1215, -3.2806, -0.7898],
         [-0.7898,  1.0328,  0.1823,  ...,  0.5468, -2.6731, -1.0935],
         [-0.4860, -0.6683,  0.3645,  ...,  0.9113, -1.9441,  1.0328]],

        [[ 0.0000, -0.0608, -0.4860,  ...,  1.3365, -0.2430,  0.9113],
         [-0.7898,  0.1823, -0.1823,  ..., -0.7898, -1.5188,  0.3645],
         [-0.3038, -0.1215, -0.2430,  ..., -1.0328, -0.2430, -0.1823],
         ...,
         [-1.3973, -1.2150,  0.1823,  ..., -0.4253, -1.2150, -0.0608],
         [-0.9113, -0.4860, -0.1215,  ...,  0.4253, -1.2150, -1.2150],
         [-0.9113, -0.7898, -0.2430,  ..., -0.8505, -1.4581, -0.2430]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.7155, -7.6548, -7.5333, -7.4118, -7.2903, -7.2295, -7.1080, -7.0473,
        -6.9865, -6.9258, -6.8650, -6.8043, -6.7435, -6.6827, -6.6220, -6.5612,
        -6.5005, -6.4397, -6.3790, -6.3182, -6.2575, -6.1967, -6.1360, -6.0752,
        -6.0145, -5.9537, -5.8930, -5.8322, -5.7715, -5.7107, -5.6500, -5.5892,
        -5.5285, -5.4677, -5.4069, -5.3462, -5.2854, -5.2247, -5.1639, -5.1032,
        -5.0424, -4.9817, -4.9209, -4.8602, -4.7994, -4.7387, -4.6779, -4.6172,
        -4.5564, -4.4957, -4.4349, -4.3742, -4.3134, -4.2527, -4.1919, -4.1312,
        -4.0704, -4.0096, -3.9489, -3.8881, -3.8274, -3.7666, -3.7059, -3.6451,
        -3.5844, -3.5236, -3.4629, -3.4021, -3.3414, -3.2806, -3.2199, -3.1591,
        -3.0984, -3.0376, -2.9769, -2.9161, -2.8554, -2.7946, -2.7339, -2.6731,
        -2.6123, -2.5516, -2.4908, -2.4301, -2.3693, -2.3086, -2.2478, -2.1871,
        -2.1263, -2.0656, -2.0048, -1.9441, -1.8833, -1.8226, -1.7618, -1.7011,
        -1.6403, -1.5796, -1.5188, -1.4581, -1.3973, -1.3365, -1.2758, -1.2150,
        -1.1543, -1.0935, -1.0328, -0.9720, -0.9113, -0.8505, -0.7898, -0.7290,
        -0.6683, -0.6075, -0.5468, -0.4860, -0.4253, -0.3645, -0.3038, -0.2430,
        -0.1823, -0.1215, -0.0608, -0.0000,  0.0608,  0.1215,  0.1823,  0.2430,
         0.3038,  0.3645,  0.4253,  0.4860,  0.5468,  0.6075,  0.6683,  0.7290,
         0.7898,  0.8505,  0.9113,  0.9720,  1.0328,  1.0935,  1.1543,  1.2150,
         1.2758,  1.3365,  1.3973,  1.4581,  1.5188,  1.5796,  1.6403,  1.7011,
         1.7618,  1.8226,  1.8833,  1.9441,  2.0048,  2.0656,  2.1263,  2.1871,
         2.2478,  2.3086,  2.3693,  2.4301,  2.4908,  2.5516,  2.6123,  2.6731,
         2.7339,  2.7946,  2.8554,  2.9161,  2.9769,  3.0376,  3.0984,  3.1591,
         3.2199,  3.2806,  3.3414,  3.4021,  3.4629,  3.5236,  3.5844,  3.6451,
         3.7059,  3.7666,  3.8274,  3.8881,  3.9489,  4.0096,  4.0704,  4.1312,
         4.1919,  4.2527,  4.3134,  4.3742,  4.4349,  4.4957,  4.5564,  4.6172,
         4.6779,  4.7387,  4.7994,  4.8602,  4.9209,  4.9817,  5.0424,  5.1032,
         5.1639,  5.2247,  5.2854,  5.3462,  5.4069,  5.4677,  5.5285,  5.5892,
         5.6500,  5.7107,  5.7715,  5.8322,  5.8930,  5.9537,  6.0145,  6.0752,
         6.1360,  6.1967,  6.2575,  6.3182,  6.3790,  6.4397,  6.5005,  6.5612,
         6.6827,  6.7435,  6.8043,  6.8650,  6.9258,  6.9865,  7.0473,  7.1080,
         7.2295,  7.4118,  7.5333,  7.5940,  7.7155], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  245
name:  bert.encoder.layer.10.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0192,  0.0251, -0.0789,  ...,  0.0008, -0.0137, -0.1020],
        [ 0.0485,  0.0594, -0.0427,  ..., -0.0052,  0.0005,  0.0854],
        [-0.0260, -0.0246, -0.0730,  ...,  0.0181,  0.0101,  0.0123],
        ...,
        [ 0.0521, -0.0129, -0.0081,  ...,  0.0221,  0.0349, -0.0142],
        [-0.0153, -0.0015, -0.0026,  ...,  0.0110, -0.0845,  0.0151],
        [-0.0844, -0.0116, -0.0087,  ...,  0.0190, -0.0088, -0.0266]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0199,  0.0259, -0.0797,  ...,  0.0000, -0.0139, -0.1016],
        [ 0.0478,  0.0598, -0.0418,  ..., -0.0060,  0.0000,  0.0857],
        [-0.0259, -0.0239, -0.0737,  ...,  0.0179,  0.0100,  0.0120],
        ...,
        [ 0.0518, -0.0120, -0.0080,  ...,  0.0219,  0.0359, -0.0139],
        [-0.0159, -0.0020, -0.0020,  ...,  0.0120, -0.0837,  0.0159],
        [-0.0837, -0.0120, -0.0080,  ...,  0.0199, -0.0080, -0.0259]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2470, -0.2251, -0.2152, -0.2072, -0.2012, -0.1992, -0.1932, -0.1912,
        -0.1893, -0.1873, -0.1853, -0.1833, -0.1813, -0.1793, -0.1753, -0.1733,
        -0.1713, -0.1693, -0.1673, -0.1653, -0.1634, -0.1614, -0.1594, -0.1574,
        -0.1554, -0.1534, -0.1514, -0.1494, -0.1474, -0.1454, -0.1434, -0.1414,
        -0.1395, -0.1375, -0.1355, -0.1335, -0.1315, -0.1295, -0.1275, -0.1255,
        -0.1235, -0.1215, -0.1195, -0.1175, -0.1155, -0.1136, -0.1116, -0.1096,
        -0.1076, -0.1056, -0.1036, -0.1016, -0.0996, -0.0976, -0.0956, -0.0936,
        -0.0916, -0.0896, -0.0877, -0.0857, -0.0837, -0.0817, -0.0797, -0.0777,
        -0.0757, -0.0737, -0.0717, -0.0697, -0.0677, -0.0657, -0.0637, -0.0618,
        -0.0598, -0.0578, -0.0558, -0.0538, -0.0518, -0.0498, -0.0478, -0.0458,
        -0.0438, -0.0418, -0.0398, -0.0379, -0.0359, -0.0339, -0.0319, -0.0299,
        -0.0279, -0.0259, -0.0239, -0.0219, -0.0199, -0.0179, -0.0159, -0.0139,
        -0.0120, -0.0100, -0.0080, -0.0060, -0.0040, -0.0020, -0.0000,  0.0020,
         0.0040,  0.0060,  0.0080,  0.0100,  0.0120,  0.0139,  0.0159,  0.0179,
         0.0199,  0.0219,  0.0239,  0.0259,  0.0279,  0.0299,  0.0319,  0.0339,
         0.0359,  0.0379,  0.0398,  0.0418,  0.0438,  0.0458,  0.0478,  0.0498,
         0.0518,  0.0538,  0.0558,  0.0578,  0.0598,  0.0618,  0.0637,  0.0657,
         0.0677,  0.0697,  0.0717,  0.0737,  0.0757,  0.0777,  0.0797,  0.0817,
         0.0837,  0.0857,  0.0877,  0.0896,  0.0916,  0.0936,  0.0956,  0.0976,
         0.0996,  0.1016,  0.1036,  0.1056,  0.1076,  0.1096,  0.1116,  0.1136,
         0.1155,  0.1175,  0.1195,  0.1215,  0.1235,  0.1255,  0.1275,  0.1295,
         0.1315,  0.1335,  0.1355,  0.1375,  0.1395,  0.1414,  0.1434,  0.1454,
         0.1474,  0.1494,  0.1514,  0.1534,  0.1554,  0.1574,  0.1594,  0.1614,
         0.1634,  0.1653,  0.1673,  0.1693,  0.1713,  0.1733,  0.1753,  0.1773,
         0.1813,  0.1833,  0.1853,  0.1893,  0.1912,  0.1932,  0.2012,  0.2152,
         0.2291,  0.2331,  0.2391,  0.2530], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  204
---------Q_out---------
None
name:  bert.encoder.layer.10.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0590, -0.0372,  0.0036,  ...,  0.0244,  0.0208,  0.0001],
        [ 0.0261, -0.0272,  0.0136,  ..., -0.0236, -0.0122, -0.0435],
        [ 0.0080,  0.0214,  0.0056,  ..., -0.0329, -0.0063, -0.0017],
        ...,
        [-0.0345, -0.0241,  0.0452,  ..., -0.0426,  0.0243, -0.0147],
        [-0.0154, -0.0068,  0.0624,  ..., -0.0048,  0.0293, -0.0042],
        [ 0.0121, -0.0876,  0.0345,  ..., -0.0284, -0.0237, -0.0068]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0585, -0.0383,  0.0045,  ...,  0.0248,  0.0203,  0.0000],
        [ 0.0270, -0.0270,  0.0135,  ..., -0.0225, -0.0113, -0.0428],
        [ 0.0090,  0.0203,  0.0045,  ..., -0.0338, -0.0068, -0.0023],
        ...,
        [-0.0338, -0.0248,  0.0450,  ..., -0.0428,  0.0248, -0.0158],
        [-0.0158, -0.0068,  0.0630,  ..., -0.0045,  0.0293, -0.0045],
        [ 0.0113, -0.0878,  0.0338,  ..., -0.0293, -0.0248, -0.0068]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2610, -0.2318, -0.2228, -0.2003, -0.1935, -0.1823, -0.1778, -0.1710,
        -0.1688, -0.1665, -0.1643, -0.1620, -0.1598, -0.1575, -0.1553, -0.1530,
        -0.1508, -0.1485, -0.1463, -0.1440, -0.1418, -0.1395, -0.1373, -0.1350,
        -0.1328, -0.1305, -0.1283, -0.1260, -0.1238, -0.1215, -0.1193, -0.1170,
        -0.1148, -0.1125, -0.1103, -0.1080, -0.1058, -0.1035, -0.1013, -0.0990,
        -0.0968, -0.0945, -0.0923, -0.0900, -0.0878, -0.0855, -0.0833, -0.0810,
        -0.0788, -0.0765, -0.0743, -0.0720, -0.0698, -0.0675, -0.0653, -0.0630,
        -0.0608, -0.0585, -0.0563, -0.0540, -0.0518, -0.0495, -0.0473, -0.0450,
        -0.0428, -0.0405, -0.0383, -0.0360, -0.0338, -0.0315, -0.0293, -0.0270,
        -0.0248, -0.0225, -0.0203, -0.0180, -0.0158, -0.0135, -0.0113, -0.0090,
        -0.0068, -0.0045, -0.0023, -0.0000,  0.0023,  0.0045,  0.0068,  0.0090,
         0.0113,  0.0135,  0.0158,  0.0180,  0.0203,  0.0225,  0.0248,  0.0270,
         0.0293,  0.0315,  0.0338,  0.0360,  0.0383,  0.0405,  0.0428,  0.0450,
         0.0473,  0.0495,  0.0518,  0.0540,  0.0563,  0.0585,  0.0608,  0.0630,
         0.0653,  0.0675,  0.0698,  0.0720,  0.0743,  0.0765,  0.0788,  0.0810,
         0.0833,  0.0855,  0.0878,  0.0900,  0.0923,  0.0945,  0.0968,  0.0990,
         0.1013,  0.1035,  0.1058,  0.1080,  0.1103,  0.1125,  0.1148,  0.1170,
         0.1193,  0.1215,  0.1238,  0.1260,  0.1283,  0.1305,  0.1328,  0.1350,
         0.1373,  0.1395,  0.1418,  0.1440,  0.1463,  0.1485,  0.1508,  0.1530,
         0.1553,  0.1575,  0.1598,  0.1620,  0.1643,  0.1710,  0.1733,  0.1800,
         0.1913,  0.1980,  0.2048,  0.2138,  0.2250,  0.2858], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  166
---------Q_out---------
None
name:  bert.encoder.layer.10.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0705, -0.0247,  0.0211,  ..., -0.0297,  0.0292,  0.0765],
        [ 0.0699,  0.0475, -0.0157,  ..., -0.0874,  0.0163, -0.0298],
        [ 0.0143, -0.0026,  0.0006,  ...,  0.0051, -0.0301, -0.0048],
        ...,
        [ 0.0621, -0.0009,  0.0687,  ..., -0.0192, -0.0083, -0.0103],
        [-0.0366, -0.0245, -0.0048,  ...,  0.0206, -0.0058,  0.0157],
        [-0.0171, -0.0094,  0.0397,  ..., -0.0135,  0.0020,  0.0645]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0646, -0.0259,  0.0259,  ..., -0.0259,  0.0259,  0.0776],
        [ 0.0646,  0.0517, -0.0129,  ..., -0.0905,  0.0129, -0.0259],
        [ 0.0129, -0.0000,  0.0000,  ...,  0.0000, -0.0259, -0.0000],
        ...,
        [ 0.0646, -0.0000,  0.0646,  ..., -0.0129, -0.0129, -0.0129],
        [-0.0388, -0.0259, -0.0000,  ...,  0.0259, -0.0000,  0.0129],
        [-0.0129, -0.0129,  0.0388,  ..., -0.0129,  0.0000,  0.0646]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-1.6419, -1.2929, -1.1636, -0.5171, -0.4784, -0.4525, -0.4266, -0.4137,
        -0.4008, -0.3879, -0.3749, -0.3620, -0.3491, -0.3232, -0.3103, -0.2974,
        -0.2844, -0.2715, -0.2586, -0.2456, -0.2327, -0.2198, -0.2069, -0.1939,
        -0.1810, -0.1681, -0.1551, -0.1422, -0.1293, -0.1164, -0.1034, -0.0905,
        -0.0776, -0.0646, -0.0517, -0.0388, -0.0259, -0.0129, -0.0000,  0.0129,
         0.0259,  0.0388,  0.0517,  0.0646,  0.0776,  0.0905,  0.1034,  0.1164,
         0.1293,  0.1422,  0.1551,  0.1681,  0.1810,  0.1939,  0.2069,  0.2198,
         0.2327,  0.2456,  0.2586,  0.2715,  0.2844,  0.2974,  0.3103,  0.3232,
         0.3361,  0.3491,  0.3620,  0.3749,  0.3879,  0.4008,  0.4266,  0.4396,
         0.4525,  0.4913,  0.6206,  0.7886,  1.2799,  1.6419], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  78
---------Q_out---------
None
name:  bert.encoder.layer.10.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0292, -0.0296, -0.0141,  ...,  0.0272,  0.0116,  0.0043],
        [ 0.0261,  0.0077,  0.0487,  ..., -0.0432, -0.0136, -0.0180],
        [-0.0226,  0.0414, -0.0277,  ...,  0.0526,  0.0347, -0.0258],
        ...,
        [ 0.0122,  0.0002,  0.1214,  ..., -0.0158,  0.0339,  0.0311],
        [-0.0337, -0.0104,  0.0046,  ..., -0.0629,  0.0016, -0.0647],
        [-0.0038, -0.0096,  0.0562,  ..., -0.0357,  0.0407,  0.0315]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0502, -0.0502, -0.0000,  ...,  0.0502,  0.0000,  0.0000],
        [ 0.0502,  0.0000,  0.0502,  ..., -0.0502, -0.0000, -0.0000],
        [-0.0000,  0.0502, -0.0502,  ...,  0.0502,  0.0502, -0.0502],
        ...,
        [ 0.0000,  0.0000,  0.1004,  ..., -0.0000,  0.0502,  0.0502],
        [-0.0502, -0.0000,  0.0000,  ..., -0.0502,  0.0000, -0.0502],
        [-0.0000, -0.0000,  0.0502,  ..., -0.0502,  0.0502,  0.0502]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-6.3732, -3.9644, -1.6058, -1.1542, -1.1040, -0.9535, -0.9033, -0.8531,
        -0.7527, -0.7026, -0.6524, -0.6022, -0.5520, -0.5018, -0.4516, -0.4015,
        -0.3513, -0.3011, -0.2509, -0.2007, -0.1505, -0.1004, -0.0502, -0.0000,
         0.0502,  0.1004,  0.1505,  0.2007,  0.2509,  0.3011,  0.3513,  0.4015,
         0.5018,  0.5520,  0.9033], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  35
---------Q_out---------
None
name:  bert.encoder.layer.11.attention.self.query
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0415, -0.0010, -0.0390,  ..., -0.0487,  0.0482, -0.0531],
        [-0.0648, -0.0347, -0.0142,  ...,  0.0333, -0.0758,  0.0059],
        [-0.0321,  0.0618, -0.0942,  ...,  0.0221, -0.0589, -0.0779],
        ...,
        [ 0.0452,  0.0145, -0.0393,  ..., -0.0526, -0.0572,  0.0079],
        [ 0.0711, -0.0196,  0.0472,  ...,  0.0009, -0.0790, -0.0108],
        [-0.0550,  0.0374,  0.0308,  ...,  0.0274,  0.0536, -0.0445]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0397, -0.0000, -0.0397,  ..., -0.0511,  0.0454, -0.0511],
        [-0.0624, -0.0341, -0.0114,  ...,  0.0341, -0.0738,  0.0057],
        [-0.0341,  0.0624, -0.0965,  ...,  0.0227, -0.0568, -0.0795],
        ...,
        [ 0.0454,  0.0170, -0.0397,  ..., -0.0511, -0.0568,  0.0057],
        [ 0.0738, -0.0170,  0.0454,  ...,  0.0000, -0.0795, -0.0114],
        [-0.0568,  0.0397,  0.0284,  ...,  0.0284,  0.0511, -0.0454]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.7209, -0.5904, -0.5620, -0.5450, -0.5222, -0.5109, -0.4655, -0.4598,
        -0.4484, -0.4371, -0.4314, -0.4030, -0.3803, -0.3747, -0.3633, -0.3576,
        -0.3519, -0.3406, -0.3349, -0.3292, -0.3236, -0.3179, -0.3122, -0.3065,
        -0.3009, -0.2952, -0.2895, -0.2838, -0.2782, -0.2725, -0.2668, -0.2611,
        -0.2554, -0.2498, -0.2441, -0.2384, -0.2327, -0.2271, -0.2214, -0.2157,
        -0.2100, -0.2044, -0.1987, -0.1930, -0.1873, -0.1817, -0.1760, -0.1703,
        -0.1646, -0.1589, -0.1533, -0.1476, -0.1419, -0.1362, -0.1306, -0.1249,
        -0.1192, -0.1135, -0.1079, -0.1022, -0.0965, -0.0908, -0.0851, -0.0795,
        -0.0738, -0.0681, -0.0624, -0.0568, -0.0511, -0.0454, -0.0397, -0.0341,
        -0.0284, -0.0227, -0.0170, -0.0114, -0.0057, -0.0000,  0.0057,  0.0114,
         0.0170,  0.0227,  0.0284,  0.0341,  0.0397,  0.0454,  0.0511,  0.0568,
         0.0624,  0.0681,  0.0738,  0.0795,  0.0851,  0.0908,  0.0965,  0.1022,
         0.1079,  0.1135,  0.1192,  0.1249,  0.1306,  0.1362,  0.1419,  0.1476,
         0.1533,  0.1589,  0.1646,  0.1703,  0.1760,  0.1817,  0.1873,  0.1930,
         0.1987,  0.2044,  0.2100,  0.2157,  0.2214,  0.2271,  0.2327,  0.2384,
         0.2498,  0.2554,  0.2611,  0.2668,  0.2725,  0.2782,  0.2838,  0.2895,
         0.2952,  0.3009,  0.3065,  0.3122,  0.3179,  0.3236,  0.3292,  0.3406,
         0.3463,  0.3519,  0.3576,  0.3633,  0.3747,  0.3917,  0.3974,  0.4030,
         0.4201,  0.4541,  0.4598,  0.4655,  0.4768,  0.4825,  0.4882,  0.4939,
         0.5336,  0.5563,  0.5904,  0.6585], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  156
---------Q_out---------
tensor([[[-0.1262,  1.3253,  1.0728,  ...,  0.0631,  0.5049,  1.8301],
         [ 1.0097,  0.6942,  1.7039,  ..., -1.3253, -0.6311,  3.4710],
         [ 0.3155,  0.3786,  0.5049,  ..., -1.1991, -0.1893,  2.8399],
         ...,
         [ 1.7670,  0.0631, -0.5049,  ..., -0.8204,  0.1893,  2.6505],
         [-0.6311,  0.2524, -0.5049,  ..., -1.5146, -0.5680,  3.2185],
         [-0.8835, -0.0631, -0.1893,  ..., -1.7670, -0.4418,  3.3447]],

        [[-0.0631,  0.6942, -0.5049,  ..., -0.5049,  0.3155,  2.3981],
         [-0.8204, -0.3155,  0.8204,  ..., -0.4418, -2.2719,  2.2719],
         [-0.6311, -0.3155, -0.1893,  ...,  0.5049, -1.3884,  2.3981],
         ...,
         [-0.2524, -0.0000, -0.1893,  ..., -1.1991,  0.8204,  1.3253],
         [-0.2524,  1.8301, -0.8204,  ..., -1.1359,  0.8835,  2.0195],
         [ 0.9466,  1.0097,  0.8835,  ...,  0.5049,  0.8835,  0.1893]],

        [[ 0.7573,  1.2622,  0.5049,  ...,  1.5146,  0.6942,  2.0826],
         [ 0.4418,  1.7039,  0.8835,  ...,  0.6311,  0.1262,  2.4612],
         [ 1.3884,  1.3253,  0.7573,  ..., -0.5680, -1.1991,  2.9661],
         ...,
         [ 0.3786,  0.3786, -0.2524,  ..., -0.9466, -0.3786,  1.7039],
         [-0.7573,  0.1262, -0.5049,  ..., -1.3253,  0.3155,  4.4176],
         [-0.4418, -0.3786,  0.0631,  ..., -1.4515,  0.3155,  4.4807]],

        ...,

        [[-0.3786,  0.2524, -0.0000,  ...,  0.4418,  0.6942,  2.4612],
         [ 0.1262, -0.0000, -0.9466,  ..., -0.8204, -1.0728,  2.3981],
         [ 0.9466,  0.1262,  0.4418,  ..., -0.1262, -0.6311,  3.0292],
         ...,
         [-1.2622, -0.1893, -1.3253,  ..., -1.1991,  0.1893,  2.9661],
         [-0.6942, -0.5049, -1.0097,  ..., -0.5680, -0.3155,  3.4710],
         [-1.3884,  0.0631, -0.7573,  ..., -1.2622,  0.3155,  3.6603]],

        [[-0.5680,  0.2524,  0.5049,  ...,  0.0631,  0.3155,  1.6408],
         [ 1.0097, -0.0000,  1.3253,  ..., -0.4418, -2.1457,  2.9661],
         [ 0.8204,  1.0097,  1.5146,  ..., -0.5049, -1.0728,  2.7768],
         ...,
         [-1.1359,  0.0631, -0.3155,  ..., -1.8301, -0.8835,  4.5438],
         [-0.3786,  0.5049, -0.1893,  ..., -1.8301,  0.2524,  4.8593],
         [ 0.2524,  0.6311, -0.0000,  ..., -1.5777, -0.3155,  3.2816]],

        [[-0.6311,  0.4418,  0.5049,  ..., -0.1893,  0.3155,  2.2719],
         [ 1.2622,  0.6311,  0.8204,  ...,  0.9466, -1.0728,  2.7137],
         [ 1.0728,  0.7573,  0.4418,  ..., -0.6942, -1.1991,  3.7234],
         ...,
         [ 0.4418, -0.3786, -0.6942,  ..., -1.3884,  0.5680,  4.9224],
         [-0.1893, -0.4418, -0.6942,  ..., -1.7670,  0.3155,  3.8496],
         [-0.4418,  0.3155, -0.8835,  ..., -1.7039,  0.4418,  4.7962]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-7.5730, -7.5099, -7.4468, -7.3837, -7.3206, -7.2575, -7.1943, -7.1312,
        -7.0681, -7.0050, -6.9419, -6.8788, -6.8157, -6.7526, -6.6895, -6.6264,
        -6.5633, -6.5002, -6.4370, -6.3739, -6.3108, -6.2477, -6.1846, -6.1215,
        -6.0584, -5.9953, -5.9322, -5.8691, -5.8060, -5.7429, -5.6797, -5.6166,
        -5.5535, -5.4904, -5.4273, -5.3642, -5.3011, -5.2380, -5.1749, -5.1118,
        -5.0487, -4.9856, -4.9224, -4.8593, -4.7962, -4.7331, -4.6700, -4.6069,
        -4.5438, -4.4807, -4.4176, -4.3545, -4.2914, -4.2283, -4.1651, -4.1020,
        -4.0389, -3.9758, -3.9127, -3.8496, -3.7865, -3.7234, -3.6603, -3.5972,
        -3.5341, -3.4710, -3.4078, -3.3447, -3.2816, -3.2185, -3.1554, -3.0923,
        -3.0292, -2.9661, -2.9030, -2.8399, -2.7768, -2.7137, -2.6505, -2.5874,
        -2.5243, -2.4612, -2.3981, -2.3350, -2.2719, -2.2088, -2.1457, -2.0826,
        -2.0195, -1.9564, -1.8932, -1.8301, -1.7670, -1.7039, -1.6408, -1.5777,
        -1.5146, -1.4515, -1.3884, -1.3253, -1.2622, -1.1991, -1.1359, -1.0728,
        -1.0097, -0.9466, -0.8835, -0.8204, -0.7573, -0.6942, -0.6311, -0.5680,
        -0.5049, -0.4418, -0.3786, -0.3155, -0.2524, -0.1893, -0.1262, -0.0631,
        -0.0000,  0.0631,  0.1262,  0.1893,  0.2524,  0.3155,  0.3786,  0.4418,
         0.5049,  0.5680,  0.6311,  0.6942,  0.7573,  0.8204,  0.8835,  0.9466,
         1.0097,  1.0728,  1.1359,  1.1991,  1.2622,  1.3253,  1.3884,  1.4515,
         1.5146,  1.5777,  1.6408,  1.7039,  1.7670,  1.8301,  1.8932,  1.9564,
         2.0195,  2.0826,  2.1457,  2.2088,  2.2719,  2.3350,  2.3981,  2.4612,
         2.5243,  2.5874,  2.6505,  2.7137,  2.7768,  2.8399,  2.9030,  2.9661,
         3.0292,  3.0923,  3.1554,  3.2185,  3.2816,  3.3447,  3.4078,  3.4710,
         3.5341,  3.5972,  3.6603,  3.7234,  3.7865,  3.8496,  3.9127,  3.9758,
         4.0389,  4.1020,  4.1651,  4.2283,  4.2914,  4.3545,  4.4176,  4.4807,
         4.5438,  4.6069,  4.6700,  4.7331,  4.7962,  4.8593,  4.9224,  4.9856,
         5.0487,  5.1118,  5.1749,  5.2380,  5.3011,  5.3642,  5.4273,  5.4904,
         5.5535,  5.6166,  5.6797,  5.7429,  5.8060,  5.8691,  5.9322,  5.9953,
         6.0584,  6.1215,  6.1846,  6.2477,  6.3108,  6.3739,  6.4370,  6.5002,
         6.5633,  6.6264,  6.6895,  6.7526,  6.8157,  6.8788,  6.9419,  7.0050,
         7.0681,  7.1312,  7.1943,  7.5099], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  236
name:  bert.encoder.layer.11.attention.self.key
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0264, -0.0122, -0.0141,  ...,  0.0246,  0.0720, -0.0255],
        [-0.0137, -0.0583,  0.0358,  ..., -0.0418,  0.0430,  0.0057],
        [-0.0609, -0.0094,  0.0293,  ..., -0.0390, -0.0028,  0.0692],
        ...,
        [ 0.0440, -0.0391, -0.0066,  ...,  0.0465, -0.0177, -0.0105],
        [-0.0056, -0.0125,  0.0749,  ...,  0.0076, -0.0264,  0.0787],
        [ 0.0481,  0.0487, -0.0021,  ...,  0.0536, -0.0033, -0.0483]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0261, -0.0112, -0.0149,  ...,  0.0261,  0.0708, -0.0261],
        [-0.0149, -0.0596,  0.0373,  ..., -0.0410,  0.0447,  0.0075],
        [-0.0596, -0.0112,  0.0298,  ..., -0.0373, -0.0037,  0.0708],
        ...,
        [ 0.0447, -0.0373, -0.0075,  ...,  0.0447, -0.0186, -0.0112],
        [-0.0075, -0.0112,  0.0745,  ...,  0.0075, -0.0261,  0.0783],
        [ 0.0484,  0.0484, -0.0037,  ...,  0.0522, -0.0037, -0.0484]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.4733, -0.4249, -0.4174, -0.3764, -0.3690, -0.3578, -0.3280, -0.3131,
        -0.3056, -0.3019, -0.2982, -0.2944, -0.2870, -0.2721, -0.2683, -0.2609,
        -0.2572, -0.2534, -0.2497, -0.2460, -0.2385, -0.2348, -0.2311, -0.2273,
        -0.2236, -0.2162, -0.2124, -0.2087, -0.2050, -0.2013, -0.1975, -0.1938,
        -0.1901, -0.1863, -0.1826, -0.1789, -0.1752, -0.1714, -0.1677, -0.1640,
        -0.1603, -0.1565, -0.1528, -0.1491, -0.1453, -0.1416, -0.1379, -0.1342,
        -0.1304, -0.1267, -0.1230, -0.1193, -0.1155, -0.1118, -0.1081, -0.1044,
        -0.1006, -0.0969, -0.0932, -0.0894, -0.0857, -0.0820, -0.0783, -0.0745,
        -0.0708, -0.0671, -0.0634, -0.0596, -0.0559, -0.0522, -0.0484, -0.0447,
        -0.0410, -0.0373, -0.0335, -0.0298, -0.0261, -0.0224, -0.0186, -0.0149,
        -0.0112, -0.0075, -0.0037, -0.0000,  0.0037,  0.0075,  0.0112,  0.0149,
         0.0186,  0.0224,  0.0261,  0.0298,  0.0335,  0.0373,  0.0410,  0.0447,
         0.0484,  0.0522,  0.0559,  0.0596,  0.0634,  0.0671,  0.0708,  0.0745,
         0.0783,  0.0820,  0.0857,  0.0894,  0.0932,  0.0969,  0.1006,  0.1044,
         0.1081,  0.1118,  0.1155,  0.1193,  0.1230,  0.1267,  0.1304,  0.1342,
         0.1379,  0.1416,  0.1453,  0.1491,  0.1528,  0.1565,  0.1603,  0.1640,
         0.1677,  0.1714,  0.1752,  0.1789,  0.1826,  0.1863,  0.1901,  0.1938,
         0.1975,  0.2013,  0.2050,  0.2087,  0.2124,  0.2162,  0.2199,  0.2236,
         0.2422,  0.2534,  0.2572,  0.2609,  0.2832,  0.2870,  0.2907,  0.2944,
         0.3093,  0.3131], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  154
---------Q_out---------
tensor([[[-1.0163, -0.2710, -0.8808,  ...,  0.6098, -1.4906, -1.1518],
         [-0.8808, -0.6775, -0.4743,  ..., -1.9649, -1.0841, -1.8294],
         [-0.1355, -1.0841, -0.5420,  ..., -1.2873, -1.0163, -2.2359],
         ...,
         [ 0.0000, -0.3388, -1.2196,  ..., -0.1355, -0.9486, -2.9134],
         [ 0.2710, -0.6775, -1.1518,  ..., -0.5420, -0.8130, -2.1681],
         [ 0.2710, -1.4906, -1.1518,  ..., -0.7453, -1.4906, -2.9812]],

        [[-1.0163, -0.6098, -0.6098,  ...,  0.1355, -0.7453, -0.9486],
         [ 0.8130, -0.0000, -0.1355,  ..., -0.8130, -1.7616, -2.7779],
         [ 0.3388,  0.6098, -0.4743,  ..., -0.2710, -1.0841, -1.7616],
         ...,
         [-1.0841, -0.1355, -0.5420,  ..., -1.0163, -0.4743, -2.7779],
         [-0.4065, -0.8130, -1.4906,  ..., -1.4228, -0.3388, -1.6938],
         [ 0.2710,  0.1355,  0.0678,  ..., -0.0000,  0.2710,  0.6098]],

        [[-1.7616, -0.7453, -0.2033,  ...,  0.9486, -0.6098, -1.6261],
         [-1.0163, -0.3388, -0.6775,  ..., -0.3388, -0.0000, -0.7453],
         [ 0.4743, -0.5420,  0.4743,  ...,  0.0000,  0.2710,  0.4743],
         ...,
         [ 0.5420, -1.0841, -0.3388,  ..., -0.6098, -0.4743, -2.1004],
         [ 0.0000, -1.0163, -1.2873,  ..., -1.1518, -0.2033, -2.2359],
         [-0.2710, -1.5583, -1.2873,  ..., -0.8808, -0.7453, -2.5069]],

        ...,

        [[-2.1004, -0.0678, -0.4065,  ..., -0.2710, -0.9486, -1.0163],
         [-0.3388, -0.1355, -2.3036,  ..., -1.2873, -1.0841,  0.5420],
         [ 1.0841, -1.0163, -0.3388,  ..., -1.5583, -0.6098,  0.7453],
         ...,
         [ 0.2033, -0.3388, -1.4906,  ..., -1.1518, -0.8808, -2.7101],
         [ 0.1355, -0.0000, -1.1518,  ..., -0.8808, -1.1518, -1.9649],
         [ 0.2033, -0.5420, -0.8130,  ..., -1.4906, -0.5420, -2.5069]],

        [[-1.6938, -0.6775,  0.5420,  ...,  0.3388, -1.3551, -2.5746],
         [-0.4065, -1.8971,  1.4906,  ..., -1.0163, -1.0163, -2.7101],
         [-0.6775, -2.4391,  0.9486,  ..., -0.2710, -0.5420, -3.1844],
         ...,
         [-0.8130, -1.2196, -0.6775,  ..., -0.9486, -1.9649, -2.6424],
         [-0.6098, -1.0163, -0.4743,  ..., -1.0163, -1.2873, -1.7616],
         [-0.5420, -1.6938, -0.0678,  ..., -0.4065, -1.2196, -2.8457]],

        [[-1.5583,  0.2710, -0.1355,  ...,  0.2033, -0.6098, -1.0841],
         [ 0.6098,  0.6775, -1.1518,  ..., -1.0841, -0.4065, -1.0163],
         [ 1.2873,  0.3388,  0.7453,  ..., -1.6261,  0.2033, -1.7616],
         ...,
         [ 0.6775, -0.3388, -1.0163,  ..., -0.2710, -0.5420, -3.3199],
         [ 0.6775, -0.2710, -1.0163,  ..., -0.6775, -1.1518, -3.9297],
         [ 0.4743, -0.6775, -1.0841,  ..., -0.2033, -1.6261, -3.6587]]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_out)
tensor([-8.5370, -7.8594, -7.7239, -7.5884, -7.5207, -7.4529, -7.3852, -7.3174,
        -7.2497, -7.1819, -7.1141, -7.0464, -6.9786, -6.9109, -6.8431, -6.7754,
        -6.7076, -6.6399, -6.5721, -6.5044, -6.4366, -6.3689, -6.3011, -6.2333,
        -6.1656, -6.0978, -6.0301, -5.9623, -5.8946, -5.8268, -5.7591, -5.6913,
        -5.6236, -5.5558, -5.4881, -5.4203, -5.3525, -5.2848, -5.2170, -5.1493,
        -5.0815, -5.0138, -4.9460, -4.8783, -4.8105, -4.7428, -4.6750, -4.6073,
        -4.5395, -4.4717, -4.4040, -4.3362, -4.2685, -4.2007, -4.1330, -4.0652,
        -3.9975, -3.9297, -3.8620, -3.7942, -3.7265, -3.6587, -3.5909, -3.5232,
        -3.4554, -3.3877, -3.3199, -3.2522, -3.1844, -3.1167, -3.0489, -2.9812,
        -2.9134, -2.8457, -2.7779, -2.7101, -2.6424, -2.5746, -2.5069, -2.4391,
        -2.3714, -2.3036, -2.2359, -2.1681, -2.1004, -2.0326, -1.9649, -1.8971,
        -1.8294, -1.7616, -1.6938, -1.6261, -1.5583, -1.4906, -1.4228, -1.3551,
        -1.2873, -1.2196, -1.1518, -1.0841, -1.0163, -0.9486, -0.8808, -0.8130,
        -0.7453, -0.6775, -0.6098, -0.5420, -0.4743, -0.4065, -0.3388, -0.2710,
        -0.2033, -0.1355, -0.0678, -0.0000,  0.0678,  0.1355,  0.2033,  0.2710,
         0.3388,  0.4065,  0.4743,  0.5420,  0.6098,  0.6775,  0.7453,  0.8130,
         0.8808,  0.9486,  1.0163,  1.0841,  1.1518,  1.2196,  1.2873,  1.3551,
         1.4228,  1.4906,  1.5583,  1.6261,  1.6938,  1.7616,  1.8294,  1.8971,
         1.9649,  2.0326,  2.1004,  2.1681,  2.2359,  2.3036,  2.3714,  2.4391,
         2.5069,  2.5746,  2.6424,  2.7101,  2.7779,  2.8457,  2.9134,  2.9812,
         3.0489,  3.1167,  3.1844,  3.2522,  3.3199,  3.3877,  3.4554,  3.5232,
         3.5909,  3.6587,  3.7265,  3.7942,  3.8620,  3.9297,  3.9975,  4.0652,
         4.1330,  4.2007,  4.2685,  4.3362,  4.4040,  4.4717,  4.5395,  4.6073,
         4.6750,  4.7428,  4.8105,  4.8783,  4.9460,  5.0138,  5.0815,  5.1493,
         5.2170,  5.2848,  5.3525,  5.4203,  5.4881,  5.5558,  5.6236,  5.6913,
         5.7591,  5.8268,  5.8946,  5.9623,  6.0301,  6.0978,  6.1656,  6.2333,
         6.3011,  6.3689,  6.4366,  6.5044,  6.5721,  6.6399,  6.7076,  6.7754,
         6.8431,  6.9109,  6.9786,  7.0464,  7.1141,  7.1819,  7.2497,  7.3174,
         7.3852,  7.4529,  7.5207,  7.5884,  7.7239,  7.7917,  7.8594,  7.9272,
         7.9949,  8.0627,  8.1304,  8.1982,  8.4692,  8.5370,  8.6047],
       device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  239
name:  bert.encoder.layer.11.attention.self.value
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0351, -0.0540,  0.0050,  ...,  0.0363,  0.0307, -0.0604],
        [-0.0069, -0.0245,  0.0291,  ..., -0.0089,  0.0236, -0.0474],
        [-0.0288,  0.0568,  0.0530,  ..., -0.0101,  0.0288, -0.0680],
        ...,
        [ 0.0160,  0.0166, -0.0315,  ...,  0.0406,  0.0097,  0.0099],
        [-0.0499, -0.0190,  0.0223,  ...,  0.0391,  0.1193, -0.0104],
        [ 0.0028, -0.0150,  0.0482,  ..., -0.0909,  0.0079, -0.0615]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0354, -0.0532,  0.0044,  ...,  0.0354,  0.0310, -0.0598],
        [-0.0066, -0.0244,  0.0288,  ..., -0.0089,  0.0244, -0.0465],
        [-0.0288,  0.0576,  0.0532,  ..., -0.0111,  0.0288, -0.0687],
        ...,
        [ 0.0155,  0.0155, -0.0310,  ...,  0.0399,  0.0089,  0.0089],
        [-0.0509, -0.0199,  0.0221,  ...,  0.0399,  0.1196, -0.0111],
        [ 0.0022, -0.0155,  0.0487,  ..., -0.0908,  0.0089, -0.0620]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.2613, -0.2525, -0.2325, -0.2082, -0.2060, -0.2037, -0.1949, -0.1927,
        -0.1905, -0.1882, -0.1860, -0.1838, -0.1816, -0.1794, -0.1772, -0.1750,
        -0.1727, -0.1705, -0.1683, -0.1661, -0.1639, -0.1617, -0.1595, -0.1572,
        -0.1550, -0.1528, -0.1506, -0.1484, -0.1462, -0.1439, -0.1417, -0.1395,
        -0.1373, -0.1351, -0.1329, -0.1307, -0.1284, -0.1262, -0.1240, -0.1218,
        -0.1196, -0.1174, -0.1152, -0.1129, -0.1107, -0.1085, -0.1063, -0.1041,
        -0.1019, -0.0997, -0.0974, -0.0952, -0.0930, -0.0908, -0.0886, -0.0864,
        -0.0842, -0.0819, -0.0797, -0.0775, -0.0753, -0.0731, -0.0709, -0.0687,
        -0.0664, -0.0642, -0.0620, -0.0598, -0.0576, -0.0554, -0.0532, -0.0509,
        -0.0487, -0.0465, -0.0443, -0.0421, -0.0399, -0.0376, -0.0354, -0.0332,
        -0.0310, -0.0288, -0.0266, -0.0244, -0.0221, -0.0199, -0.0177, -0.0155,
        -0.0133, -0.0111, -0.0089, -0.0066, -0.0044, -0.0022, -0.0000,  0.0022,
         0.0044,  0.0066,  0.0089,  0.0111,  0.0133,  0.0155,  0.0177,  0.0199,
         0.0221,  0.0244,  0.0266,  0.0288,  0.0310,  0.0332,  0.0354,  0.0376,
         0.0399,  0.0421,  0.0443,  0.0465,  0.0487,  0.0509,  0.0532,  0.0554,
         0.0576,  0.0598,  0.0620,  0.0642,  0.0664,  0.0687,  0.0709,  0.0731,
         0.0753,  0.0775,  0.0797,  0.0819,  0.0842,  0.0864,  0.0886,  0.0908,
         0.0930,  0.0952,  0.0974,  0.0997,  0.1019,  0.1041,  0.1063,  0.1085,
         0.1107,  0.1129,  0.1152,  0.1174,  0.1196,  0.1218,  0.1240,  0.1262,
         0.1284,  0.1307,  0.1329,  0.1351,  0.1373,  0.1395,  0.1417,  0.1439,
         0.1462,  0.1484,  0.1506,  0.1528,  0.1550,  0.1572,  0.1595,  0.1617,
         0.1639,  0.1661,  0.1683,  0.1705,  0.1727,  0.1750,  0.1772,  0.1794,
         0.1816,  0.1838,  0.1860,  0.1882,  0.1993,  0.2015,  0.2104,  0.2148,
         0.2192,  0.2259,  0.2436,  0.2480,  0.2813], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  189
---------Q_out---------
None
name:  bert.encoder.layer.11.attention.output.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0235, -0.0469,  0.0435,  ...,  0.0162, -0.0311, -0.0276],
        [ 0.0312, -0.0221,  0.0546,  ...,  0.0040, -0.0057, -0.0303],
        [-0.0005, -0.0159,  0.0461,  ...,  0.0393,  0.0152,  0.0573],
        ...,
        [-0.0148, -0.0276, -0.0218,  ...,  0.0260,  0.0731, -0.0708],
        [-0.0116, -0.0652, -0.1288,  ...,  0.0432,  0.0769,  0.0070],
        [-0.0847,  0.0353,  0.0356,  ..., -0.0091, -0.0572,  0.0280]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0246, -0.0468,  0.0444,  ...,  0.0172, -0.0320, -0.0271],
        [ 0.0320, -0.0222,  0.0542,  ...,  0.0049, -0.0049, -0.0296],
        [-0.0000, -0.0148,  0.0468,  ...,  0.0394,  0.0148,  0.0567],
        ...,
        [-0.0148, -0.0271, -0.0222,  ...,  0.0271,  0.0739, -0.0715],
        [-0.0123, -0.0641, -0.1281,  ...,  0.0444,  0.0764,  0.0074],
        [-0.0838,  0.0345,  0.0345,  ..., -0.0099, -0.0567,  0.0271]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3129, -0.2415, -0.2218, -0.1922, -0.1774, -0.1700, -0.1675, -0.1651,
        -0.1626, -0.1602, -0.1577, -0.1552, -0.1528, -0.1503, -0.1478, -0.1454,
        -0.1429, -0.1404, -0.1380, -0.1355, -0.1331, -0.1306, -0.1281, -0.1257,
        -0.1232, -0.1207, -0.1183, -0.1158, -0.1133, -0.1109, -0.1084, -0.1060,
        -0.1035, -0.1010, -0.0986, -0.0961, -0.0936, -0.0912, -0.0887, -0.0862,
        -0.0838, -0.0813, -0.0788, -0.0764, -0.0739, -0.0715, -0.0690, -0.0665,
        -0.0641, -0.0616, -0.0591, -0.0567, -0.0542, -0.0517, -0.0493, -0.0468,
        -0.0444, -0.0419, -0.0394, -0.0370, -0.0345, -0.0320, -0.0296, -0.0271,
        -0.0246, -0.0222, -0.0197, -0.0172, -0.0148, -0.0123, -0.0099, -0.0074,
        -0.0049, -0.0025, -0.0000,  0.0025,  0.0049,  0.0074,  0.0099,  0.0123,
         0.0148,  0.0172,  0.0197,  0.0222,  0.0246,  0.0271,  0.0296,  0.0320,
         0.0345,  0.0370,  0.0394,  0.0419,  0.0444,  0.0468,  0.0493,  0.0517,
         0.0542,  0.0567,  0.0591,  0.0616,  0.0641,  0.0665,  0.0690,  0.0715,
         0.0739,  0.0764,  0.0788,  0.0813,  0.0838,  0.0862,  0.0887,  0.0912,
         0.0936,  0.0961,  0.0986,  0.1010,  0.1035,  0.1060,  0.1084,  0.1109,
         0.1133,  0.1158,  0.1183,  0.1207,  0.1232,  0.1257,  0.1281,  0.1306,
         0.1331,  0.1355,  0.1380,  0.1404,  0.1429,  0.1454,  0.1478,  0.1503,
         0.1528,  0.1552,  0.1577,  0.1602,  0.1626,  0.1651,  0.1675,  0.1700,
         0.1848,  0.1873,  0.1897,  0.1996], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  148
---------Q_out---------
None
name:  bert.encoder.layer.11.intermediate.dense
size :  torch.Size([3072, 768])
---------FP_weight---------
Parameter containing:
tensor([[ 0.0587,  0.0547, -0.0215,  ...,  0.0322,  0.0472, -0.0258],
        [-0.0113, -0.0312, -0.0500,  ...,  0.0239,  0.0018,  0.0107],
        [ 0.0021,  0.0416, -0.0370,  ...,  0.0285,  0.0118, -0.0310],
        ...,
        [ 0.0422,  0.0794,  0.0203,  ...,  0.0413, -0.0606,  0.0130],
        [-0.0061,  0.0453,  0.0392,  ..., -0.0118,  0.0521,  0.0408],
        [-0.0438, -0.0513,  0.0375,  ...,  0.0036,  0.0825, -0.0068]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[ 0.0599,  0.0564, -0.0211,  ...,  0.0317,  0.0458, -0.0247],
        [-0.0106, -0.0317, -0.0493,  ...,  0.0247,  0.0035,  0.0106],
        [ 0.0035,  0.0423, -0.0352,  ...,  0.0282,  0.0106, -0.0317],
        ...,
        [ 0.0423,  0.0811,  0.0211,  ...,  0.0423, -0.0599,  0.0141],
        [-0.0070,  0.0458,  0.0388,  ..., -0.0106,  0.0529,  0.0423],
        [-0.0423, -0.0529,  0.0388,  ...,  0.0035,  0.0811, -0.0070]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.4476, -0.3172, -0.3137, -0.3031, -0.2996, -0.2961, -0.2820, -0.2785,
        -0.2644, -0.2608, -0.2538, -0.2467, -0.2432, -0.2362, -0.2291, -0.2256,
        -0.2185, -0.2150, -0.2080, -0.2044, -0.2009, -0.1974, -0.1939, -0.1903,
        -0.1868, -0.1833, -0.1798, -0.1762, -0.1727, -0.1692, -0.1657, -0.1621,
        -0.1586, -0.1551, -0.1516, -0.1480, -0.1445, -0.1410, -0.1375, -0.1339,
        -0.1304, -0.1269, -0.1234, -0.1198, -0.1163, -0.1128, -0.1093, -0.1057,
        -0.1022, -0.0987, -0.0952, -0.0916, -0.0881, -0.0846, -0.0811, -0.0775,
        -0.0740, -0.0705, -0.0670, -0.0634, -0.0599, -0.0564, -0.0529, -0.0493,
        -0.0458, -0.0423, -0.0388, -0.0352, -0.0317, -0.0282, -0.0247, -0.0211,
        -0.0176, -0.0141, -0.0106, -0.0070, -0.0035, -0.0000,  0.0035,  0.0070,
         0.0106,  0.0141,  0.0176,  0.0211,  0.0247,  0.0282,  0.0317,  0.0352,
         0.0388,  0.0423,  0.0458,  0.0493,  0.0529,  0.0564,  0.0599,  0.0634,
         0.0670,  0.0705,  0.0740,  0.0775,  0.0811,  0.0846,  0.0881,  0.0916,
         0.0952,  0.0987,  0.1022,  0.1057,  0.1093,  0.1128,  0.1163,  0.1198,
         0.1234,  0.1269,  0.1304,  0.1339,  0.1375,  0.1410,  0.1445,  0.1480,
         0.1516,  0.1551,  0.1586,  0.1621,  0.1657,  0.1692,  0.1727,  0.1762,
         0.1798,  0.1833,  0.1868,  0.1903,  0.1939,  0.1974,  0.2009,  0.2044,
         0.2115,  0.2150,  0.2185,  0.2221,  0.2291,  0.2326,  0.2467,  0.2608,
         0.2679,  0.2820], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  146
---------Q_out---------
None
name:  bert.encoder.layer.11.output.dense
size :  torch.Size([768, 3072])
---------FP_weight---------
Parameter containing:
tensor([[-0.0224,  0.0540, -0.0090,  ..., -0.0940, -0.0108,  0.0464],
        [ 0.0107, -0.0080, -0.0157,  ...,  0.0101, -0.0168,  0.0185],
        [-0.0135, -0.0597, -0.0227,  ...,  0.0008,  0.0512,  0.0980],
        ...,
        [-0.0232, -0.0076,  0.0177,  ..., -0.0031, -0.0151,  0.0121],
        [-0.0113,  0.0444, -0.0051,  ..., -0.0604, -0.0259,  0.0889],
        [-0.0367, -0.0216,  0.0212,  ..., -0.0129, -0.0067,  0.0139]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0169,  0.0507, -0.0169,  ..., -0.1015, -0.0169,  0.0507],
        [ 0.0169, -0.0000, -0.0169,  ...,  0.0169, -0.0169,  0.0169],
        [-0.0169, -0.0677, -0.0169,  ...,  0.0000,  0.0507,  0.1015],
        ...,
        [-0.0169, -0.0000,  0.0169,  ..., -0.0000, -0.0169,  0.0169],
        [-0.0169,  0.0507, -0.0000,  ..., -0.0677, -0.0338,  0.0846],
        [-0.0338, -0.0169,  0.0169,  ..., -0.0169, -0.0000,  0.0169]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-2.1484, -1.9792, -1.6240, -1.2857, -1.2180, -0.9135, -0.8627, -0.7443,
        -0.7105, -0.6936, -0.6090, -0.5752, -0.5244, -0.4737, -0.3722, -0.3214,
        -0.3045, -0.2876, -0.2707, -0.2368, -0.2199, -0.2030, -0.1861, -0.1692,
        -0.1522, -0.1353, -0.1184, -0.1015, -0.0846, -0.0677, -0.0507, -0.0338,
        -0.0169, -0.0000,  0.0169,  0.0338,  0.0507,  0.0677,  0.0846,  0.1015,
         0.1184,  0.1353,  0.1522,  0.1692,  0.1861,  0.2030,  0.2199,  0.2368,
         0.2707,  0.2876,  0.3214,  0.3552,  0.4229,  0.4398,  0.5075,  0.5921,
         0.6090,  0.6428,  0.6767,  0.8797,  0.9135,  0.9473], device='cuda:0',
       grad_fn=<NotImplemented>)
number of unique Q_weights:  62
---------Q_out---------
None
name:  bert.pooler.dense
size :  torch.Size([768, 768])
---------FP_weight---------
Parameter containing:
tensor([[-0.0012, -0.0380, -0.0158,  ...,  0.0245, -0.0009,  0.0240],
        [ 0.0021,  0.0151,  0.0033,  ...,  0.0180, -0.0024,  0.0232],
        [-0.0385,  0.0145,  0.0622,  ...,  0.0375, -0.0106, -0.0396],
        ...,
        [-0.0110,  0.0137,  0.0542,  ...,  0.0667,  0.0016, -0.0090],
        [ 0.0002,  0.0025, -0.0125,  ...,  0.0047, -0.0015, -0.0078],
        [ 0.0414,  0.0751,  0.0304,  ...,  0.0316,  0.0480,  0.0081]],
       device='cuda:0', requires_grad=True)



---------Q_weight---------
tensor([[-0.0000, -0.0372, -0.0155,  ...,  0.0248, -0.0000,  0.0248],
        [ 0.0031,  0.0155,  0.0031,  ...,  0.0186, -0.0031,  0.0217],
        [-0.0372,  0.0155,  0.0620,  ...,  0.0372, -0.0093, -0.0403],
        ...,
        [-0.0124,  0.0124,  0.0527,  ...,  0.0682,  0.0031, -0.0093],
        [ 0.0000,  0.0031, -0.0124,  ...,  0.0062, -0.0000, -0.0093],
        [ 0.0403,  0.0744,  0.0310,  ...,  0.0310,  0.0465,  0.0093]],
       device='cuda:0', grad_fn=<FakeLinearQuantizationWithSTEBackward>)
torch.unique(module.Q_weight)
tensor([-0.3935, -0.3501, -0.3439, -0.3315, -0.3284, -0.3253, -0.3222, -0.3191,
        -0.3160, -0.3098, -0.3067, -0.3036, -0.3005, -0.2974, -0.2943, -0.2913,
        -0.2882, -0.2851, -0.2820, -0.2789, -0.2758, -0.2727, -0.2696, -0.2665,
        -0.2634, -0.2603, -0.2572, -0.2541, -0.2510, -0.2479, -0.2448, -0.2417,
        -0.2386, -0.2355, -0.2324, -0.2293, -0.2262, -0.2231, -0.2200, -0.2169,
        -0.2138, -0.2107, -0.2076, -0.2045, -0.2014, -0.1983, -0.1952, -0.1921,
        -0.1890, -0.1859, -0.1828, -0.1797, -0.1766, -0.1735, -0.1704, -0.1673,
        -0.1642, -0.1611, -0.1580, -0.1549, -0.1518, -0.1487, -0.1456, -0.1425,
        -0.1394, -0.1363, -0.1332, -0.1301, -0.1270, -0.1239, -0.1208, -0.1177,
        -0.1146, -0.1115, -0.1084, -0.1053, -0.1022, -0.0991, -0.0961, -0.0930,
        -0.0899, -0.0868, -0.0837, -0.0806, -0.0775, -0.0744, -0.0713, -0.0682,
        -0.0651, -0.0620, -0.0589, -0.0558, -0.0527, -0.0496, -0.0465, -0.0434,
        -0.0403, -0.0372, -0.0341, -0.0310, -0.0279, -0.0248, -0.0217, -0.0186,
        -0.0155, -0.0124, -0.0093, -0.0062, -0.0031, -0.0000,  0.0031,  0.0062,
         0.0093,  0.0124,  0.0155,  0.0186,  0.0217,  0.0248,  0.0279,  0.0310,
         0.0341,  0.0372,  0.0403,  0.0434,  0.0465,  0.0496,  0.0527,  0.0558,
         0.0589,  0.0620,  0.0651,  0.0682,  0.0713,  0.0744,  0.0775,  0.0806,
         0.0837,  0.0868,  0.0899,  0.0930,  0.0961,  0.0991,  0.1022,  0.1053,
         0.1084,  0.1115,  0.1146,  0.1177,  0.1208,  0.1239,  0.1270,  0.1301,
         0.1332,  0.1363,  0.1394,  0.1425,  0.1456,  0.1487,  0.1518,  0.1549,
         0.1580,  0.1611,  0.1642,  0.1673,  0.1704,  0.1735,  0.1766,  0.1797,
         0.1828,  0.1859,  0.1890,  0.1921,  0.1952,  0.1983,  0.2014,  0.2045,
         0.2076,  0.2107,  0.2138,  0.2169,  0.2200,  0.2231,  0.2262,  0.2293,
         0.2324,  0.2355,  0.2386,  0.2417,  0.2448,  0.2479,  0.2510,  0.2541,
         0.2572,  0.2603,  0.2634,  0.2665,  0.2696,  0.2727,  0.2758,  0.2789,
         0.2820,  0.2882,  0.2943,  0.2974,  0.3005,  0.3067,  0.3098,  0.3160,
         0.3191,  0.3222,  0.3284,  0.3315,  0.3346,  0.3470,  0.3532,  0.3811,
         0.3873], device='cuda:0', grad_fn=<NotImplemented>)
number of unique Q_weights:  217
---------Q_out---------
None



> /home/imza/nlp-architect/nlp_architect/models/transformers/base_model.py(347)_train()
-> loss = outputs[0]  # get loss
(Pdb) 
Traceback (most recent call last):
  File "/home/imza/anaconda3/bin/nlp-train", line 7, in <module>
    exec(compile(f.read(), __file__, 'exec'))
  File "/home/imza/nlp-architect/nlp_architect/nlp-train", line 5, in <module>
    nlp_train_cli()
  File "/home/imza/nlp-architect/nlp_architect/cli/__init__.py", line 45, in nlp_train_cli
    args.func(args)
  File "/home/imza/nlp-architect/nlp_architect/procedures/transformers/glue.py", line 46, in run_procedure
    do_training(args)
  File "/home/imza/nlp-architect/nlp_architect/procedures/transformers/glue.py", line 175, in do_training
    save_steps=args.save_steps,
  File "/home/imza/nlp-architect/nlp_architect/models/transformers/sequence_classification.py", line 163, in train
    save_steps=save_steps,
  File "/home/imza/nlp-architect/nlp_architect/models/transformers/base_model.py", line 347, in _train
    loss = outputs[0]  # get loss
  File "/home/imza/nlp-architect/nlp_architect/models/transformers/base_model.py", line 347, in _train
    loss = outputs[0]  # get loss
  File "/home/imza/anaconda3/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/imza/anaconda3/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
bdb.BdbQuit
(base) imza@watson:~/nlp-architect$ exit
exit

Script done on 2021-04-06 10:56:49+0900
